---
sidebar_position: 8
title: 'Chapter 8: Sensor Simulation'
---

# Sensor Simulation

## Overview

Sensor simulation is a critical component of robotic simulation environments, particularly for humanoid robots that rely on multiple sensor modalities for perception, balance, and interaction. This chapter explores how to accurately simulate various sensors in Gazebo and other simulation environments, ensuring that the virtual sensors closely match their real-world counterparts.

## Learning Objectives

By the end of this chapter, you will be able to:
- Understand different types of sensors used in humanoid robots
- Configure and implement sensor simulation in Gazebo
- Calibrate virtual sensors to match real-world characteristics
- Integrate simulated sensors with ROS 2
- Handle sensor noise and uncertainty in simulation

## Types of Sensors in Humanoid Robots

Humanoid robots typically employ multiple sensor types to perceive their environment and maintain balance:

### Proprioceptive Sensors
- Joint encoders: Measure joint positions and velocities
- Force/torque sensors: Measure forces at joints and end effectors
- IMU (Inertial Measurement Unit): Measure orientation and acceleration

### Exteroceptive Sensors
- Cameras: Visual perception and object recognition
- LIDAR: Distance measurement and mapping
- Tactile sensors: Contact detection and force measurement
- Microphones: Audio input and speech recognition

### Specialized Sensors
- Force plates: Ground reaction forces
- Air pressure sensors: Altitude and weather
- Temperature sensors: Environmental monitoring

## Gazebo Sensor Simulation

### Camera Simulation

Cameras are essential for humanoid robots for navigation, object recognition, and human interaction:

```xml
<gazebo reference="head_camera">
  <sensor name="camera_sensor" type="camera">
    <always_on>true</always_on>
    <update_rate>30</update_rate>
    <camera name="head_camera">
      <horizontal_fov>1.047</horizontal_fov> <!-- 60 degrees -->
      <image>
        <width>640</width>
        <height>480</height>
        <format>R8G8B8</format>
      </image>
      <clip>
        <near>0.1</near>
        <far>10.0</far>
      </clip>
      <noise>
        <type>gaussian</type>
        <mean>0.0</mean>
        <stddev>0.007</stddev>
      </noise>
    </camera>
    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">
      <frame_name>head_camera_optical_frame</frame_name>
      <min_depth>0.1</min_depth>
      <max_depth>10.0</max_depth>
      <update_rate>30.0</update_rate>
    </plugin>
  </sensor>
</gazebo>
```

### Depth Camera Simulation

For 3D perception, depth cameras are crucial:

```xml
<gazebo reference="depth_camera">
  <sensor name="depth_camera_sensor" type="depth">
    <always_on>true</always_on>
    <update_rate>30</update_rate>
    <camera name="depth_camera">
      <horizontal_fov>1.047</horizontal_fov>
      <image>
        <width>640</width>
        <height>480</height>
        <format>R8G8B8</format>
      </image>
      <clip>
        <near>0.1</near>
        <far>10.0</far>
      </clip>
    </camera>
    <plugin name="depth_camera_controller" filename="libgazebo_ros_openni_kinect.so">
      <baseline>0.2</baseline>
      <distortion_k1>0.0</distortion_k1>
      <distortion_k2>0.0</distortion_k2>
      <distortion_k3>0.0</distortion_k3>
      <distortion_t1>0.0</distortion_t1>
      <distortion_t2>0.0</distortion_t2>
      <point_cloud_cutoff>0.1</point_cloud_cutoff>
      <point_cloud_cutoff_max>10.0</point_cloud_cutoff_max>
      <frame_name>depth_camera_optical_frame</frame_name>
      <point_cloud_transport_hints>raw</point_cloud_transport_hints>
      <update_rate>30.0</update_rate>
    </plugin>
  </sensor>
</gazebo>
```

### LIDAR Simulation

LIDAR sensors provide distance measurements for mapping and navigation:

```xml
<gazebo reference="laser_link">
  <sensor name="laser_sensor" type="ray">
    <always_on>true</always_on>
    <update_rate>40</update_rate>
    <ray>
      <scan>
        <horizontal>
          <samples>720</samples>
          <resolution>1</resolution>
          <min_angle>-1.570796</min_angle> <!-- -90 degrees -->
          <max_angle>1.570796</max_angle>   <!-- 90 degrees -->
        </horizontal>
      </scan>
      <range>
        <min>0.1</min>
        <max>30.0</max>
        <resolution>0.01</resolution>
      </range>
      <noise>
        <type>gaussian</type>
        <mean>0.0</mean>
        <stddev>0.01</stddev>
      </noise>
    </ray>
    <plugin name="laser_controller" filename="libgazebo_ros_ray_sensor.so">
      <ros>
        <namespace>/my_humanoid</namespace>
        <remapping>~/out:=scan</remapping>
      </ros>
      <output_type>sensor_msgs/LaserScan</output_type>
      <frame_name>laser_link</frame_name>
    </plugin>
  </sensor>
</gazebo>
```

### IMU Simulation

IMUs are critical for balance and orientation in humanoid robots:

```xml
<gazebo reference="imu_link">
  <sensor name="imu_sensor" type="imu">
    <always_on>true</always_on>
    <update_rate>100</update_rate>
    <imu>
      <angular_velocity>
        <x>
          <noise type="gaussian">
            <mean>0.0</mean>
            <stddev>0.0017</stddev> <!-- ~0.1 deg/s -->
            <bias_mean>0.0000</bias_mean>
            <bias_stddev>0.0001</bias_stddev>
          </noise>
        </x>
        <y>
          <noise type="gaussian">
            <mean>0.0</mean>
            <stddev>0.0017</stddev>
            <bias_mean>0.0000</bias_mean>
            <bias_stddev>0.0001</bias_stddev>
          </noise>
        </y>
        <z>
          <noise type="gaussian">
            <mean>0.0</mean>
            <stddev>0.0017</stddev>
            <bias_mean>0.0000</bias_mean>
            <bias_stddev>0.0001</bias_stddev>
          </noise>
        </z>
      </angular_velocity>
      <linear_acceleration>
        <x>
          <noise type="gaussian">
            <mean>0.0</mean>
            <stddev>1.7e-2</stddev>
            <bias_mean>0.0</bias_mean>
            <bias_stddev>1.7e-3</bias_stddev>
          </noise>
        </x>
        <y>
          <noise type="gaussian">
            <mean>0.0</mean>
            <stddev>1.7e-2</stddev>
            <bias_mean>0.0</bias_mean>
            <bias_stddev>1.7e-3</bias_stddev>
          </noise>
        </y>
        <z>
          <noise type="gaussian">
            <mean>0.0</mean>
            <stddev>1.7e-2</stddev>
            <bias_mean>0.0</bias_mean>
            <bias_stddev>1.7e-3</bias_stddev>
          </noise>
        </z>
      </linear_acceleration>
    </imu>
    <plugin name="imu_controller" filename="libgazebo_ros_imu.so">
      <ros>
        <namespace>/my_humanoid</namespace>
        <remapping>~/out:=imu</remapping>
      </ros>
      <frame_name>imu_link</frame_name>
      <body_name>torso</body_name>
      <update_rate>100</update_rate>
    </plugin>
  </sensor>
</gazebo>
```

### Force/Torque Sensor Simulation

Force/torque sensors are essential for humanoid balance and manipulation:

```xml
<gazebo>
  <plugin name="left_foot_ft_sensor" filename="libgazebo_ros_ft_sensor.so">
    <updateRate>100.0</updateRate>
    <topicName>/my_humanoid/left_foot/force_torque</topicName>
    <jointName>left_ankle_roll</jointName> <!-- Joint to measure forces at -->
  </plugin>
</gazebo>
```

## Sensor Noise and Realism

### Adding Realistic Noise Models

Real sensors have noise characteristics that must be simulated:

```xml
<!-- Example of realistic camera noise -->
<camera name="realistic_camera">
  <image>
    <width>640</width>
    <height>480</height>
    <format>R8G8B8</format>
  </image>
  <noise>
    <type>gaussian</type>
    <mean>0.0</mean>
    <stddev>0.01</stddev> <!-- 1% noise -->
  </noise>
</camera>

<!-- Example of realistic LIDAR noise -->
<ray>
  <scan>
    <horizontal>
      <samples>360</samples>
      <resolution>1</resolution>
      <min_angle>-3.14159</min_angle>
      <max_angle>3.14159</max_angle>
    </horizontal>
  </scan>
  <range>
    <min>0.1</min>
    <max>10.0</max>
    <resolution>0.01</resolution>
  </range>
  <noise>
    <type>gaussian</type>
    <mean>0.0</mean>
    <stddev>0.02</stddev> <!-- 2cm noise at 1m -->
  </noise>
</ray>
```

### Dynamic Noise Models

For more realistic simulation, noise characteristics can vary based on environmental conditions:

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import LaserScan
import numpy as np

class DynamicNoiseSimulator(Node):
    def __init__(self):
        super().__init__('dynamic_noise_simulator')

        # Subscribe to raw sensor data
        self.raw_scan_sub = self.create_subscription(
            LaserScan, '/gazebo/scan', self.raw_scan_callback, 10)

        # Publish noisy sensor data
        self.noisy_scan_pub = self.create_publisher(
            LaserScan, '/scan', 10)

    def raw_scan_callback(self, msg):
        """Add dynamic noise to laser scan data"""
        noisy_msg = LaserScan()
        noisy_msg.header = msg.header
        noisy_msg.angle_min = msg.angle_min
        noisy_msg.angle_max = msg.angle_max
        noisy_msg.angle_increment = msg.angle_increment
        noisy_msg.time_increment = msg.time_increment
        noisy_msg.scan_time = msg.scan_time
        noisy_msg.range_min = msg.range_min
        noisy_msg.range_max = msg.range_max

        # Add distance-dependent noise
        noisy_ranges = []
        for i, original_range in enumerate(msg.ranges):
            if np.isfinite(original_range):
                # Noise increases with distance (beam divergence)
                noise_std = 0.01 + 0.005 * original_range  # 1cm + 0.5% of range
                noise = np.random.normal(0, noise_std)
                noisy_range = max(
                    msg.range_min,
                    min(msg.range_max, original_range + noise)
                )
            else:
                noisy_range = original_range  # Keep inf or nan as is

            noisy_ranges.append(noisy_range)

        noisy_msg.ranges = noisy_ranges
        self.noisy_scan_pub.publish(noisy_msg)

def main(args=None):
    rclpy.init(args=args)
    node = DynamicNoiseSimulator()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Multi-Sensor Integration

### Sensor Fusion Node

Humanoid robots often combine multiple sensors for enhanced perception:

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import LaserScan, Imu, Image
from geometry_msgs.msg import PoseStamped
from cv_bridge import CvBridge
import numpy as np
import tf2_ros
from tf2_ros import TransformException

class SensorFusionNode(Node):
    def __init__(self):
        super().__init__('sensor_fusion_node')

        # Initialize CV bridge
        self.cv_bridge = CvBridge()

        # Sensor data storage
        self.laser_data = None
        self.imu_data = None
        self.camera_data = None

        # Subscribers for different sensors
        self.laser_sub = self.create_subscription(
            LaserScan, '/scan', self.laser_callback, 10)
        self.imu_sub = self.create_subscription(
            Imu, '/imu', self.imu_callback, 10)
        self.camera_sub = self.create_subscription(
            Image, '/camera/image_raw', self.camera_callback, 10)

        # Publisher for fused perception
        self.perception_pub = self.create_publisher(
            PoseStamped, '/perception/target_pose', 10)

        # TF buffer for coordinate transforms
        self.tf_buffer = tf2_ros.Buffer()
        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self)

        # Processing timer
        self.fusion_timer = self.create_timer(0.1, self.sensor_fusion_process)

    def laser_callback(self, msg):
        """Process laser scan data"""
        self.laser_data = msg

    def imu_callback(self, msg):
        """Process IMU data"""
        self.imu_data = msg

    def camera_callback(self, msg):
        """Process camera data"""
        try:
            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
            self.camera_data = cv_image
        except Exception as e:
            self.get_logger().error(f'Camera data conversion failed: {e}')

    def sensor_fusion_process(self):
        """Process and fuse sensor data"""
        if all([self.laser_data, self.imu_data, self.camera_data]):
            # Example: Combine visual and LIDAR data to detect objects
            target_pose = self.fuse_visual_lidar_data()

            if target_pose is not None:
                self.perception_pub.publish(target_pose)

    def fuse_visual_lidar_data(self):
        """Example fusion algorithm"""
        # Convert camera image to detect objects
        # Use LIDAR to get distance to detected objects
        # Use IMU to account for robot orientation
        # Return fused target pose in world coordinates

        # This is a simplified example
        # Real fusion would involve complex algorithms
        return None

def main(args=None):
    rclpy.init(args=args)
    node = SensorFusionNode()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Calibration and Validation

### Sensor Calibration

Virtual sensors need to be calibrated to match real sensor characteristics:

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import CameraInfo
from cv2 import FileStorage, FileNode

class SensorCalibrator(Node):
    def __init__(self):
        super().__init__('sensor_calibrator')

        # Publisher for calibrated camera info
        self.camera_info_pub = self.create_publisher(
            CameraInfo, '/camera/camera_info', 10)

        # Load calibration parameters
        self.calibration_params = self.load_calibration('camera_calibration.yaml')

        # Timer to publish calibration info
        self.calibration_timer = self.create_timer(1.0, self.publish_calibration)

    def load_calibration(self, filename):
        """Load calibration parameters from file"""
        # In practice, load from YAML or other calibration file
        params = {
            'camera_matrix': [[525.0, 0.0, 319.5],
                              [0.0, 525.0, 239.5],
                              [0.0, 0.0, 1.0]],
            'distortion_coefficients': [0.0, 0.0, 0.0, 0.0, 0.0],
            'image_width': 640,
            'image_height': 480
        }
        return params

    def publish_calibration(self):
        """Publish camera calibration info"""
        msg = CameraInfo()
        msg.width = self.calibration_params['image_width']
        msg.height = self.calibration_params['image_height']
        msg.k = self.calibration_params['camera_matrix']
        msg.d = self.calibration_params['distortion_coefficients']

        # Set other calibration parameters
        msg.header.frame_id = 'camera_optical_frame'
        self.camera_info_pub.publish(msg)

def main(args=None):
    rclpy.init(args=args)
    node = SensorCalibrator()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Advanced Sensor Simulation

### Custom Sensor Plugins

For specialized sensors, you can create custom Gazebo plugins:

```cpp
#include <gazebo/gazebo.hh>
#include <gazebo/sensors/sensors.hh>
#include <gazebo/physics/physics.hh>
#include <ros/ros.h>
#include <sensor_msgs/Range.h>

namespace gazebo
{
  class TactileSensorPlugin : public SensorPlugin
  {
    public: void Load(sensors::SensorPtr _sensor, sdf::ElementPtr _sdf)
    {
      // Get the range sensor
      this->parentSensor =
        std::dynamic_pointer_cast<sensors::RaySensor>(_sensor);

      if (!this->parentSensor)
      {
        gzerr << "TactileSensorPlugin not attached to a ray sensor\n";
        return;
      }

      // Connect to the sensor update event
      this->updateConnection = this->parentSensor->ConnectUpdated(
          std::bind(&TactileSensorPlugin::OnUpdate, this));

      // Make sure the parent sensor is active
      this->parentSensor->SetActive(true);

      // Initialize ROS
      if (!ros::isInitialized())
      {
        int argc = 0;
        char **argv = NULL;
        ros::init(argc, argv, "gazebo_client",
                  ros::init_options::NoSigintHandler);
      }

      this->rosnode = new ros::NodeHandle("gazebo_client");
      this->pub = this->rosnode->advertise<sensor_msgs::Range>(
          "/tactile_sensor", 1);
    }

    public: void OnUpdate()
    {
      // Get range data from sensor
      double range = this->parentSensor->Range(0);

      // Publish tactile sensor data
      sensor_msgs::Range msg;
      msg.header.stamp = ros::Time::now();
      msg.header.frame_id = "tactile_sensor_frame";
      msg.radiation_type = sensor_msgs::Range::ULTRASOUND;
      msg.field_of_view = 0.1;
      msg.min_range = 0.01;
      msg.max_range = 1.0;
      msg.range = range;

      this->pub.publish(msg);
    }

    private: sensors::RaySensorPtr parentSensor;
    private: ros::NodeHandle* rosnode;
    private: ros::Publisher pub;
    private: event::ConnectionPtr updateConnection;
  };

  GZ_REGISTER_SENSOR_PLUGIN(TactileSensorPlugin)
}
```

### Multi-Modal Sensor Simulation

Humanoid robots often require complex multi-modal sensor simulation:

```xml
<!-- Multi-modal head sensor array -->
<gazebo reference="head_link">
  <!-- RGB Camera -->
  <sensor name="rgb_camera" type="camera">
    <pose>0.1 0 0.05 0 0 0</pose>
    <camera name="head_camera">
      <horizontal_fov>1.047</horizontal_fov>
      <image><width>640</width><height>480</height><format>R8G8B8</format></image>
      <clip><near>0.1</near><far>10</far></clip>
    </camera>
    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">
      <frame_name>head_camera_optical_frame</frame_name>
    </plugin>
  </sensor>

  <!-- Depth Camera -->
  <sensor name="depth_camera" type="depth">
    <pose>0.1 0.05 0.05 0 0 0</pose>
    <camera name="head_depth_camera">
      <horizontal_fov>1.047</horizontal_fov>
      <image><width>320</width><height>240</height><format>L8</format></image>
      <clip><near>0.3</near><far>5</far></clip>
    </camera>
    <plugin name="depth_controller" filename="libgazebo_ros_openni_kinect.so">
      <frame_name>head_depth_optical_frame</frame_name>
    </plugin>
  </sensor>

  <!-- Microphone Array -->
  <sensor name="microphone_array" type="audio">
    <pose>0.05 0.05 0 0 0 0</pose>
    <always_on>true</always_on>
    <update_rate>44100</update_rate>
  </sensor>
</gazebo>
```

## Sensor Data Processing

### Real-time Sensor Processing Node

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import LaserScan, Image, Imu
from std_msgs.msg import Float32MultiArray
from cv_bridge import CvBridge
import numpy as np
import cv2
from collections import deque

class SensorProcessingNode(Node):
    def __init__(self):
        super().__init__('sensor_processing_node')

        # Initialize CV bridge
        self.cv_bridge = CvBridge()

        # Sensor data buffers
        self.scan_buffer = deque(maxlen=10)
        self.imu_buffer = deque(maxlen=100)
        self.image_buffer = deque(maxlen=5)

        # Subscribers
        self.scan_sub = self.create_subscription(
            LaserScan, '/scan', self.scan_callback, 10)
        self.imu_sub = self.create_subscription(
            Imu, '/imu', self.imu_callback, 10)
        self.image_sub = self.create_subscription(
            Image, '/camera/image_raw', self.image_callback, 10)

        # Publishers for processed data
        self.obstacle_pub = self.create_publisher(
            Float32MultiArray, '/processed/obstacles', 10)
        self.balance_pub = self.create_publisher(
            Float32MultiArray, '/processed/balance', 10)

        # Processing timer
        self.process_timer = self.create_timer(0.05, self.process_sensors)

    def scan_callback(self, msg):
        """Store laser scan data"""
        self.scan_buffer.append(msg)

    def imu_callback(self, msg):
        """Store IMU data"""
        self.imu_buffer.append(msg)

    def image_callback(self, msg):
        """Store image data"""
        try:
            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
            self.image_buffer.append(cv_image)
        except Exception as e:
            self.get_logger().error(f'Image conversion failed: {e}')

    def process_sensors(self):
        """Process sensor data in real-time"""
        if len(self.scan_buffer) > 0:
            # Process laser scan for obstacle detection
            obstacles = self.detect_obstacles()
            if obstacles is not None:
                obstacle_msg = Float32MultiArray()
                obstacle_msg.data = obstacles
                self.obstacle_pub.publish(obstacle_msg)

        if len(self.imu_buffer) > 0:
            # Process IMU data for balance
            balance_data = self.calculate_balance()
            if balance_data is not None:
                balance_msg = Float32MultiArray()
                balance_msg.data = balance_data
                self.balance_pub.publish(balance_msg)

    def detect_obstacles(self):
        """Detect obstacles from laser scan"""
        if len(self.scan_buffer) == 0:
            return None

        latest_scan = self.scan_buffer[-1]
        ranges = np.array(latest_scan.ranges)

        # Remove invalid ranges
        valid_ranges = ranges[np.isfinite(ranges)]

        # Detect obstacles within 1m
        obstacle_distances = valid_ranges[valid_ranges < 1.0]

        if len(obstacle_distances) > 0:
            # Return minimum distance and count
            return [np.min(obstacle_distances), len(obstacle_distances)]
        else:
            return [float('inf'), 0]

    def calculate_balance(self):
        """Calculate balance metrics from IMU data"""
        if len(self.imu_buffer) < 10:
            return None

        # Get recent IMU data
        recent_imu = list(self.imu_buffer)[-10:]

        # Calculate average orientation
        roll_avg = np.mean([self.quat_to_roll_pitch_yaw(
            imu.orientation)[0] for imu in recent_imu])
        pitch_avg = np.mean([self.quat_to_roll_pitch_yaw(
            imu.orientation)[1] for imu in recent_imu])

        # Calculate angular velocity
        angular_vel = np.mean([
            np.sqrt(imu.angular_velocity.x**2 +
                   imu.angular_velocity.y**2 +
                   imu.angular_velocity.z**2)
            for imu in recent_imu
        ])

        return [roll_avg, pitch_avg, angular_vel]

    def quat_to_roll_pitch_yaw(self, quat):
        """Convert quaternion to roll-pitch-yaw"""
        import math
        w, x, y, z = quat.w, quat.x, quat.y, quat.z

        # Roll (x-axis rotation)
        sinr_cosp = 2 * (w * x + y * z)
        cosr_cosp = 1 - 2 * (x * x + y * y)
        roll = math.atan2(sinr_cosp, cosr_cosp)

        # Pitch (y-axis rotation)
        sinp = 2 * (w * y - z * x)
        pitch = math.asin(sinp)

        # Yaw (z-axis rotation)
        siny_cosp = 2 * (w * z + x * y)
        cosy_cosp = 1 - 2 * (y * y + z * z)
        yaw = math.atan2(siny_cosp, cosy_cosp)

        return roll, pitch, yaw

def main(args=None):
    rclpy.init(args=args)
    node = SensorProcessingNode()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Sim-to-Real Transfer Considerations

### Handling the Reality Gap

One of the main challenges in sensor simulation is the "reality gap":

#### Domain Randomization
```python
# Example: Randomizing sensor parameters during training
class DomainRandomizationNode(Node):
    def __init__(self):
        super().__init__('domain_randomization')

        # Randomize sensor parameters within realistic bounds
        self.noise_std_range = [0.005, 0.02]  # Different noise levels
        self.bias_range = [-0.001, 0.001]     # Different biases

    def randomize_sensor_params(self):
        """Randomize sensor parameters for training"""
        noise_std = np.random.uniform(*self.noise_std_range)
        bias = np.random.uniform(*self.bias_range)
        return noise_std, bias
```

#### Sensor Simulation Accuracy
- Validate simulation against real sensor data
- Use real-world calibration parameters
- Implement realistic noise models
- Test edge cases and failure modes

## Best Practices

### 1. Sensor Placement
- Position sensors to match real robot configuration
- Consider field of view and sensing range
- Avoid occlusions where possible
- Account for robot kinematics

### 2. Computational Performance
- Balance sensor fidelity with simulation speed
- Use appropriate update rates
- Consider sensor data compression
- Optimize sensor processing pipelines

### 3. Validation and Testing
- Compare simulated vs. real sensor data
- Test sensor fusion algorithms
- Validate under various environmental conditions
- Test sensor failure scenarios

## Summary

Sensor simulation is a critical component of humanoid robot simulation, enabling safe testing and development of perception and control algorithms. Accurate simulation of various sensor types - cameras, LIDAR, IMU, force/torque sensors - is essential for effective sim-to-real transfer.

The key to successful sensor simulation lies in understanding the characteristics of real sensors and implementing appropriate noise models, calibration parameters, and processing pipelines. Multi-sensor fusion and real-time processing add additional complexity but are essential for humanoid robot operation.

In the next chapter, we'll explore environment building in Gazebo, focusing on creating realistic environments for humanoid robot testing and development.