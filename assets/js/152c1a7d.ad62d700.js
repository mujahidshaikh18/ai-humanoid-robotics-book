"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[951],{3472:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>t,metadata:()=>o,toc:()=>d});var a=r(4848),i=r(8453);const t={sidebar_position:12,title:"Chapter 12: Photorealistic Simulation & Synthetic Data"},s="Photorealistic Simulation & Synthetic Data",o={id:"module-3/chapter-12",title:"Chapter 12: Photorealistic Simulation & Synthetic Data",description:"Overview",source:"@site/docs/module-3/chapter-12.mdx",sourceDirName:"module-3",slug:"/module-3/chapter-12",permalink:"/ai-humanoid-robotics-book/docs/module-3/chapter-12",draft:!1,unlisted:!1,editUrl:"https://github.com/mujahidshaikh18/ai-humanoid-robotics-book/tree/main/docs/module-3/chapter-12.mdx",tags:[],version:"current",sidebarPosition:12,frontMatter:{sidebar_position:12,title:"Chapter 12: Photorealistic Simulation & Synthetic Data"},sidebar:"tutorialSidebar",previous:{title:"Chapter 11: Introduction to NVIDIA Isaac",permalink:"/ai-humanoid-robotics-book/docs/module-3/chapter-11"},next:{title:"Chapter 13: Isaac ROS: Hardware-Accelerated Robotics",permalink:"/ai-humanoid-robotics-book/docs/module-3/chapter-13"}},l={},d=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Photorealistic Rendering in Robotics",id:"photorealistic-rendering-in-robotics",level:2},{value:"Why Photorealistic Simulation Matters",id:"why-photorealistic-simulation-matters",level:3},{value:"Rendering Pipeline Components",id:"rendering-pipeline-components",level:3},{value:"Physically-Based Rendering (PBR)",id:"physically-based-rendering-pbr",level:3},{value:"Isaac Perception Package",id:"isaac-perception-package",level:2},{value:"Overview of Isaac Perception",id:"overview-of-isaac-perception",level:3},{value:"Setting Up Synthetic Data Generation",id:"setting-up-synthetic-data-generation",level:3},{value:"Domain Randomization Techniques",id:"domain-randomization-techniques",level:2},{value:"Principles of Domain Randomization",id:"principles-of-domain-randomization",level:3},{value:"Synthetic Data Formats and Labels",id:"synthetic-data-formats-and-labels",level:2},{value:"Data Format Standards",id:"data-format-standards",level:3},{value:"Advanced Sensor Simulation",id:"advanced-sensor-simulation",level:2},{value:"Camera Simulation with Realistic Effects",id:"camera-simulation-with-realistic-effects",level:3},{value:"Quality Assessment and Validation",id:"quality-assessment-and-validation",level:2},{value:"Evaluating Synthetic Data Quality",id:"evaluating-synthetic-data-quality",level:3},{value:"Integration with AI Training Pipelines",id:"integration-with-ai-training-pipelines",level:2},{value:"Synthetic Data Training Pipeline",id:"synthetic-data-training-pipeline",level:3},{value:"Best Practices for Synthetic Data Generation",id:"best-practices-for-synthetic-data-generation",level:2},{value:"1. Quality Control",id:"1-quality-control",level:3},{value:"2. Domain Randomization Strategy",id:"2-domain-randomization-strategy",level:3},{value:"3. Computational Efficiency",id:"3-computational-efficiency",level:3},{value:"4. Data Pipeline Management",id:"4-data-pipeline-management",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"Rendering Artifacts",id:"rendering-artifacts",level:3},{value:"Performance Issues",id:"performance-issues",level:3},{value:"Sim-to-Real Transfer Problems",id:"sim-to-real-transfer-problems",level:3},{value:"Summary",id:"summary",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h1,{id:"photorealistic-simulation--synthetic-data",children:"Photorealistic Simulation & Synthetic Data"}),"\n",(0,a.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(e.p,{children:"Photorealistic simulation and synthetic data generation are transformative technologies in robotics, particularly for humanoid robots that require extensive training on diverse visual data. NVIDIA Isaac's advanced rendering capabilities enable the creation of synthetic datasets that can rival or exceed real-world data quality, significantly accelerating AI model development while reducing real-world data collection costs."}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understand photorealistic rendering principles in robotics simulation"}),"\n",(0,a.jsx)(e.li,{children:"Generate synthetic datasets using Isaac's tools"}),"\n",(0,a.jsx)(e.li,{children:"Apply domain randomization techniques"}),"\n",(0,a.jsx)(e.li,{children:"Create labeled training data for computer vision models"}),"\n",(0,a.jsx)(e.li,{children:"Evaluate synthetic vs. real data quality"}),"\n",(0,a.jsx)(e.li,{children:"Integrate synthetic data into AI training pipelines"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"photorealistic-rendering-in-robotics",children:"Photorealistic Rendering in Robotics"}),"\n",(0,a.jsx)(e.h3,{id:"why-photorealistic-simulation-matters",children:"Why Photorealistic Simulation Matters"}),"\n",(0,a.jsx)(e.p,{children:"Photorealistic simulation is crucial for humanoid robots because:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Visual Perception"}),": Humanoid robots rely heavily on visual data for navigation and interaction"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Training Efficiency"}),": Synthetic data can be generated rapidly with perfect labels"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Edge Case Coverage"}),": Simulation can generate rare or dangerous scenarios safely"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Cost Reduction"}),": Eliminates need for extensive real-world data collection"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Consistency"}),": Controlled environments ensure reproducible results"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"rendering-pipeline-components",children:"Rendering Pipeline Components"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   3D Scene      \u2502    \u2502   Lighting      \u2502    \u2502   Materials     \u2502\r\n\u2502   Description   \u2502\u2500\u2500\u2500\u25b6\u2502   Simulation    \u2502\u2500\u2500\u2500\u25b6\u2502   & Textures    \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n         \u2502                       \u2502                       \u2502\r\n         \u25bc                       \u25bc                       \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                    Physically-Based Rendering                   \u2502\r\n\u2502         (Global Illumination, Shadows, Reflections)            \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n         \u2502\r\n         \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   Sensor        \u2502    \u2502   Post-         \u2502    \u2502   Output        \u2502\r\n\u2502   Simulation    \u2502\u2500\u2500\u2500\u25b6\u2502   Processing    \u2502\u2500\u2500\u2500\u25b6\u2502   Formats       \u2502\r\n\u2502   (Cameras,     \u2502    \u2502   (Noise,      \u2502    \u2502   (RGB, Depth,  \u2502\r\n\u2502   LIDAR, etc.)  \u2502    \u2502   Distortion)   \u2502    \u2502   Semantic)     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,a.jsx)(e.h3,{id:"physically-based-rendering-pbr",children:"Physically-Based Rendering (PBR)"}),"\n",(0,a.jsx)(e.p,{children:"PBR ensures that materials and lighting behave realistically:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Example PBR material setup in Isaac Sim\r\nimport omni\r\nfrom pxr import Gf, Sdf, UsdShade\r\nfrom omni.isaac.core.utils.prims import create_prim\r\nfrom omni.isaac.core.utils.materials import create_preview_surface\r\n\r\ndef setup_pbr_material(prim_path, albedo_color, roughness=0.5, metallic=0.0):\r\n    """Create physically-based material with realistic properties"""\r\n\r\n    # Create material prim\r\n    material_path = f"{prim_path}/material"\r\n    create_prim(\r\n        prim_path=material_path,\r\n        prim_type="Material"\r\n    )\r\n\r\n    # Create PBR surface shader\r\n    shader_path = f"{material_path}/Shader"\r\n    shader = create_preview_surface(\r\n        prim_path=shader_path,\r\n        color=albedo_color,\r\n        roughness=roughness,\r\n        metallic=metallic\r\n    )\r\n\r\n    return shader\r\n\r\ndef apply_realistic_materials(robot_prim_path):\r\n    """Apply realistic materials to robot parts"""\r\n\r\n    # Head material (metallic surface)\r\n    head_material = setup_pbr_material(\r\n        f"{robot_prim_path}/head",\r\n        albedo_color=(0.7, 0.7, 0.8),  # Light gray metallic\r\n        roughness=0.2,\r\n        metallic=0.8\r\n    )\r\n\r\n    # Torso material (matte plastic)\r\n    torso_material = setup_pbr_material(\r\n        f"{robot_prim_path}/torso",\r\n        albedo_color=(0.3, 0.3, 0.8),  # Blue plastic\r\n        roughness=0.7,\r\n        metallic=0.0\r\n    )\r\n\r\n    # Hand material (rubber-like)\r\n    hand_material = setup_pbr_material(\r\n        f"{robot_prim_path}/hand",\r\n        albedo_color=(0.2, 0.2, 0.2),  # Black rubber\r\n        roughness=0.9,\r\n        metallic=0.1\r\n    )\n'})}),"\n",(0,a.jsx)(e.h2,{id:"isaac-perception-package",children:"Isaac Perception Package"}),"\n",(0,a.jsx)(e.h3,{id:"overview-of-isaac-perception",children:"Overview of Isaac Perception"}),"\n",(0,a.jsx)(e.p,{children:"The Isaac Perception package provides tools for synthetic data generation:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Synthetic Data Generation"}),": Create labeled datasets with perfect annotations"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Domain Randomization"}),": Randomize environments to improve model robustness"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Sensor Simulation"}),": Accurate simulation of cameras, LIDAR, and other sensors"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Annotation Tools"}),": Automatic generation of ground truth labels"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"setting-up-synthetic-data-generation",children:"Setting Up Synthetic Data Generation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import omni\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\r\nfrom omni.synthetic_utils.scripts import Annotators\r\nimport numpy as np\r\n\r\nclass SyntheticDataGenerator:\r\n    def __init__(self):\r\n        self.world = World(stage_units_in_meters=1.0)\r\n        self.setup_scene()\r\n        self.setup_synthetic_data_pipeline()\r\n\r\n    def setup_scene(self):\r\n        """Set up the simulation scene"""\r\n        # Add ground plane\r\n        self.world.scene.add_default_ground_plane()\r\n\r\n        # Add lighting\r\n        self.setup_lighting()\r\n\r\n        # Add objects for data generation\r\n        self.add_random_objects()\r\n\r\n    def setup_lighting(self):\r\n        """Set up realistic lighting conditions"""\r\n        # Create dome light for even illumination\r\n        dome_light = omni.kit.commands.execute(\r\n            "CreateDomeLightCommand",\r\n            position=(0, 0, 10),\r\n            name="dome_light"\r\n        )\r\n\r\n        # Add directional light for shadows\r\n        sun_light = omni.kit.commands.execute(\r\n            "CreateDistantLightCommand",\r\n            position=(10, 10, 10),\r\n            direction=(-1, -1, -1),\r\n            name="sun_light"\r\n        )\r\n\r\n    def add_random_objects(self):\r\n        """Add random objects for diverse training data"""\r\n        object_types = [\r\n            "cube", "sphere", "cylinder", "cone", "torus"\r\n        ]\r\n\r\n        for i in range(20):  # Add 20 random objects\r\n            obj_type = np.random.choice(object_types)\r\n            position = (\r\n                np.random.uniform(-5, 5),\r\n                np.random.uniform(-5, 5),\r\n                np.random.uniform(0.5, 2.0)\r\n            )\r\n\r\n            # Add object to scene based on type\r\n            self.add_object(obj_type, position, i)\r\n\r\n    def setup_synthetic_data_pipeline(self):\r\n        """Set up the synthetic data generation pipeline"""\r\n        # Initialize synthetic data helper\r\n        self.sd_helper = SyntheticDataHelper()\r\n\r\n        # Enable required annotators\r\n        self.enable_annotators()\r\n\r\n    def enable_annotators(self):\r\n        """Enable various annotation types"""\r\n        # RGB camera data\r\n        Annotators.ENABLED_ANNOTATORS.add("rgb")\r\n\r\n        # Depth data\r\n        Annotators.ENABLED_ANNOTATORS.add("depth")\r\n\r\n        # Semantic segmentation\r\n        Annotators.ENABLED_ANNOTATORS.add("semantic_segmentation")\r\n\r\n        # Instance segmentation\r\n        Annotators.ENABLED_ANNOTATORS.add("instance_segmentation")\r\n\r\n        # Bounding boxes\r\n        Annotators.ENABLED_ANNOTATORS.add("bounding_box_2d_tight")\r\n\r\n        # Object poses\r\n        Annotators.ENABLED_ANNOTATORS.add("pose")\r\n\r\n    def generate_dataset(self, num_samples=1000):\r\n        """Generate synthetic dataset"""\r\n        dataset = {\r\n            \'rgb\': [],\r\n            \'depth\': [],\r\n            \'semantic\': [],\r\n            \'instances\': [],\r\n            \'boxes\': [],\r\n            \'poses\': []\r\n        }\r\n\r\n        for i in range(num_samples):\r\n            # Randomize scene\r\n            self.randomize_scene()\r\n\r\n            # Step simulation\r\n            self.world.step(render=True)\r\n\r\n            # Capture synthetic data\r\n            sample = self.capture_synthetic_data()\r\n            for key, value in sample.items():\r\n                dataset[key].append(value)\r\n\r\n            print(f"Generated sample {i+1}/{num_samples}")\r\n\r\n        return dataset\r\n\r\n    def randomize_scene(self):\r\n        """Apply domain randomization to scene"""\r\n        # Randomize lighting\r\n        self.randomize_lighting()\r\n\r\n        # Randomize object positions\r\n        self.randomize_object_positions()\r\n\r\n        # Randomize materials\r\n        self.randomize_materials()\r\n\r\n        # Randomize camera parameters\r\n        self.randomize_camera()\r\n\r\n    def randomize_lighting(self):\r\n        """Randomize lighting conditions"""\r\n        # Randomize dome light intensity\r\n        dome_intensity = np.random.uniform(0.5, 2.0)\r\n\r\n        # Randomize sun light direction\r\n        sun_direction = (\r\n            np.random.uniform(-1, 1),\r\n            np.random.uniform(-1, 1),\r\n            np.random.uniform(-1, 0)\r\n        )\r\n\r\n    def randomize_object_positions(self):\r\n        """Randomize object positions"""\r\n        # Move objects to new positions\r\n        pass\r\n\r\n    def randomize_materials(self):\r\n        """Randomize material properties"""\r\n        # Randomize colors, textures, and surface properties\r\n        pass\r\n\r\n    def randomize_camera(self):\r\n        """Randomize camera parameters"""\r\n        # Randomize camera position, orientation, and intrinsics\r\n        pass\r\n\r\n    def capture_synthetic_data(self):\r\n        """Capture all synthetic data for current frame"""\r\n        # This would interface with Isaac\'s annotators\r\n        # to capture RGB, depth, semantic, etc.\r\n        sample = {\r\n            \'rgb\': np.random.rand(480, 640, 3),  # Placeholder\r\n            \'depth\': np.random.rand(480, 640),   # Placeholder\r\n            \'semantic\': np.random.randint(0, 10, (480, 640)),  # Placeholder\r\n            \'instances\': np.random.randint(0, 20, (480, 640)), # Placeholder\r\n            \'boxes\': [],  # Placeholder\r\n            \'poses\': {}   # Placeholder\r\n        }\r\n        return sample\n'})}),"\n",(0,a.jsx)(e.h2,{id:"domain-randomization-techniques",children:"Domain Randomization Techniques"}),"\n",(0,a.jsx)(e.h3,{id:"principles-of-domain-randomization",children:"Principles of Domain Randomization"}),"\n",(0,a.jsx)(e.p,{children:"Domain randomization helps bridge the sim-to-real gap by training models on diverse synthetic data:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import numpy as np\r\nimport random\r\n\r\nclass DomainRandomization:\r\n    def __init__(self):\r\n        self.randomization_params = self.setup_randomization_params()\r\n\r\n    def setup_randomization_params(self):\r\n        \"\"\"Define randomization parameter ranges\"\"\"\r\n        return {\r\n            'lighting': {\r\n                'intensity_range': (0.3, 2.0),\r\n                'color_temperature_range': (3000, 8000),  # Kelvin\r\n                'direction_variance': 0.5\r\n            },\r\n            'materials': {\r\n                'albedo_range': (0.1, 1.0),\r\n                'roughness_range': (0.0, 1.0),\r\n                'metallic_range': (0.0, 1.0),\r\n                'specular_range': (0.0, 1.0)\r\n            },\r\n            'textures': {\r\n                'scale_range': (0.1, 5.0),\r\n                'rotation_range': (0, 360),\r\n                'distortion_range': (0.0, 0.1)\r\n            },\r\n            'camera': {\r\n                'fov_range': (30, 90),  # degrees\r\n                'position_variance': 0.1,\r\n                'orientation_variance': 0.1\r\n            },\r\n            'backgrounds': {\r\n                'complexity_range': (0, 10),  # Number of background objects\r\n                'clutter_range': (0, 20),     # Number of clutter objects\r\n                'pattern_range': (0, 5)       # Different background patterns\r\n            }\r\n        }\r\n\r\n    def randomize_lighting(self):\r\n        \"\"\"Apply random lighting conditions\"\"\"\r\n        params = self.randomization_params['lighting']\r\n\r\n        # Random intensity\r\n        intensity = np.random.uniform(*params['intensity_range'])\r\n\r\n        # Random color temperature (converted to RGB)\r\n        color_temp = np.random.uniform(*params['color_temperature_range'])\r\n        color_rgb = self.color_temperature_to_rgb(color_temp)\r\n\r\n        # Random direction variance\r\n        direction_variance = params['direction_variance']\r\n\r\n        return {\r\n            'intensity': intensity,\r\n            'color': color_rgb,\r\n            'direction_variance': direction_variance\r\n        }\r\n\r\n    def randomize_materials(self):\r\n        \"\"\"Apply random material properties\"\"\"\r\n        params = self.randomization_params['materials']\r\n\r\n        material_properties = {\r\n            'albedo': np.random.uniform(*params['albedo_range'], 3),\r\n            'roughness': np.random.uniform(*params['roughness_range']),\r\n            'metallic': np.random.uniform(*params['metallic_range']),\r\n            'specular': np.random.uniform(*params['specular_range'])\r\n        }\r\n\r\n        return material_properties\r\n\r\n    def randomize_camera(self):\r\n        \"\"\"Apply random camera parameters\"\"\"\r\n        params = self.randomization_params['camera']\r\n\r\n        camera_params = {\r\n            'fov': np.random.uniform(*params['fov_range']),\r\n            'position_variance': np.random.uniform(-params['position_variance'],\r\n                                                 params['position_variance'], 3),\r\n            'orientation_variance': np.random.uniform(-params['orientation_variance'],\r\n                                                     params['orientation_variance'], 3)\r\n        }\r\n\r\n        return camera_params\r\n\r\n    def color_temperature_to_rgb(self, color_temp):\r\n        \"\"\"Convert color temperature to RGB (simplified)\"\"\"\r\n        # This is a simplified conversion - real implementation would be more complex\r\n        temp = color_temp / 100\r\n        if temp <= 66:\r\n            red = 255\r\n            green = temp\r\n            green = 99.4708025861 * np.log(green) - 161.1195681661\r\n        else:\r\n            red = temp - 60\r\n            red = 329.698727446 * (red ** -0.1332047592)\r\n            green = temp - 60\r\n            green = 288.1221695283 * (green ** -0.0755148492)\r\n\r\n        blue = 255 if temp >= 66 else temp - 10\r\n        blue = 0 if temp < 19 else 138.5177312231 * np.log(blue) - 305.0447927307\r\n\r\n        # Clamp values to [0, 255]\r\n        red = np.clip(red, 0, 255) / 255.0\r\n        green = np.clip(green, 0, 255) / 255.0\r\n        blue = np.clip(blue, 0, 255) / 255.0\r\n\r\n        return (red, green, blue)\r\n\r\n    def apply_randomization(self, scene_object):\r\n        \"\"\"Apply domain randomization to a scene object\"\"\"\r\n        # Randomize material\r\n        material_params = self.randomize_materials()\r\n        self.apply_material_properties(scene_object, material_params)\r\n\r\n        # Randomize position and orientation\r\n        self.randomize_object_pose(scene_object)\r\n\r\n        # Add texture variations\r\n        self.add_texture_variations(scene_object)\r\n\r\n    def apply_material_properties(self, obj, material_params):\r\n        \"\"\"Apply material properties to object\"\"\"\r\n        # This would interface with the rendering engine\r\n        # to apply the randomized material properties\r\n        pass\r\n\r\n    def randomize_object_pose(self, obj):\r\n        \"\"\"Randomize object position and orientation\"\"\"\r\n        # Apply random translation\r\n        translation = np.random.uniform(-0.5, 0.5, 3)\r\n\r\n        # Apply random rotation\r\n        rotation = np.random.uniform(-15, 15, 3)  # degrees\r\n\r\n        # Apply transformations to object\r\n        pass\r\n\r\n    def add_texture_variations(self, obj):\r\n        \"\"\"Add texture variations to object\"\"\"\r\n        # Apply random texture scaling, rotation, etc.\r\n        pass\n"})}),"\n",(0,a.jsx)(e.h2,{id:"synthetic-data-formats-and-labels",children:"Synthetic Data Formats and Labels"}),"\n",(0,a.jsx)(e.h3,{id:"data-format-standards",children:"Data Format Standards"}),"\n",(0,a.jsx)(e.p,{children:"Synthetic data should follow standard formats for compatibility:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import json\r\nimport numpy as np\r\nimport cv2\r\nfrom dataclasses import dataclass\r\nfrom typing import List, Dict, Any\r\n\r\n@dataclass\r\nclass BoundingBox:\r\n    """Bounding box annotation"""\r\n    x_min: float\r\n    y_min: float\r\n    x_max: float\r\n    y_max: float\r\n    class_id: int\r\n    class_name: str\r\n    confidence: float = 1.0\r\n\r\n@dataclass\r\nclass InstanceMask:\r\n    """Instance segmentation mask"""\r\n    mask: np.ndarray  # Binary mask\r\n    class_id: int\r\n    instance_id: int\r\n    bbox: BoundingBox\r\n\r\n@dataclass\r\nclass CameraIntrinsics:\r\n    """Camera intrinsic parameters"""\r\n    fx: float  # Focal length x\r\n    fy: float  # Focal length y\r\n    cx: float  # Principal point x\r\n    cy: float  # Principal point y\r\n    width: int\r\n    height: int\r\n    distortion_coeffs: List[float]\r\n\r\nclass SyntheticDataFormatter:\r\n    def __init__(self):\r\n        self.label_map = self.create_label_map()\r\n\r\n    def create_label_map(self):\r\n        """Create mapping from class IDs to names"""\r\n        return {\r\n            0: \'background\',\r\n            1: \'humanoid_robot\',\r\n            2: \'table\',\r\n            3: \'chair\',\r\n            4: \'obstacle\',\r\n            5: \'target_object\',\r\n            # Add more classes as needed\r\n        }\r\n\r\n    def format_for_coco(self, sample_data):\r\n        """Format synthetic data in COCO format"""\r\n        coco_format = {\r\n            "info": {\r\n                "year": 2024,\r\n                "version": "1.0",\r\n                "description": "Synthetic Humanoid Robot Dataset",\r\n                "contributor": "Isaac Synthetic Data Generator",\r\n                "url": "",\r\n                "date_created": "2024-01-01"\r\n            },\r\n            "licenses": [\r\n                {\r\n                    "id": 1,\r\n                    "name": "Synthetic Data License",\r\n                    "url": ""\r\n                }\r\n            ],\r\n            "images": [],\r\n            "annotations": [],\r\n            "categories": []\r\n        }\r\n\r\n        # Add categories\r\n        for class_id, class_name in self.label_map.items():\r\n            if class_id > 0:  # Skip background\r\n                coco_format["categories"].append({\r\n                    "id": class_id,\r\n                    "name": class_name,\r\n                    "supercategory": "object"\r\n                })\r\n\r\n        # Add image and annotations\r\n        image_info = {\r\n            "id": 1,\r\n            "width": sample_data[\'rgb\'].shape[1],\r\n            "height": sample_data[\'rgb\'].shape[0],\r\n            "file_name": "synthetic_000001.png",\r\n            "license": 1,\r\n            "flickr_url": "",\r\n            "coco_url": "",\r\n            "date_captured": "2024-01-01 00:00:00"\r\n        }\r\n        coco_format["images"].append(image_info)\r\n\r\n        # Add annotations\r\n        for i, bbox in enumerate(sample_data[\'bboxes\']):\r\n            annotation = {\r\n                "id": i + 1,\r\n                "image_id": 1,\r\n                "category_id": bbox.class_id,\r\n                "bbox": [\r\n                    bbox.x_min,\r\n                    bbox.y_min,\r\n                    bbox.x_max - bbox.x_min,\r\n                    bbox.y_max - bbox.y_min\r\n                ],\r\n                "area": (bbox.x_max - bbox.x_min) * (bbox.y_max - bbox.y_min),\r\n                "iscrowd": 0\r\n            }\r\n            coco_format["annotations"].append(annotation)\r\n\r\n        return coco_format\r\n\r\n    def format_for_yolo(self, sample_data, output_dir):\r\n        """Format synthetic data in YOLO format"""\r\n        # Create label file\r\n        label_file = f"{output_dir}/labels/synthetic_000001.txt"\r\n\r\n        with open(label_file, \'w\') as f:\r\n            for bbox in sample_data[\'bboxes\']:\r\n                # YOLO format: class_id center_x center_y width height (normalized)\r\n                img_width = sample_data[\'rgb\'].shape[1]\r\n                img_height = sample_data[\'rgb\'].shape[0]\r\n\r\n                center_x = (bbox.x_min + bbox.x_max) / 2.0 / img_width\r\n                center_y = (bbox.y_min + bbox.y_max) / 2.0 / img_height\r\n                width = (bbox.x_max - bbox.x_min) / img_width\r\n                height = (bbox.y_max - bbox.y_min) / img_height\r\n\r\n                f.write(f"{bbox.class_id} {center_x} {center_y} {width} {height}\\n")\r\n\r\n    def format_for_depth_estimation(self, sample_data, output_dir):\r\n        """Format depth data for depth estimation training"""\r\n        # Save depth map\r\n        depth_path = f"{output_dir}/depth/synthetic_000001_depth.png"\r\n        # Normalize depth to 16-bit for PNG storage\r\n        normalized_depth = (sample_data[\'depth\'] / np.max(sample_data[\'depth\']) * 65535).astype(np.uint16)\r\n        cv2.imwrite(depth_path, normalized_depth)\r\n\r\n        # Save corresponding RGB image\r\n        rgb_path = f"{output_dir}/rgb/synthetic_000001_rgb.png"\r\n        cv2.imwrite(rgb_path, cv2.cvtColor(sample_data[\'rgb\'], cv2.COLOR_RGB2BGR))\r\n\r\n    def format_for_pose_estimation(self, sample_data, output_dir):\r\n        """Format data for 6D pose estimation"""\r\n        pose_file = f"{output_dir}/poses/synthetic_000001_pose.json"\r\n\r\n        pose_data = {\r\n            "object_poses": [],\r\n            "camera_intrinsics": sample_data.get(\'camera_intrinsics\', None)\r\n        }\r\n\r\n        for obj_id, pose in sample_data[\'poses\'].items():\r\n            pose_data["object_poses"].append({\r\n                "object_id": obj_id,\r\n                "rotation_matrix": pose[\'rotation\'].tolist(),\r\n                "translation_vector": pose[\'translation\'].tolist(),\r\n                "bounding_box": pose.get(\'bbox\', None)\r\n            })\r\n\r\n        with open(pose_file, \'w\') as f:\r\n            json.dump(pose_data, f, indent=2)\n'})}),"\n",(0,a.jsx)(e.h2,{id:"advanced-sensor-simulation",children:"Advanced Sensor Simulation"}),"\n",(0,a.jsx)(e.h3,{id:"camera-simulation-with-realistic-effects",children:"Camera Simulation with Realistic Effects"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import numpy as np\r\nimport cv2\r\nfrom scipy import ndimage\r\n\r\nclass RealisticCameraSimulator:\r\n    def __init__(self, width=640, height=480, fov=60):\r\n        self.width = width\r\n        self.height = height\r\n        self.fov = fov\r\n        self.intrinsics = self.compute_intrinsics()\r\n\r\n    def compute_intrinsics(self):\r\n        """Compute camera intrinsic matrix"""\r\n        f = 0.5 * self.width / np.tan(0.5 * np.radians(self.fov))\r\n        cx = self.width / 2\r\n        cy = self.height / 2\r\n\r\n        return np.array([\r\n            [f, 0, cx],\r\n            [0, f, cy],\r\n            [0, 0, 1]\r\n        ])\r\n\r\n    def add_lens_distortion(self, image):\r\n        """Add realistic lens distortion"""\r\n        # Distortion coefficients (typical values for wide-angle lens)\r\n        k1, k2, p1, p2, k3 = -0.1, 0.05, 0.001, 0.001, -0.01\r\n\r\n        # Get image dimensions\r\n        h, w = image.shape[:2]\r\n\r\n        # Create coordinate grid\r\n        x = np.linspace(-1, 1, w)\r\n        y = np.linspace(-1, 1, h)\r\n        x_grid, y_grid = np.meshgrid(x, y)\r\n\r\n        # Apply radial distortion\r\n        r_squared = x_grid**2 + y_grid**2\r\n        radial_distortion = 1 + k1*r_squared + k2*r_squared**2 + k3*r_squared**3\r\n\r\n        # Apply tangential distortion\r\n        x_distorted = x_grid * radial_distortion + 2*p1*x_grid*y_grid + p2*(r_squared + 2*x_grid**2)\r\n        y_distorted = y_grid * radial_distortion + p1*(r_squared + 2*y_grid**2) + 2*p2*x_grid*y_grid\r\n\r\n        # Normalize back to pixel coordinates\r\n        x_pixels = ((x_distorted + 1) * 0.5 * (w - 1)).astype(np.float32)\r\n        y_pixels = ((y_distorted + 1) * 0.5 * (h - 1)).astype(np.float32)\r\n\r\n        # Remap image\r\n        map_x = np.clip(x_pixels, 0, w-1).astype(np.float32)\r\n        map_y = np.clip(y_pixels, 0, h-1).astype(np.float32)\r\n\r\n        distorted_image = cv2.remap(image, map_x, map_y, interpolation=cv2.INTER_LINEAR)\r\n\r\n        return distorted_image\r\n\r\n    def add_sensor_noise(self, image, iso=100):\r\n        """Add realistic sensor noise"""\r\n        # Convert to float in [0, 1]\r\n        img_float = image.astype(np.float32) / 255.0\r\n\r\n        # Shot noise (photon noise) - proportional to signal\r\n        shot_noise_std = np.sqrt(img_float * iso / 100 * 0.01)\r\n        shot_noise = np.random.normal(0, shot_noise_std, img_float.shape)\r\n\r\n        # Read noise (constant) - independent of signal\r\n        read_noise_std = 0.005 * (iso / 100)\r\n        read_noise = np.random.normal(0, read_noise_std, img_float.shape)\r\n\r\n        # Combine noises\r\n        total_noise = shot_noise + read_noise\r\n\r\n        # Add noise to image\r\n        noisy_img = np.clip(img_float + total_noise, 0, 1)\r\n\r\n        # Convert back to uint8\r\n        return (noisy_img * 255).astype(np.uint8)\r\n\r\n    def add_motion_blur(self, image, motion_vector=(0.1, 0.05)):\r\n        """Add motion blur based on camera/object motion"""\r\n        # Create motion blur kernel\r\n        size = max(5, int(np.sqrt(motion_vector[0]**2 + motion_vector[1]**2) * 50))\r\n        if size < 3:\r\n            return image\r\n\r\n        kernel = np.zeros((size, size))\r\n        kernel[size//2, :] = 1.0 / size  # Horizontal motion blur\r\n\r\n        # Apply motion blur\r\n        blurred = cv2.filter2D(image, -1, kernel)\r\n        return blurred\r\n\r\n    def simulate_camera_effects(self, rgb_image):\r\n        """Apply all camera effects to RGB image"""\r\n        # Add lens distortion\r\n        distorted = self.add_lens_distortion(rgb_image)\r\n\r\n        # Add sensor noise\r\n        noisy = self.add_sensor_noise(distorted)\r\n\r\n        # Add motion blur (if applicable)\r\n        motion_blurred = self.add_motion_blur(noisy)\r\n\r\n        return motion_blurred\n'})}),"\n",(0,a.jsx)(e.h2,{id:"quality-assessment-and-validation",children:"Quality Assessment and Validation"}),"\n",(0,a.jsx)(e.h3,{id:"evaluating-synthetic-data-quality",children:"Evaluating Synthetic Data Quality"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import numpy as np\r\nfrom scipy import stats\r\nimport matplotlib.pyplot as plt\r\n\r\nclass SyntheticDataQualityAssessment:\r\n    def __init__(self):\r\n        self.metrics = {}\r\n\r\n    def assess_visual_quality(self, synthetic_img, real_img):\r\n        """Assess visual quality of synthetic vs real images"""\r\n        # Calculate various image quality metrics\r\n\r\n        # Structural Similarity Index (SSIM) - conceptual implementation\r\n        ssim_score = self.calculate_ssim(synthetic_img, real_img)\r\n\r\n        # Peak Signal-to-Noise Ratio (PSNR)\r\n        psnr_score = self.calculate_psnr(synthetic_img, real_img)\r\n\r\n        # Naturalness Image Quality Evaluator (NIQE) - simplified\r\n        niqe_score = self.estimate_niqe(synthetic_img)\r\n\r\n        self.metrics[\'visual_quality\'] = {\r\n            \'ssim\': ssim_score,\r\n            \'psnr\': psnr_score,\r\n            \'niqe\': niqe_score\r\n        }\r\n\r\n        return self.metrics[\'visual_quality\']\r\n\r\n    def calculate_ssim(self, img1, img2):\r\n        """Calculate Structural Similarity Index"""\r\n        # Simplified SSIM calculation\r\n        # In practice, use scikit-image or similar\r\n        mean1 = np.mean(img1.astype(float))\r\n        mean2 = np.mean(img2.astype(float))\r\n\r\n        std1 = np.std(img1.astype(float))\r\n        std2 = np.std(img2.astype(float))\r\n\r\n        covariance = np.mean((img1.astype(float) - mean1) * (img2.astype(float) - mean2))\r\n\r\n        c1 = (0.01 * 255) ** 2\r\n        c2 = (0.03 * 255) ** 2\r\n\r\n        ssim = ((2 * mean1 * mean2 + c1) * (2 * covariance + c2)) / \\\r\n               ((mean1**2 + mean2**2 + c1) * (std1**2 + std2**2 + c2))\r\n\r\n        return ssim\r\n\r\n    def calculate_psnr(self, img1, img2):\r\n        """Calculate Peak Signal-to-Noise Ratio"""\r\n        mse = np.mean((img1.astype(float) - img2.astype(float)) ** 2)\r\n        if mse == 0:\r\n            return float(\'inf\')\r\n\r\n        max_pixel = 255.0\r\n        psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\r\n        return psnr\r\n\r\n    def estimate_niqe(self, img):\r\n        """Estimate Naturalness Image Quality Evaluator score"""\r\n        # Simplified NIQE estimation\r\n        # Real implementation would use more sophisticated features\r\n        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) if len(img.shape) == 3 else img\r\n\r\n        # Calculate gradient magnitude as a proxy for naturalness\r\n        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\r\n        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\r\n        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)\r\n\r\n        # A more natural image would have gradient distribution\r\n        # closer to natural image statistics\r\n        grad_hist, _ = np.histogram(gradient_magnitude.ravel(), bins=256)\r\n        grad_hist = grad_hist / np.sum(grad_hist)  # Normalize\r\n\r\n        # Simplified score (lower is better for NIQE)\r\n        niqe_score = np.std(gradient_magnitude)\r\n        return niqe_score\r\n\r\n    def assess_label_quality(self, synthetic_labels, real_labels):\r\n        """Assess quality of synthetic labels vs real labels"""\r\n        # Compare label distributions\r\n        synthetic_hist = self.compute_label_histogram(synthetic_labels)\r\n        real_hist = self.compute_label_histogram(real_labels)\r\n\r\n        # Calculate histogram intersection\r\n        intersection = np.sum(np.minimum(synthetic_hist, real_hist))\r\n        union = np.sum(np.maximum(synthetic_hist, real_hist))\r\n\r\n        label_quality = intersection / union if union > 0 else 0\r\n\r\n        self.metrics[\'label_quality\'] = {\r\n            \'distribution_similarity\': label_quality,\r\n            \'synthetic_histogram\': synthetic_hist,\r\n            \'real_histogram\': real_hist\r\n        }\r\n\r\n        return self.metrics[\'label_quality\']\r\n\r\n    def compute_label_histogram(self, labels):\r\n        """Compute histogram of label classes"""\r\n        if isinstance(labels, (list, np.ndarray)):\r\n            unique, counts = np.unique(labels, return_counts=True)\r\n            histogram = np.zeros(int(np.max(unique)) + 1)\r\n            histogram[unique.astype(int)] = counts\r\n        else:\r\n            # Assume it\'s already a histogram\r\n            histogram = labels\r\n\r\n        return histogram\r\n\r\n    def assess_diversity(self, synthetic_dataset):\r\n        """Assess diversity of synthetic dataset"""\r\n        # Calculate diversity metrics\r\n        feature_vectors = self.extract_features(synthetic_dataset)\r\n\r\n        # Calculate pairwise distances\r\n        distances = self.calculate_pairwise_distances(feature_vectors)\r\n\r\n        # Diversity is related to average distance between samples\r\n        diversity_score = np.mean(distances)\r\n\r\n        self.metrics[\'diversity\'] = {\r\n            \'average_pairwise_distance\': diversity_score,\r\n            \'num_samples\': len(synthetic_dataset),\r\n            \'feature_space_coverage\': self.estimate_coverage(feature_vectors)\r\n        }\r\n\r\n        return self.metrics[\'diversity\']\r\n\r\n    def extract_features(self, dataset):\r\n        """Extract features for diversity assessment"""\r\n        # This would extract meaningful features from the dataset\r\n        # For images, this could be CNN features, color histograms, etc.\r\n        features = []\r\n\r\n        for sample in dataset:\r\n            # Extract simple features (in practice, use more sophisticated features)\r\n            rgb_mean = np.mean(sample[\'rgb\'], axis=(0,1))\r\n            rgb_std = np.std(sample[\'rgb\'], axis=(0,1))\r\n            texture_features = self.extract_texture_features(sample[\'rgb\'])\r\n\r\n            feature_vector = np.concatenate([rgb_mean, rgb_std, texture_features])\r\n            features.append(feature_vector)\r\n\r\n        return np.array(features)\r\n\r\n    def extract_texture_features(self, image):\r\n        """Extract simple texture features"""\r\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\r\n\r\n        # Use local binary patterns or other texture descriptors\r\n        # Simplified version using gradient statistics\r\n        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\r\n        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\r\n\r\n        texture_features = [\r\n            np.mean(np.abs(grad_x)),\r\n            np.mean(np.abs(grad_y)),\r\n            np.std(grad_x),\r\n            np.std(grad_y)\r\n        ]\r\n\r\n        return np.array(texture_features)\r\n\r\n    def calculate_pairwise_distances(self, features):\r\n        """Calculate pairwise distances between feature vectors"""\r\n        n_samples = len(features)\r\n        distances = np.zeros((n_samples, n_samples))\r\n\r\n        for i in range(n_samples):\r\n            for j in range(i+1, n_samples):\r\n                dist = np.linalg.norm(features[i] - features[j])\r\n                distances[i, j] = dist\r\n                distances[j, i] = dist  # Symmetric matrix\r\n\r\n        return distances[np.triu_indices_from(distances, k=1)]\r\n\r\n    def estimate_coverage(self, features):\r\n        """Estimate feature space coverage"""\r\n        # Calculate volume of feature space covered\r\n        feature_ranges = np.max(features, axis=0) - np.min(features, axis=0)\r\n        coverage = np.prod(feature_ranges)  # Simplified coverage metric\r\n\r\n        return coverage\r\n\r\n    def generate_quality_report(self):\r\n        """Generate comprehensive quality assessment report"""\r\n        report = {\r\n            \'overall_quality_score\': self.compute_overall_score(),\r\n            \'detailed_metrics\': self.metrics,\r\n            \'recommendations\': self.generate_recommendations()\r\n        }\r\n\r\n        return report\r\n\r\n    def compute_overall_score(self):\r\n        """Compute overall quality score"""\r\n        if not self.metrics:\r\n            return 0.0\r\n\r\n        scores = []\r\n        if \'visual_quality\' in self.metrics:\r\n            # Normalize and weight visual quality metrics\r\n            ssim = self.metrics[\'visual_quality\'][\'ssim\']\r\n            psnr = self.metrics[\'visual_quality\'][\'psnr\'] / 50  # Normalize PSNR\r\n            scores.append((ssim + min(psnr, 1.0)) / 2)\r\n\r\n        if \'label_quality\' in self.metrics:\r\n            scores.append(self.metrics[\'label_quality\'][\'distribution_similarity\'])\r\n\r\n        if \'diversity\' in self.metrics:\r\n            # Normalize diversity score\r\n            diversity = min(self.metrics[\'diversity\'][\'average_pairwise_distance\'] / 100, 1.0)\r\n            scores.append(diversity)\r\n\r\n        return np.mean(scores) if scores else 0.0\r\n\r\n    def generate_recommendations(self):\r\n        """Generate recommendations based on quality assessment"""\r\n        recommendations = []\r\n\r\n        if \'visual_quality\' in self.metrics:\r\n            if self.metrics[\'visual_quality\'][\'ssim\'] < 0.5:\r\n                recommendations.append("Consider improving visual realism through better materials and lighting")\r\n            if self.metrics[\'visual_quality\'][\'psnr\'] < 30:\r\n                recommendations.append("Reduce noise artifacts in synthetic images")\r\n\r\n        if \'label_quality\' in self.metrics:\r\n            if self.metrics[\'label_quality\'][\'distribution_similarity\'] < 0.7:\r\n                recommendations.append("Adjust synthetic data generation to better match real data distribution")\r\n\r\n        if \'diversity\' in self.metrics:\r\n            if self.metrics[\'diversity\'][\'feature_space_coverage\'] < 0.3:\r\n                recommendations.append("Increase domain randomization to improve dataset diversity")\r\n\r\n        return recommendations\n'})}),"\n",(0,a.jsx)(e.h2,{id:"integration-with-ai-training-pipelines",children:"Integration with AI Training Pipelines"}),"\n",(0,a.jsx)(e.h3,{id:"synthetic-data-training-pipeline",children:"Synthetic Data Training Pipeline"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\nimport torchvision.transforms as transforms\r\nfrom torch.utils.data import Dataset, DataLoader\r\nimport os\r\nimport json\r\nfrom PIL import Image\r\n\r\nclass SyntheticRoboticsDataset(Dataset):\r\n    def __init__(self, data_dir, transform=None, task='classification'):\r\n        self.data_dir = data_dir\r\n        self.transform = transform\r\n        self.task = task\r\n\r\n        # Load annotations\r\n        self.annotations = self.load_annotations()\r\n\r\n    def load_annotations(self):\r\n        \"\"\"Load annotations from synthetic dataset\"\"\"\r\n        annotations_file = os.path.join(self.data_dir, 'annotations.json')\r\n\r\n        with open(annotations_file, 'r') as f:\r\n            annotations = json.load(f)\r\n\r\n        return annotations\r\n\r\n    def __len__(self):\r\n        return len(self.annotations['images'])\r\n\r\n    def __getitem__(self, idx):\r\n        # Get image and annotation\r\n        img_info = self.annotations['images'][idx]\r\n        img_path = os.path.join(self.data_dir, 'images', img_info['file_name'])\r\n\r\n        # Load image\r\n        image = Image.open(img_path).convert('RGB')\r\n\r\n        # Apply transforms\r\n        if self.transform:\r\n            image = self.transform(image)\r\n\r\n        # Get annotation\r\n        if self.task == 'classification':\r\n            label = self.get_classification_label(idx)\r\n            return image, label\r\n        elif self.task == 'detection':\r\n            target = self.get_detection_target(idx)\r\n            return image, target\r\n        else:\r\n            raise ValueError(f\"Unsupported task: {self.task}\")\r\n\r\n    def get_classification_label(self, idx):\r\n        \"\"\"Get classification label for the image\"\"\"\r\n        # Find annotations for this image\r\n        img_id = self.annotations['images'][idx]['id']\r\n\r\n        # In classification, we might have a single class per image\r\n        # or multiple classes for multi-label classification\r\n        return 0  # Placeholder\r\n\r\n    def get_detection_target(self, idx):\r\n        \"\"\"Get detection target for the image\"\"\"\r\n        # Find annotations for this image\r\n        img_id = self.annotations['images'][idx]['id']\r\n\r\n        # Get all annotations for this image\r\n        img_annotations = [ann for ann in self.annotations['annotations']\r\n                          if ann['image_id'] == img_id]\r\n\r\n        # Format for detection training\r\n        boxes = []\r\n        labels = []\r\n\r\n        for ann in img_annotations:\r\n            bbox = ann['bbox']  # [x_min, y_min, width, height]\r\n            boxes.append([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])  # Convert to [x1, y1, x2, y2]\r\n            labels.append(ann['category_id'])\r\n\r\n        target = {\r\n            'boxes': torch.tensor(boxes, dtype=torch.float32),\r\n            'labels': torch.tensor(labels, dtype=torch.int64)\r\n        }\r\n\r\n        return target\r\n\r\ndef create_training_pipeline():\r\n    \"\"\"Create training pipeline with synthetic data\"\"\"\r\n\r\n    # Define data transforms\r\n    train_transform = transforms.Compose([\r\n        transforms.Resize((224, 224)),\r\n        transforms.RandomHorizontalFlip(p=0.5),\r\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\r\n        transforms.ToTensor(),\r\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\r\n    ])\r\n\r\n    # Create dataset\r\n    dataset = SyntheticRoboticsDataset(\r\n        data_dir='/path/to/synthetic/dataset',\r\n        transform=train_transform,\r\n        task='classification'\r\n    )\r\n\r\n    # Create data loader\r\n    dataloader = DataLoader(\r\n        dataset,\r\n        batch_size=32,\r\n        shuffle=True,\r\n        num_workers=4,\r\n        pin_memory=True\r\n    )\r\n\r\n    # Example model\r\n    model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\r\n    model.fc = nn.Linear(model.fc.in_features, num_classes=10)  # Adjust for your classes\r\n\r\n    # Training setup\r\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n    model.to(device)\r\n\r\n    criterion = nn.CrossEntropyLoss()\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\r\n\r\n    # Training loop\r\n    model.train()\r\n    for epoch in range(10):\r\n        for batch_idx, (data, target) in enumerate(dataloader):\r\n            data, target = data.to(device), target.to(device)\r\n\r\n            optimizer.zero_grad()\r\n            output = model(data)\r\n            loss = criterion(output, target)\r\n            loss.backward()\r\n            optimizer.step()\r\n\r\n            if batch_idx % 100 == 0:\r\n                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.6f}')\r\n\r\n    return model\r\n\r\n# Example usage\r\nif __name__ == \"__main__\":\r\n    # This would be used to train a model on synthetic data\r\n    pass\n"})}),"\n",(0,a.jsx)(e.h2,{id:"best-practices-for-synthetic-data-generation",children:"Best Practices for Synthetic Data Generation"}),"\n",(0,a.jsx)(e.h3,{id:"1-quality-control",children:"1. Quality Control"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Validate synthetic data against real data distributions"}),"\n",(0,a.jsx)(e.li,{children:"Use quality assessment metrics regularly"}),"\n",(0,a.jsx)(e.li,{children:"Implement feedback loops for improvement"}),"\n",(0,a.jsx)(e.li,{children:"Test models on real data to validate transfer"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"2-domain-randomization-strategy",children:"2. Domain Randomization Strategy"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Start with minimal randomization"}),"\n",(0,a.jsx)(e.li,{children:"Gradually increase complexity"}),"\n",(0,a.jsx)(e.li,{children:"Focus on relevant variations for your task"}),"\n",(0,a.jsx)(e.li,{children:"Monitor model performance during randomization"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"3-computational-efficiency",children:"3. Computational Efficiency"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Use appropriate scene complexity"}),"\n",(0,a.jsx)(e.li,{children:"Optimize rendering settings for speed"}),"\n",(0,a.jsx)(e.li,{children:"Implement progressive generation"}),"\n",(0,a.jsx)(e.li,{children:"Consider distributed generation"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"4-data-pipeline-management",children:"4. Data Pipeline Management"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Version control for synthetic datasets"}),"\n",(0,a.jsx)(e.li,{children:"Automated quality checks"}),"\n",(0,a.jsx)(e.li,{children:"Efficient storage and retrieval"}),"\n",(0,a.jsx)(e.li,{children:"Documentation of generation parameters"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,a.jsx)(e.h3,{id:"rendering-artifacts",children:"Rendering Artifacts"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Check material properties and PBR compliance"}),"\n",(0,a.jsx)(e.li,{children:"Verify lighting setup and intensity"}),"\n",(0,a.jsx)(e.li,{children:"Adjust camera parameters and clipping planes"}),"\n",(0,a.jsx)(e.li,{children:"Validate geometry and texture coordinates"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"performance-issues",children:"Performance Issues"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Reduce scene complexity"}),"\n",(0,a.jsx)(e.li,{children:"Use level-of-detail systems"}),"\n",(0,a.jsx)(e.li,{children:"Optimize material complexity"}),"\n",(0,a.jsx)(e.li,{children:"Consider multi-GPU setups"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"sim-to-real-transfer-problems",children:"Sim-to-Real Transfer Problems"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Analyze domain gap systematically"}),"\n",(0,a.jsx)(e.li,{children:"Adjust randomization parameters"}),"\n",(0,a.jsx)(e.li,{children:"Add real data to training set"}),"\n",(0,a.jsx)(e.li,{children:"Use domain adaptation techniques"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"Photorealistic simulation and synthetic data generation with NVIDIA Isaac provide powerful tools for accelerating AI development in robotics. The combination of physically accurate rendering, domain randomization, and automatic annotation creates high-quality training datasets that can significantly reduce the time and cost of developing perception systems for humanoid robots."}),"\n",(0,a.jsx)(e.p,{children:"Key advantages include:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Rapid generation of diverse, labeled training data"}),"\n",(0,a.jsx)(e.li,{children:"Safe testing of edge cases and failure scenarios"}),"\n",(0,a.jsx)(e.li,{children:"Consistent, reproducible experimental conditions"}),"\n",(0,a.jsx)(e.li,{children:"Cost-effective alternative to real-world data collection"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"The success of synthetic data approaches depends on careful attention to rendering quality, appropriate domain randomization strategies, and thorough validation of sim-to-real transfer capabilities. As we continue through this module, we'll explore how these synthetic data capabilities integrate with Isaac's other features for comprehensive AI robotics development."}),"\n",(0,a.jsx)(e.p,{children:"In the next chapter, we'll examine Isaac ROS in detail, focusing on hardware-accelerated robotics capabilities."})]})}function m(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(c,{...n})}):c(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>s,x:()=>o});var a=r(6540);const i={},t=a.createContext(i);function s(n){const e=a.useContext(t);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),a.createElement(t.Provider,{value:e},n.children)}}}]);