"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[512],{3735:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var r=t(4848),a=t(8453);const i={sidebar_position:18,title:"Chapter 18: LLM-Based Cognitive Planning"},o="LLM-Based Cognitive Planning",s={id:"module-4/chapter-18",title:"Chapter 18: LLM-Based Cognitive Planning",description:"Overview",source:"@site/docs/module-4/chapter-18.mdx",sourceDirName:"module-4",slug:"/module-4/chapter-18",permalink:"/ai-humanoid-robotics-book/docs/module-4/chapter-18",draft:!1,unlisted:!1,editUrl:"https://github.com/mujahidshaikh18/ai-humanoid-robotics-book/tree/main/docs/module-4/chapter-18.mdx",tags:[],version:"current",sidebarPosition:18,frontMatter:{sidebar_position:18,title:"Chapter 18: LLM-Based Cognitive Planning"},sidebar:"tutorialSidebar",previous:{title:"Chapter 17: Voice-to-Action with Whisper",permalink:"/ai-humanoid-robotics-book/docs/module-4/chapter-17"},next:{title:"Chapter 19: Multimodal Perception",permalink:"/ai-humanoid-robotics-book/docs/module-4/chapter-19"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to LLM-Based Cognitive Planning",id:"introduction-to-llm-based-cognitive-planning",level:2},{value:"Cognitive Planning in Robotics",id:"cognitive-planning-in-robotics",level:3},{value:"LLMs as Cognitive Planners",id:"llms-as-cognitive-planners",level:3},{value:"LLM Planning Architecture",id:"llm-planning-architecture",level:2},{value:"Planning System Components",id:"planning-system-components",level:3},{value:"Context-Aware Planning",id:"context-aware-planning",level:2},{value:"Environment and State Modeling",id:"environment-and-state-modeling",level:3},{value:"Plan Execution and Monitoring",id:"plan-execution-and-monitoring",level:2},{value:"Plan Execution System",id:"plan-execution-system",level:3},{value:"Plan Adaptation and Learning",id:"plan-adaptation-and-learning",level:2},{value:"Adaptive Planning System",id:"adaptive-planning-system",level:3},{value:"Advanced Planning Techniques",id:"advanced-planning-techniques",level:2},{value:"Hierarchical and Symbolic Planning",id:"hierarchical-and-symbolic-planning",level:3},{value:"Integration with VLA Systems",id:"integration-with-vla-systems",level:2},{value:"VLA Planning Integration",id:"vla-planning-integration",level:3},{value:"Prompt Engineering for Robotics",id:"prompt-engineering-for-robotics",level:2},{value:"Effective Prompting Strategies",id:"effective-prompting-strategies",level:3},{value:"Performance Optimization and Caching",id:"performance-optimization-and-caching",level:2},{value:"Plan Caching System",id:"plan-caching-system",level:3},{value:"Summary",id:"summary",level:2}];function p(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.h1,{id:"llm-based-cognitive-planning",children:"LLM-Based Cognitive Planning"}),"\n",(0,r.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(e.p,{children:"Large Language Models (LLMs) have emerged as powerful tools for cognitive planning in robotics, enabling humanoid robots to understand complex natural language commands and generate sophisticated multi-step plans. This chapter explores how LLMs can serve as cognitive planners, bridging high-level human instructions with low-level robot actions through reasoning, planning, and adaptation."}),"\n",(0,r.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Understand how LLMs function as cognitive planners for robotics"}),"\n",(0,r.jsx)(e.li,{children:"Implement LLM-based planning systems for humanoid robots"}),"\n",(0,r.jsx)(e.li,{children:"Design effective prompting strategies for robotic planning"}),"\n",(0,r.jsx)(e.li,{children:"Handle plan execution, monitoring, and adaptation"}),"\n",(0,r.jsx)(e.li,{children:"Evaluate and optimize LLM-based planning performance"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"introduction-to-llm-based-cognitive-planning",children:"Introduction to LLM-Based Cognitive Planning"}),"\n",(0,r.jsx)(e.h3,{id:"cognitive-planning-in-robotics",children:"Cognitive Planning in Robotics"}),"\n",(0,r.jsx)(e.p,{children:"Cognitive planning involves high-level reasoning to transform goals into executable action sequences. For humanoid robots, this includes:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Goal Interpretation"}),": Understanding natural language commands"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"World Modeling"}),": Creating internal representations of the environment"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Plan Generation"}),": Creating multi-step action sequences"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Plan Execution"}),": Coordinating low-level controllers"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Plan Adaptation"}),": Adjusting plans based on feedback"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"llms-as-cognitive-planners",children:"LLMs as Cognitive Planners"}),"\n",(0,r.jsx)(e.p,{children:"LLMs excel at cognitive planning due to their:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Commonsense Reasoning"}),": Understanding of physical and social world"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Language Understanding"}),": Natural interpretation of commands"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Sequential Reasoning"}),": Ability to generate step-by-step plans"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Knowledge Integration"}),": Access to vast world knowledge"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Adaptability"}),": Ability to handle novel situations"]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   Natural       \u2502    \u2502   LLM Cognitive \u2502    \u2502   Action        \u2502\r\n\u2502   Language      \u2502\u2500\u2500\u2500\u25b6\u2502   Planner       \u2502\u2500\u2500\u2500\u25b6\u2502   Execution     \u2502\r\n\u2502   Command       \u2502    \u2502   (Reasoning,   \u2502    \u2502   System        \u2502\r\n\u2502   ("Clean the   \u2502    \u2502   Planning,     \u2502    \u2502   (Navigation,  \u2502\r\n\u2502   kitchen and   \u2502    \u2502   Knowledge)    \u2502    \u2502   Manipulation) \u2502\r\n\u2502   then set      \u2502    \u2502                 \u2502    \u2502                 \u2502\r\n\u2502   the table")   \u2502    \u2502                 \u2502    \u2502                 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n         \u2502                       \u2502                       \u2502\r\n         \u25bc                       \u25bc                       \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   Context       \u2502    \u2502   Plan          \u2502    \u2502   Robot         \u2502\r\n\u2502   (Environment  \u2502    \u2502   (Sequence of  \u2502    \u2502   State         \u2502\r\n\u2502   State,        \u2502    \u2502   Actions with  \u2502    \u2502   (Position,    \u2502\r\n\u2502   Capabilities) \u2502    \u2502   Justifications)\u2502    \u2502   Orientation, \u2502\r\n\u2502                 \u2502    \u2502                 \u2502    \u2502   Gripper, etc.)\u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,r.jsx)(e.h2,{id:"llm-planning-architecture",children:"LLM Planning Architecture"}),"\n",(0,r.jsx)(e.h3,{id:"planning-system-components",children:"Planning System Components"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import openai\r\nimport json\r\nimport re\r\nimport time\r\nfrom typing import Dict, List, Optional, Any\r\nfrom dataclasses import dataclass\r\nfrom enum import Enum\r\n\r\n@dataclass\r\nclass PlanStep:\r\n    """Represents a single step in a robot plan"""\r\n    step_id: int\r\n    action: str\r\n    parameters: Dict[str, Any]\r\n    description: str\r\n    preconditions: List[str]\r\n    effects: List[str]\r\n    success_criteria: List[str]\r\n\r\n@dataclass\r\nclass Plan:\r\n    """Complete plan for robot execution"""\r\n    plan_id: str\r\n    goal: str\r\n    steps: List[PlanStep]\r\n    estimated_duration: float\r\n    confidence: float\r\n\r\nclass PlanStatus(Enum):\r\n    """Status of plan execution"""\r\n    PENDING = "pending"\r\n    EXECUTING = "executing"\r\n    COMPLETED = "completed"\r\n    FAILED = "failed"\r\n    ADAPTED = "adapted"\r\n\r\nclass LLMBasedPlanner:\r\n    def __init__(self, model_name="gpt-4", api_key=None):\r\n        """\r\n        Initialize LLM-based cognitive planner\r\n\r\n        Args:\r\n            model_name: Name of the LLM model to use\r\n            api_key: API key for the LLM service\r\n        """\r\n        self.model_name = model_name\r\n        self.client = openai.OpenAI(api_key=api_key) if api_key else None\r\n\r\n        # Robot capabilities and constraints\r\n        self.robot_capabilities = {\r\n            "navigation": {\r\n                "max_speed": 0.5,\r\n                "turn_rate": 0.8,\r\n                "indoor_only": True\r\n            },\r\n            "manipulation": {\r\n                "max_weight": 2.0,\r\n                "reach_distance": 1.0,\r\n                "gripper_types": ["precision", "power"]\r\n            },\r\n            "sensors": ["camera", "lidar", "imu", "force_torque"],\r\n            "actuators": ["wheels", "arm_joints", "head_pan_tilt"]\r\n        }\r\n\r\n        # Environment knowledge\r\n        self.environment_knowledge = {}\r\n        self.object_knowledge = {}\r\n\r\n        # Plan execution tracking\r\n        self.active_plans = {}\r\n        self.plan_history = []\r\n\r\n    def create_plan(self, goal: str, context: Dict[str, Any] = None) -> Optional[Plan]:\r\n        """\r\n        Create a plan for achieving the specified goal using LLM\r\n\r\n        Args:\r\n            goal: Natural language description of the goal\r\n            context: Additional context information\r\n\r\n        Returns:\r\n            Plan object or None if planning fails\r\n        """\r\n        if context is None:\r\n            context = {}\r\n\r\n        # Construct the planning prompt\r\n        prompt = self._construct_planning_prompt(goal, context)\r\n\r\n        try:\r\n            # Generate plan using LLM\r\n            response = self.client.chat.completions.create(\r\n                model=self.model_name,\r\n                messages=[\r\n                    {"role": "system", "content": self._get_system_prompt()},\r\n                    {"role": "user", "content": prompt}\r\n                ],\r\n                temperature=0.3,\r\n                max_tokens=2000,\r\n                response_format={"type": "json_object"}\r\n            )\r\n\r\n            # Parse the response\r\n            plan_data = json.loads(response.choices[0].message.content)\r\n\r\n            # Convert to Plan object\r\n            plan = self._parse_plan_response(plan_data, goal)\r\n\r\n            # Store in active plans\r\n            plan.plan_id = f"plan_{int(time.time())}"\r\n            self.active_plans[plan.plan_id] = plan\r\n\r\n            return plan\r\n\r\n        except Exception as e:\r\n            print(f"Plan creation failed: {e}")\r\n            return None\r\n\r\n    def _construct_planning_prompt(self, goal: str, context: Dict[str, Any]) -> str:\r\n        """Construct the prompt for LLM-based planning"""\r\n        prompt = f"""\r\n        You are an advanced cognitive planner for a humanoid robot. Your task is to create a detailed step-by-step plan to achieve the following goal:\r\n\r\n        GOAL: {goal}\r\n\r\n        ROBOT CAPABILITIES:\r\n        - Navigation: Can move at max 0.5 m/s, turn at 0.8 rad/s, indoor navigation only\r\n        - Manipulation: Can lift max 2.0 kg, reach distance 1.0 m, precision and power grippers available\r\n        - Sensors: Camera, LIDAR, IMU, force/torque sensors\r\n        - Actuators: Wheels, arm joints, head pan/tilt\r\n\r\n        CURRENT CONTEXT:\r\n        {json.dumps(context, indent=2)}\r\n\r\n        Please generate a detailed plan with the following structure:\r\n        {{\r\n            "goal": "original goal",\r\n            "estimated_duration": float,\r\n            "confidence": float,\r\n            "steps": [\r\n                {{\r\n                    "step_id": int,\r\n                    "action": "action_name",\r\n                    "parameters": {{"param1": "value1", ...}},\r\n                    "description": "human-readable description",\r\n                    "preconditions": ["condition1", "condition2", ...],\r\n                    "effects": ["effect1", "effect2", ...],\r\n                    "success_criteria": ["criterion1", "criterion2", ...]\r\n                }}\r\n            ]\r\n        }}\r\n\r\n        Actions should be from this list: move_to, pick_object, place_object, look_at, grasp, release, speak, wait, navigate, manipulate.\r\n\r\n        Ensure the plan is safe, feasible given robot capabilities, and accounts for the current context.\r\n        """\r\n\r\n        return prompt\r\n\r\n    def _get_system_prompt(self) -> str:\r\n        """Get the system prompt for the LLM"""\r\n        return """\r\n        You are an expert cognitive planner for humanoid robots. You generate detailed, executable plans that transform high-level goals into specific robot actions. Consider:\r\n\r\n        1. Robot capabilities and limitations\r\n        2. Physical constraints and safety\r\n        3. Environmental context\r\n        4. Logical sequence of actions\r\n        5. Feasibility and safety of each step\r\n        6. Precondition checking before actions\r\n        7. Effect tracking for state management\r\n\r\n        Always provide structured JSON output with the exact format specified in the user prompt.\r\n        """\r\n\r\n    def _parse_plan_response(self, plan_data: Dict, original_goal: str) -> Plan:\r\n        """Parse LLM response into Plan object"""\r\n        steps = []\r\n        for step_data in plan_data.get("steps", []):\r\n            step = PlanStep(\r\n                step_id=step_data["step_id"],\r\n                action=step_data["action"],\r\n                parameters=step_data.get("parameters", {}),\r\n                description=step_data["description"],\r\n                preconditions=step_data.get("preconditions", []),\r\n                effects=step_data.get("effects", []),\r\n                success_criteria=step_data.get("success_criteria", [])\r\n            )\r\n            steps.append(step)\r\n\r\n        plan = Plan(\r\n            plan_id="",\r\n            goal=original_goal,\r\n            steps=steps,\r\n            estimated_duration=plan_data.get("estimated_duration", 0.0),\r\n            confidence=plan_data.get("confidence", 0.5)\r\n        )\r\n\r\n        return plan\n'})}),"\n",(0,r.jsx)(e.h2,{id:"context-aware-planning",children:"Context-Aware Planning"}),"\n",(0,r.jsx)(e.h3,{id:"environment-and-state-modeling",children:"Environment and State Modeling"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'class EnvironmentModel:\r\n    def __init__(self):\r\n        self.objects = {}\r\n        self.locations = {}\r\n        self.spatial_relations = {}\r\n        self.dynamics = {}\r\n        self.update_timestamp = time.time()\r\n\r\n    def update_from_perception(self, perception_data: Dict):\r\n        """Update environment model from perception system"""\r\n        # Update objects\r\n        for obj_data in perception_data.get("objects", []):\r\n            obj_id = obj_data["id"]\r\n            self.objects[obj_id] = {\r\n                "type": obj_data["type"],\r\n                "position": obj_data["position"],\r\n                "size": obj_data["size"],\r\n                "properties": obj_data.get("properties", {}),\r\n                "last_seen": time.time()\r\n            }\r\n\r\n        # Update locations\r\n        for location_data in perception_data.get("locations", []):\r\n            loc_id = location_data["id"]\r\n            self.locations[loc_id] = {\r\n                "name": location_data["name"],\r\n                "position": location_data["position"],\r\n                "type": location_data["type"],\r\n                "accessible": location_data.get("accessible", True)\r\n            }\r\n\r\n        # Update spatial relations\r\n        self.spatial_relations = self._compute_spatial_relations()\r\n\r\n    def _compute_spatial_relations(self) -> Dict:\r\n        """Compute spatial relations between objects and locations"""\r\n        relations = {}\r\n\r\n        for obj_id, obj_data in self.objects.items():\r\n            for loc_id, loc_data in self.locations.items():\r\n                distance = self._calculate_distance(\r\n                    obj_data["position"], loc_data["position"]\r\n                )\r\n\r\n                relation = {\r\n                    "distance": distance,\r\n                    "accessible": distance < 2.0,  # Within robot reach\r\n                    "relative_position": self._get_relative_position(\r\n                        obj_data["position"], loc_data["position"]\r\n                    )\r\n                }\r\n\r\n                if obj_id not in relations:\r\n                    relations[obj_id] = {}\r\n                relations[obj_id][loc_id] = relation\r\n\r\n        return relations\r\n\r\n    def _calculate_distance(self, pos1: List[float], pos2: List[float]) -> float:\r\n        """Calculate Euclidean distance between two positions"""\r\n        return sum((a - b) ** 2 for a, b in zip(pos1, pos2)) ** 0.5\r\n\r\n    def _get_relative_position(self, pos1: List[float], pos2: List[float]) -> str:\r\n        """Get relative position description"""\r\n        dx = pos2[0] - pos1[0]\r\n        dy = pos2[1] - pos1[1]\r\n\r\n        if abs(dx) > abs(dy):\r\n            return "east" if dx > 0 else "west"\r\n        else:\r\n            return "north" if dy > 0 else "south"\r\n\r\nclass ContextManager:\r\n    def __init__(self):\r\n        self.environment = EnvironmentModel()\r\n        self.robot_state = {}\r\n        self.task_history = []\r\n        self.user_preferences = {}\r\n        self.current_plan = None\r\n\r\n    def get_context_for_planning(self) -> Dict:\r\n        """Get complete context for planning"""\r\n        return {\r\n            "environment": {\r\n                "objects": self.environment.objects,\r\n                "locations": self.environment.locations,\r\n                "spatial_relations": self.environment.spatial_relations\r\n            },\r\n            "robot_state": self.robot_state,\r\n            "task_history": self.task_history[-10:],  # Last 10 tasks\r\n            "current_plan": self.current_plan,\r\n            "time_of_day": self._get_time_of_day(),\r\n            "user_preferences": self.user_preferences\r\n        }\r\n\r\n    def _get_time_of_day(self) -> str:\r\n        """Get current time of day for context"""\r\n        current_hour = time.localtime().tm_hour\r\n        if 5 <= current_hour < 12:\r\n            return "morning"\r\n        elif 12 <= current_hour < 17:\r\n            return "afternoon"\r\n        elif 17 <= current_hour < 21:\r\n            return "evening"\r\n        else:\r\n            return "night"\n'})}),"\n",(0,r.jsx)(e.h2,{id:"plan-execution-and-monitoring",children:"Plan Execution and Monitoring"}),"\n",(0,r.jsx)(e.h3,{id:"plan-execution-system",children:"Plan Execution System"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import asyncio\r\nfrom typing import Callable, Awaitable\r\n\r\nclass PlanExecutor:\r\n    def __init__(self, robot_interface=None):\r\n        self.robot_interface = robot_interface\r\n        self.current_plan = None\r\n        self.current_step = 0\r\n        self.execution_status = PlanStatus.PENDING\r\n        self.step_callbacks = []\r\n        self.plan_callbacks = []\r\n\r\n    async def execute_plan(self, plan: Plan) -> PlanStatus:\r\n        """Execute a complete plan step by step"""\r\n        self.current_plan = plan\r\n        self.current_step = 0\r\n        self.execution_status = PlanStatus.EXECUTING\r\n\r\n        print(f"Starting execution of plan: {plan.goal}")\r\n\r\n        # Execute each step\r\n        for step_idx, step in enumerate(plan.steps):\r\n            self.current_step = step_idx\r\n\r\n            # Check preconditions\r\n            if not await self._check_preconditions(step):\r\n                print(f"Preconditions failed for step {step_idx}: {step.description}")\r\n                self.execution_status = PlanStatus.FAILED\r\n                break\r\n\r\n            # Execute the step\r\n            success = await self._execute_step(step)\r\n\r\n            if not success:\r\n                print(f"Step {step_idx} failed: {step.description}")\r\n                self.execution_status = PlanStatus.FAILED\r\n                break\r\n\r\n            # Check success criteria\r\n            if not await self._check_success_criteria(step):\r\n                print(f"Success criteria not met for step {step_idx}")\r\n                self.execution_status = PlanStatus.FAILED\r\n                break\r\n\r\n            # Notify callbacks\r\n            for callback in self.step_callbacks:\r\n                await callback(step_idx, step, success)\r\n\r\n        # Final status\r\n        if self.execution_status == PlanStatus.EXECUTING:\r\n            self.execution_status = PlanStatus.COMPLETED\r\n            print(f"Plan completed successfully: {plan.goal}")\r\n        else:\r\n            print(f"Plan execution failed: {plan.goal}")\r\n\r\n        # Notify plan completion\r\n        for callback in self.plan_callbacks:\r\n            await callback(self.execution_status, plan)\r\n\r\n        return self.execution_status\r\n\r\n    async def _check_preconditions(self, step: PlanStep) -> bool:\r\n        """Check if preconditions for a step are met"""\r\n        # This would interface with the robot\'s state system\r\n        # For now, we\'ll assume most preconditions are met\r\n        for precondition in step.preconditions:\r\n            if not await self._evaluate_precondition(precondition):\r\n                return False\r\n        return True\r\n\r\n    async def _evaluate_precondition(self, precondition: str) -> bool:\r\n        """Evaluate a specific precondition"""\r\n        # This would check actual robot state\r\n        # For demonstration, return True for most cases\r\n        if precondition.startswith("robot_at_"):\r\n            # Check if robot is at required location\r\n            return True  # Simplified\r\n        elif precondition.startswith("object_"):\r\n            # Check if object is available\r\n            return True  # Simplified\r\n        else:\r\n            return True\r\n\r\n    async def _execute_step(self, step: PlanStep) -> bool:\r\n        """Execute a single plan step"""\r\n        print(f"Executing step: {step.description}")\r\n\r\n        if not self.robot_interface:\r\n            # Simulate execution\r\n            print(f"Simulating action: {step.action} with parameters {step.parameters}")\r\n            await asyncio.sleep(0.5)  # Simulate execution time\r\n            return True\r\n\r\n        try:\r\n            # Execute based on action type\r\n            if step.action == "move_to":\r\n                return await self._execute_move_to(step.parameters)\r\n            elif step.action == "pick_object":\r\n                return await self._execute_pick_object(step.parameters)\r\n            elif step.action == "place_object":\r\n                return await self._execute_place_object(step.parameters)\r\n            elif step.action == "look_at":\r\n                return await self._execute_look_at(step.parameters)\r\n            elif step.action == "speak":\r\n                return await self._execute_speak(step.parameters)\r\n            elif step.action == "wait":\r\n                return await self._execute_wait(step.parameters)\r\n            else:\r\n                print(f"Unknown action: {step.action}")\r\n                return False\r\n        except Exception as e:\r\n            print(f"Error executing step {step.action}: {e}")\r\n            return False\r\n\r\n    async def _execute_move_to(self, parameters: Dict) -> bool:\r\n        """Execute move to location action"""\r\n        location = parameters.get("location", "default")\r\n        speed = parameters.get("speed", 0.5)\r\n\r\n        print(f"Moving to {location} at speed {speed}")\r\n        # In real implementation: call navigation system\r\n        await asyncio.sleep(1)  # Simulate movement\r\n        return True\r\n\r\n    async def _execute_pick_object(self, parameters: Dict) -> bool:\r\n        """Execute pick object action"""\r\n        object_id = parameters.get("object_id", "")\r\n        grasp_type = parameters.get("grasp_type", "precision")\r\n\r\n        print(f"Picking up {object_id} with {grasp_type} grasp")\r\n        # In real implementation: call manipulation system\r\n        await asyncio.sleep(1)  # Simulate manipulation\r\n        return True\r\n\r\n    async def _execute_place_object(self, parameters: Dict) -> bool:\r\n        """Execute place object action"""\r\n        location = parameters.get("location", "default")\r\n        object_id = parameters.get("object_id", "")\r\n\r\n        print(f"Placing {object_id} at {location}")\r\n        # In real implementation: call manipulation system\r\n        await asyncio.sleep(1)  # Simulate manipulation\r\n        return True\r\n\r\n    async def _execute_look_at(self, parameters: Dict) -> bool:\r\n        """Execute look at action"""\r\n        target = parameters.get("target", "")\r\n\r\n        print(f"Looking at {target}")\r\n        # In real implementation: call head/eye control\r\n        await asyncio.sleep(0.5)  # Simulate head movement\r\n        return True\r\n\r\n    async def _execute_speak(self, parameters: Dict) -> bool:\r\n        """Execute speak action"""\r\n        text = parameters.get("text", "")\r\n\r\n        print(f"Speaking: {text}")\r\n        # In real implementation: call text-to-speech\r\n        await asyncio.sleep(len(text.split()) * 0.1)  # Simulate speech duration\r\n        return True\r\n\r\n    async def _execute_wait(self, parameters: Dict) -> bool:\r\n        """Execute wait action"""\r\n        duration = parameters.get("duration", 1.0)\r\n\r\n        print(f"Waiting for {duration} seconds")\r\n        await asyncio.sleep(duration)\r\n        return True\r\n\r\n    async def _check_success_criteria(self, step: PlanStep) -> bool:\r\n        """Check if success criteria for a step are met"""\r\n        for criterion in step.success_criteria:\r\n            if not await self._evaluate_success_criterion(criterion):\r\n                return False\r\n        return True\r\n\r\n    async def _evaluate_success_criterion(self, criterion: str) -> bool:\r\n        """Evaluate a specific success criterion"""\r\n        # This would check actual robot state after action\r\n        # For demonstration, return True\r\n        return True\r\n\r\n    def add_step_callback(self, callback: Callable[[int, PlanStep, bool], Awaitable[None]]):\r\n        """Add callback for step execution"""\r\n        self.step_callbacks.append(callback)\r\n\r\n    def add_plan_callback(self, callback: Callable[[PlanStatus, Plan], Awaitable[None]]):\r\n        """Add callback for plan execution"""\r\n        self.plan_callbacks.append(callback)\n'})}),"\n",(0,r.jsx)(e.h2,{id:"plan-adaptation-and-learning",children:"Plan Adaptation and Learning"}),"\n",(0,r.jsx)(e.h3,{id:"adaptive-planning-system",children:"Adaptive Planning System"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'class AdaptivePlanner:\r\n    def __init__(self, base_planner: LLMBasedPlanner):\r\n        self.base_planner = base_planner\r\n        self.execution_history = []\r\n        self.failure_patterns = {}\r\n        self.adaptation_rules = []\r\n\r\n    async def adapt_plan(self, original_plan: Plan, failure_info: Dict) -> Optional[Plan]:\r\n        """Adapt a plan based on execution failure or environmental changes"""\r\n        # Record the failure\r\n        self.execution_history.append({\r\n            "plan_id": original_plan.plan_id,\r\n            "goal": original_plan.goal,\r\n            "failure_info": failure_info,\r\n            "timestamp": time.time()\r\n        })\r\n\r\n        # Analyze failure pattern\r\n        failure_pattern = self._analyze_failure_pattern(failure_info)\r\n\r\n        # Check for known adaptation rules\r\n        adaptation_rule = self._find_adaptation_rule(failure_pattern)\r\n\r\n        if adaptation_rule:\r\n            # Apply known adaptation\r\n            adapted_plan = await self._apply_adaptation_rule(\r\n                original_plan, adaptation_rule, failure_info\r\n            )\r\n        else:\r\n            # Generate new plan with failure context\r\n            adapted_plan = await self._generate_adapted_plan(\r\n                original_plan, failure_info\r\n            )\r\n\r\n        return adapted_plan\r\n\r\n    def _analyze_failure_pattern(self, failure_info: Dict) -> str:\r\n        """Analyze failure to identify pattern"""\r\n        failure_type = failure_info.get("type", "unknown")\r\n        failed_step = failure_info.get("step_id", -1)\r\n        error_message = failure_info.get("error", "")\r\n\r\n        # Create a pattern identifier\r\n        pattern = f"{failure_type}_{failed_step}_{hash(error_message) % 1000}"\r\n        return pattern\r\n\r\n    def _find_adaptation_rule(self, pattern: str) -> Optional[Dict]:\r\n        """Find existing adaptation rule for pattern"""\r\n        return self.adaptation_rules.get(pattern)\r\n\r\n    async def _apply_adaptation_rule(self, plan: Plan, rule: Dict, failure_info: Dict) -> Plan:\r\n        """Apply a known adaptation rule to a plan"""\r\n        # This would modify the plan based on the rule\r\n        # For demonstration, we\'ll just return a slightly modified plan\r\n        adapted_plan = Plan(\r\n            plan_id=f"{plan.plan_id}_adapted",\r\n            goal=plan.goal,\r\n            steps=plan.steps.copy(),\r\n            estimated_duration=plan.estimated_duration,\r\n            confidence=plan.confidence * 0.9  # Reduce confidence due to adaptation\r\n        )\r\n\r\n        # Apply specific adaptations based on rule\r\n        if rule.get("type") == "retry_with_different_approach":\r\n            # Modify the failed step\r\n            failed_step_id = failure_info.get("step_id", 0)\r\n            if 0 <= failed_step_id < len(adapted_plan.steps):\r\n                step = adapted_plan.steps[failed_step_id]\r\n                step.description = f"{step.description} (adapted approach)"\r\n\r\n        return adapted_plan\r\n\r\n    async def _generate_adapted_plan(self, original_plan: Plan, failure_info: Dict) -> Optional[Plan]:\r\n        """Generate a new adapted plan based on failure information"""\r\n        # Create context with failure information\r\n        context = {\r\n            "original_goal": original_plan.goal,\r\n            "failed_plan_steps": [step.description for step in original_plan.steps],\r\n            "failure_info": failure_info,\r\n            "available_alternatives": self._get_available_alternatives(failure_info)\r\n        }\r\n\r\n        # Generate new plan with failure context\r\n        adapted_goal = f"{original_plan.goal} (with consideration of previous failure: {failure_info.get(\'error\', \'unknown\')})"\r\n        adapted_plan = self.base_planner.create_plan(adapted_goal, context)\r\n\r\n        return adapted_plan\r\n\r\n    def _get_available_alternatives(self, failure_info: Dict) -> List[str]:\r\n        """Get available alternative approaches based on failure"""\r\n        alternatives = []\r\n\r\n        if failure_info.get("type") == "object_not_found":\r\n            alternatives.append("search_in_different_location")\r\n            alternatives.append("use_alternative_object")\r\n        elif failure_info.get("type") == "navigation_failed":\r\n            alternatives.append("find_alternative_path")\r\n            alternatives.append("request_assistance")\r\n        elif failure_info.get("type") == "manipulation_failed":\r\n            alternatives.append("try_different_grasp")\r\n            alternatives.append("use_different_gripper")\r\n        else:\r\n            alternatives.append("retry_with_adjusted_parameters")\r\n            alternatives.append("break_down_into_smaller_steps")\r\n\r\n        return alternatives\r\n\r\n    def learn_from_execution(self, plan: Plan, execution_result: Dict):\r\n        """Learn from plan execution to improve future planning"""\r\n        # Update failure patterns\r\n        if execution_result.get("status") == "failed":\r\n            failure_info = execution_result.get("failure_info", {})\r\n            pattern = self._analyze_failure_pattern(failure_info)\r\n\r\n            if pattern not in self.failure_patterns:\r\n                self.failure_patterns[pattern] = {\r\n                    "count": 0,\r\n                    "solutions": []\r\n                }\r\n\r\n            self.failure_patterns[pattern]["count"] += 1\r\n            self.failure_patterns[pattern]["solutions"].append(\r\n                execution_result.get("solution_applied", "unknown")\r\n            )\r\n\r\n        # Update adaptation rules based on successful adaptations\r\n        if execution_result.get("adaptation_successful"):\r\n            self._update_adaptation_rules(execution_result)\n'})}),"\n",(0,r.jsx)(e.h2,{id:"advanced-planning-techniques",children:"Advanced Planning Techniques"}),"\n",(0,r.jsx)(e.h3,{id:"hierarchical-and-symbolic-planning",children:"Hierarchical and Symbolic Planning"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'class HierarchicalPlanner:\r\n    def __init__(self, low_level_planner: LLMBasedPlanner):\r\n        self.low_level_planner = low_level_planner\r\n        self.high_level_goals = {}\r\n        self.symbolic_knowledge = self._initialize_symbolic_knowledge()\r\n\r\n    def _initialize_symbolic_knowledge(self) -> Dict:\r\n        """Initialize symbolic knowledge base"""\r\n        return {\r\n            "actions": {\r\n                "clean": {\r\n                    "preconditions": ["room_accessible", "cleaning_tools_available"],\r\n                    "effects": ["room_cleanliness_increased", "objects_organized"],\r\n                    "subactions": ["sweep_floor", "wipe_surfaces", "organize_items"]\r\n                },\r\n                "set_table": {\r\n                    "preconditions": ["table_available", "tableware_available"],\r\n                    "effects": ["table_set", "ready_for_dining"],\r\n                    "subactions": ["place_dishes", "set_cutlery", "add_glassware"]\r\n                },\r\n                "greet_guest": {\r\n                    "preconditions": ["guest_detected", "robot_available"],\r\n                    "effects": ["guest_acknowledged", "interaction_started"],\r\n                    "subactions": ["approach_guest", "speak_greeting", "offer_assistance"]\r\n                }\r\n            },\r\n            "objects": {\r\n                "cleaning_tools": ["broom", "mop", "cloth", "vacuum"],\r\n                "tableware": ["plates", "cups", "forks", "knives", "spoons"],\r\n                "furniture": ["table", "chair", "couch", "desk"]\r\n            },\r\n            "locations": {\r\n                "kitchen": ["cooking_area", "dining_area", "storage_area"],\r\n                "living_room": ["seating_area", "entertainment_area"],\r\n                "bedroom": ["sleeping_area", "dressing_area"]\r\n            }\r\n        }\r\n\r\n    def create_hierarchical_plan(self, high_level_goal: str, context: Dict = None) -> Optional[Plan]:\r\n        """Create a hierarchical plan by decomposing high-level goals"""\r\n        # Parse the high-level goal\r\n        goal_components = self._decompose_goal(high_level_goal)\r\n\r\n        if not goal_components:\r\n            return None\r\n\r\n        # Create plan steps for each component\r\n        all_steps = []\r\n        step_id = 0\r\n\r\n        for component in goal_components:\r\n            component_plan = self.low_level_planner.create_plan(\r\n                component["goal"],\r\n                self._update_context_for_component(context, component)\r\n            )\r\n\r\n            if component_plan:\r\n                # Adjust step IDs\r\n                for step in component_plan.steps:\r\n                    step.step_id = step_id\r\n                    all_steps.append(step)\r\n                    step_id += 1\r\n\r\n        # Create the overall plan\r\n        hierarchical_plan = Plan(\r\n            plan_id=f"hierarchical_{int(time.time())}",\r\n            goal=high_level_goal,\r\n            steps=all_steps,\r\n            estimated_duration=sum(\r\n                (step.parameters.get("estimated_duration", 1.0) for step in all_steps),\r\n                0\r\n            ),\r\n            confidence=min((plan.confidence for plan in [component_plan] if component_plan), default=0.5)\r\n        )\r\n\r\n        return hierarchical_plan\r\n\r\n    def _decompose_goal(self, goal: str) -> List[Dict]:\r\n        """Decompose high-level goal into sub-goals"""\r\n        # This would use more sophisticated NLP and symbolic reasoning\r\n        # For demonstration, simple decomposition rules\r\n\r\n        goal_lower = goal.lower()\r\n\r\n        components = []\r\n\r\n        if "clean" in goal_lower and "kitchen" in goal_lower:\r\n            components.extend([\r\n                {"goal": "navigate to kitchen", "type": "navigation"},\r\n                {"goal": "find cleaning supplies", "type": "search"},\r\n                {"goal": "clean kitchen surfaces", "type": "manipulation"},\r\n                {"goal": "sweep kitchen floor", "type": "manipulation"}\r\n            ])\r\n\r\n        if "set" in goal_lower and "table" in goal_lower:\r\n            components.extend([\r\n                {"goal": "navigate to dining area", "type": "navigation"},\r\n                {"goal": "find tableware", "type": "search"},\r\n                {"goal": "place plates on table", "type": "manipulation"},\r\n                {"goal": "set cutlery", "type": "manipulation"}\r\n            ])\r\n\r\n        return components\r\n\r\n    def _update_context_for_component(self, context: Dict, component: Dict) -> Dict:\r\n        """Update context for a specific component"""\r\n        if context is None:\r\n            context = {}\r\n\r\n        updated_context = context.copy()\r\n        updated_context["current_subtask"] = component["type"]\r\n        updated_context["expected_objects"] = self._get_expected_objects(component["goal"])\r\n\r\n        return updated_context\r\n\r\n    def _get_expected_objects(self, goal: str) -> List[str]:\r\n        """Get expected objects based on goal"""\r\n        for action, info in self.symbolic_knowledge["actions"].items():\r\n            if action in goal.lower():\r\n                return info.get("subactions", [])\r\n\r\n        return []\n'})}),"\n",(0,r.jsx)(e.h2,{id:"integration-with-vla-systems",children:"Integration with VLA Systems"}),"\n",(0,r.jsx)(e.h3,{id:"vla-planning-integration",children:"VLA Planning Integration"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'class VLAPlanningSystem:\r\n    def __init__(self, llm_planner: LLMBasedPlanner, context_manager: ContextManager):\r\n        self.llm_planner = llm_planner\r\n        self.context_manager = context_manager\r\n        self.plan_executor = PlanExecutor()\r\n        self.adaptive_planner = AdaptivePlanner(llm_planner)\r\n        self.hierarchical_planner = HierarchicalPlanner(llm_planner)\r\n\r\n        # Setup callbacks\r\n        self.plan_executor.add_plan_callback(self._handle_plan_completion)\r\n\r\n    async def process_command(self, command: str) -> Optional[Plan]:\r\n        """Process a natural language command and execute plan"""\r\n        # Get current context\r\n        context = self.context_manager.get_context_for_planning()\r\n\r\n        # Create plan\r\n        plan = self.llm_planner.create_plan(command, context)\r\n\r\n        if not plan:\r\n            print(f"Failed to create plan for command: {command}")\r\n            return None\r\n\r\n        print(f"Created plan with {len(plan.steps)} steps for: {command}")\r\n\r\n        # Execute plan\r\n        execution_status = await self.plan_executor.execute_plan(plan)\r\n\r\n        # Handle execution result\r\n        if execution_status == PlanStatus.FAILED:\r\n            # Attempt adaptation\r\n            failure_info = {\r\n                "type": "execution_failed",\r\n                "error": "Plan execution failed",\r\n                "step_id": self.plan_executor.current_step\r\n            }\r\n            adapted_plan = await self.adaptive_planner.adapt_plan(plan, failure_info)\r\n\r\n            if adapted_plan:\r\n                print("Attempting execution with adapted plan")\r\n                await self.plan_executor.execute_plan(adapted_plan)\r\n\r\n        return plan\r\n\r\n    def _handle_plan_completion(self, status: PlanStatus, plan: Plan):\r\n        """Handle plan completion and update context"""\r\n        # Update task history\r\n        self.context_manager.task_history.append({\r\n            "goal": plan.goal,\r\n            "status": status.value,\r\n            "steps_completed": len(plan.steps),\r\n            "timestamp": time.time()\r\n        })\r\n\r\n        # Learn from execution\r\n        execution_result = {\r\n            "status": status.value,\r\n            "plan_id": plan.plan_id,\r\n            "original_goal": plan.goal\r\n        }\r\n\r\n        if status == PlanStatus.FAILED:\r\n            execution_result["failure_info"] = {"type": "execution_failed"}\r\n\r\n        self.adaptive_planner.learn_from_execution(plan, execution_result)\r\n\r\n    def handle_perception_update(self, perception_data: Dict):\r\n        """Handle perception updates and update environment model"""\r\n        self.context_manager.environment.update_from_perception(perception_data)\r\n\r\n    def get_system_status(self) -> Dict:\r\n        """Get current system status"""\r\n        return {\r\n            "active_plan": self.plan_executor.current_plan,\r\n            "current_step": self.plan_executor.current_step,\r\n            "execution_status": self.plan_executor.execution_status.value,\r\n            "context_objects": list(self.context_manager.environment.objects.keys()),\r\n            "context_locations": list(self.context_manager.environment.locations.keys())\r\n        }\n'})}),"\n",(0,r.jsx)(e.h2,{id:"prompt-engineering-for-robotics",children:"Prompt Engineering for Robotics"}),"\n",(0,r.jsx)(e.h3,{id:"effective-prompting-strategies",children:"Effective Prompting Strategies"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'class RobotPlanningPrompter:\r\n    def __init__(self):\r\n        self.prompt_templates = self._initialize_prompt_templates()\r\n\r\n    def _initialize_prompt_templates(self) -> Dict[str, str]:\r\n        """Initialize various prompt templates for different scenarios"""\r\n        return {\r\n            "navigation": """\r\n            Plan a navigation task for a humanoid robot. Consider:\r\n            - Current robot position: {current_position}\r\n            - Target location: {target_location}\r\n            - Obstacles in environment: {obstacles}\r\n            - Robot capabilities: {capabilities}\r\n\r\n            Generate a plan with steps like: move_to, avoid_obstacle, reach_destination.\r\n            """,\r\n\r\n            "manipulation": """\r\n            Plan a manipulation task for a humanoid robot. Consider:\r\n            - Object to manipulate: {object}\r\n            - Object properties: {object_properties}\r\n            - Target location: {target_location}\r\n            - Robot gripper capabilities: {gripper_types}\r\n\r\n            Generate a plan with steps like: approach_object, grasp_object, transport, place_object.\r\n            """,\r\n\r\n            "multi_task": """\r\n            Plan a multi-step task involving both navigation and manipulation. Consider:\r\n            - Main goal: {main_goal}\r\n            - Sub-goals: {sub_goals}\r\n            - Available time: {time_limit}\r\n            - Environmental constraints: {constraints}\r\n\r\n            Generate a hierarchical plan that sequences navigation and manipulation subtasks.\r\n            """,\r\n\r\n            "social_interaction": """\r\n            Plan a social interaction task for a humanoid robot. Consider:\r\n            - Interaction goal: {interaction_goal}\r\n            - User context: {user_context}\r\n            - Social norms: {social_norms}\r\n            - Robot personality: {personality}\r\n\r\n            Generate a plan that includes navigation, speech, gesture, and appropriate interaction steps.\r\n            """\r\n        }\r\n\r\n    def create_navigation_prompt(self, current_pos: List[float], target: str,\r\n                                obstacles: List[Dict], capabilities: Dict) -> str:\r\n        """Create a navigation-specific prompt"""\r\n        return self.prompt_templates["navigation"].format(\r\n            current_position=current_pos,\r\n            target_location=target,\r\n            obstacles=obstacles,\r\n            capabilities=capabilities\r\n        )\r\n\r\n    def create_manipulation_prompt(self, obj: str, obj_props: Dict,\r\n                                  target_location: str, gripper_types: List[str]) -> str:\r\n        """Create a manipulation-specific prompt"""\r\n        return self.prompt_templates["manipulation"].format(\r\n            object=obj,\r\n            object_properties=obj_props,\r\n            target_location=target_location,\r\n            gripper_types=gripper_types\r\n        )\r\n\r\n    def create_multi_task_prompt(self, main_goal: str, sub_goals: List[str],\r\n                                time_limit: float, constraints: List[str]) -> str:\r\n        """Create a multi-task planning prompt"""\r\n        return self.prompt_templates["multi_task"].format(\r\n            main_goal=main_goal,\r\n            sub_goals=sub_goals,\r\n            time_limit=time_limit,\r\n            constraints=constraints\r\n        )\r\n\r\n    def create_social_interaction_prompt(self, interaction_goal: str, user_context: Dict,\r\n                                       social_norms: List[str], personality: str) -> str:\r\n        """Create a social interaction planning prompt"""\r\n        return self.prompt_templates["social_interaction"].format(\r\n            interaction_goal=interaction_goal,\r\n            user_context=user_context,\r\n            social_norms=social_norms,\r\n            personality=personality\r\n        )\r\n\r\n    def apply_chain_of_thought(self, prompt: str) -> str:\r\n        """Apply chain-of-thought reasoning to the prompt"""\r\n        cot_prompt = f"""\r\n        Let\'s approach this step by step:\r\n\r\n        1. First, understand the goal and constraints\r\n        2. Identify the robot\'s current state and capabilities\r\n        3. Consider the environment and potential obstacles\r\n        4. Plan the sequence of actions needed\r\n        5. Verify safety and feasibility of each step\r\n        6. Estimate duration and resource requirements\r\n\r\n        Now, generate a detailed plan for: {prompt}\r\n        """\r\n        return cot_prompt\r\n\r\n    def apply_few_shot_examples(self, prompt: str, examples: List[Dict]) -> str:\r\n        """Apply few-shot learning examples to the prompt"""\r\n        examples_text = "\\n\\nExamples:\\n"\r\n        for i, example in enumerate(examples):\r\n            examples_text += f"Example {i+1}:\\n"\r\n            examples_text += f"Goal: {example[\'goal\']}\\n"\r\n            examples_text += f"Plan: {example[\'plan\']}\\n\\n"\r\n\r\n        return examples_text + prompt\n'})}),"\n",(0,r.jsx)(e.h2,{id:"performance-optimization-and-caching",children:"Performance Optimization and Caching"}),"\n",(0,r.jsx)(e.h3,{id:"plan-caching-system",children:"Plan Caching System"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import hashlib\r\nfrom datetime import datetime, timedelta\r\n\r\nclass PlanCache:\r\n    def __init__(self, max_size=100, ttl_minutes=60):\r\n        self.max_size = max_size\r\n        self.ttl_minutes = ttl_minutes\r\n        self.cache = {}\r\n        self.access_order = []  # For LRU eviction\r\n\r\n    def get_plan(self, goal: str, context: Dict) -> Optional[Plan]:\r\n        """Get cached plan if available"""\r\n        cache_key = self._generate_cache_key(goal, context)\r\n\r\n        if cache_key in self.cache:\r\n            cached_item = self.cache[cache_key]\r\n\r\n            # Check if cache is still valid\r\n            if datetime.now() < cached_item["expiry"]:\r\n                print(f"Retrieved plan from cache for goal: {goal[:50]}...")\r\n                self._update_access_order(cache_key)\r\n                return cached_item["plan"]\r\n            else:\r\n                # Remove expired cache entry\r\n                del self.cache[cache_key]\r\n                if cache_key in self.access_order:\r\n                    self.access_order.remove(cache_key)\r\n\r\n        return None\r\n\r\n    def put_plan(self, goal: str, context: Dict, plan: Plan):\r\n        """Cache a plan"""\r\n        cache_key = self._generate_cache_key(goal, context)\r\n\r\n        # Create expiry time\r\n        expiry = datetime.now() + timedelta(minutes=self.ttl_minutes)\r\n\r\n        # Store in cache\r\n        self.cache[cache_key] = {\r\n            "plan": plan,\r\n            "expiry": expiry,\r\n            "access_time": datetime.now()\r\n        }\r\n\r\n        self._update_access_order(cache_key)\r\n\r\n        # Evict oldest if cache is too large\r\n        if len(self.cache) > self.max_size:\r\n            self._evict_lru()\r\n\r\n    def _generate_cache_key(self, goal: str, context: Dict) -> str:\r\n        """Generate a unique cache key for the goal and context"""\r\n        context_str = json.dumps(context, sort_keys=True)\r\n        key_str = f"{goal}_{context_str}"\r\n        return hashlib.md5(key_str.encode()).hexdigest()\r\n\r\n    def _update_access_order(self, cache_key: str):\r\n        """Update access order for LRU"""\r\n        if cache_key in self.access_order:\r\n            self.access_order.remove(cache_key)\r\n        self.access_order.append(cache_key)\r\n\r\n    def _evict_lru(self):\r\n        """Evict the least recently used item"""\r\n        if self.access_order:\r\n            oldest_key = self.access_order.pop(0)\r\n            if oldest_key in self.cache:\r\n                del self.cache[oldest_key]\r\n\r\nclass OptimizedLLMPlanner(LLMBasedPlanner):\r\n    def __init__(self, model_name="gpt-4", api_key=None, use_cache=True):\r\n        super().__init__(model_name, api_key)\r\n        self.use_cache = use_cache\r\n        self.plan_cache = PlanCache() if use_cache else None\r\n\r\n    def create_plan(self, goal: str, context: Dict[str, Any] = None) -> Optional[Plan]:\r\n        """Create a plan with caching optimization"""\r\n        if context is None:\r\n            context = {}\r\n\r\n        # Check cache first if enabled\r\n        if self.use_cache:\r\n            cached_plan = self.plan_cache.get_plan(goal, context)\r\n            if cached_plan:\r\n                return cached_plan\r\n\r\n        # Generate new plan\r\n        plan = super().create_plan(goal, context)\r\n\r\n        # Cache the plan if successful\r\n        if plan and self.use_cache:\r\n            self.plan_cache.put_plan(goal, context, plan)\r\n\r\n        return plan\n'})}),"\n",(0,r.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(e.p,{children:"LLM-based cognitive planning represents a significant advancement in robotic intelligence, enabling humanoid robots to understand complex natural language commands and generate sophisticated action plans. The key components of effective LLM-based planning systems include:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Context Awareness"}),": Maintaining rich environment and state models"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Hierarchical Reasoning"}),": Breaking down complex tasks into manageable subtasks"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Adaptive Planning"}),": Adjusting plans based on execution feedback"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Performance Optimization"}),": Caching and efficient prompting strategies"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Safety Integration"}),": Ensuring plans are safe and feasible"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"The success of LLM-based cognitive planning in robotics depends on:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Effective prompt engineering for robotic tasks"}),"\n",(0,r.jsx)(e.li,{children:"Integration with perception and action systems"}),"\n",(0,r.jsx)(e.li,{children:"Robust error handling and adaptation mechanisms"}),"\n",(0,r.jsx)(e.li,{children:"Continuous learning from execution experiences"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"In the next chapter, we'll explore multimodal perception systems that enable humanoid robots to integrate visual, auditory, and other sensory information for enhanced environmental understanding and interaction."})]})}function d(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(p,{...n})}):p(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>o,x:()=>s});var r=t(6540);const a={},i=r.createContext(a);function o(n){const e=r.useContext(i);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:o(n.components),r.createElement(i.Provider,{value:e},n.children)}}}]);