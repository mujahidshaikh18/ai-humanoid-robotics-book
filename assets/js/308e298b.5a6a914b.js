"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[70],{4584:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>t,toc:()=>c});var a=r(4848),s=r(8453);const i={sidebar_position:8,title:"Chapter 8: Sensor Simulation"},o="Sensor Simulation",t={id:"module-2/chapter-8",title:"Chapter 8: Sensor Simulation",description:"Overview",source:"@site/docs/module-2/chapter-8.mdx",sourceDirName:"module-2",slug:"/module-2/chapter-8",permalink:"/ai-humanoid-robotics-book/docs/module-2/chapter-8",draft:!1,unlisted:!1,editUrl:"https://github.com/mujahidshaikh18/ai-humanoid-robotics-book/tree/main/docs/module-2/chapter-8.mdx",tags:[],version:"current",sidebarPosition:8,frontMatter:{sidebar_position:8,title:"Chapter 8: Sensor Simulation"},sidebar:"tutorialSidebar",previous:{title:"Chapter 7: Gazebo for Humanoid Physics",permalink:"/ai-humanoid-robotics-book/docs/module-2/chapter-7"},next:{title:"Chapter 9: Environment Building",permalink:"/ai-humanoid-robotics-book/docs/module-2/chapter-9"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Types of Sensors in Humanoid Robots",id:"types-of-sensors-in-humanoid-robots",level:2},{value:"Proprioceptive Sensors",id:"proprioceptive-sensors",level:3},{value:"Exteroceptive Sensors",id:"exteroceptive-sensors",level:3},{value:"Specialized Sensors",id:"specialized-sensors",level:3},{value:"Gazebo Sensor Simulation",id:"gazebo-sensor-simulation",level:2},{value:"Camera Simulation",id:"camera-simulation",level:3},{value:"Depth Camera Simulation",id:"depth-camera-simulation",level:3},{value:"LIDAR Simulation",id:"lidar-simulation",level:3},{value:"IMU Simulation",id:"imu-simulation",level:3},{value:"Force/Torque Sensor Simulation",id:"forcetorque-sensor-simulation",level:3},{value:"Sensor Noise and Realism",id:"sensor-noise-and-realism",level:2},{value:"Adding Realistic Noise Models",id:"adding-realistic-noise-models",level:3},{value:"Dynamic Noise Models",id:"dynamic-noise-models",level:3},{value:"Multi-Sensor Integration",id:"multi-sensor-integration",level:2},{value:"Sensor Fusion Node",id:"sensor-fusion-node",level:3},{value:"Calibration and Validation",id:"calibration-and-validation",level:2},{value:"Sensor Calibration",id:"sensor-calibration",level:3},{value:"Advanced Sensor Simulation",id:"advanced-sensor-simulation",level:2},{value:"Custom Sensor Plugins",id:"custom-sensor-plugins",level:3},{value:"Multi-Modal Sensor Simulation",id:"multi-modal-sensor-simulation",level:3},{value:"Sensor Data Processing",id:"sensor-data-processing",level:2},{value:"Real-time Sensor Processing Node",id:"real-time-sensor-processing-node",level:3},{value:"Sim-to-Real Transfer Considerations",id:"sim-to-real-transfer-considerations",level:2},{value:"Handling the Reality Gap",id:"handling-the-reality-gap",level:3},{value:"Domain Randomization",id:"domain-randomization",level:4},{value:"Sensor Simulation Accuracy",id:"sensor-simulation-accuracy",level:4},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Sensor Placement",id:"1-sensor-placement",level:3},{value:"2. Computational Performance",id:"2-computational-performance",level:3},{value:"3. Validation and Testing",id:"3-validation-and-testing",level:3},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h1,{id:"sensor-simulation",children:"Sensor Simulation"}),"\n",(0,a.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(e.p,{children:"Sensor simulation is a critical component of robotic simulation environments, particularly for humanoid robots that rely on multiple sensor modalities for perception, balance, and interaction. This chapter explores how to accurately simulate various sensors in Gazebo and other simulation environments, ensuring that the virtual sensors closely match their real-world counterparts."}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understand different types of sensors used in humanoid robots"}),"\n",(0,a.jsx)(e.li,{children:"Configure and implement sensor simulation in Gazebo"}),"\n",(0,a.jsx)(e.li,{children:"Calibrate virtual sensors to match real-world characteristics"}),"\n",(0,a.jsx)(e.li,{children:"Integrate simulated sensors with ROS 2"}),"\n",(0,a.jsx)(e.li,{children:"Handle sensor noise and uncertainty in simulation"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"types-of-sensors-in-humanoid-robots",children:"Types of Sensors in Humanoid Robots"}),"\n",(0,a.jsx)(e.p,{children:"Humanoid robots typically employ multiple sensor types to perceive their environment and maintain balance:"}),"\n",(0,a.jsx)(e.h3,{id:"proprioceptive-sensors",children:"Proprioceptive Sensors"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Joint encoders: Measure joint positions and velocities"}),"\n",(0,a.jsx)(e.li,{children:"Force/torque sensors: Measure forces at joints and end effectors"}),"\n",(0,a.jsx)(e.li,{children:"IMU (Inertial Measurement Unit): Measure orientation and acceleration"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"exteroceptive-sensors",children:"Exteroceptive Sensors"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Cameras: Visual perception and object recognition"}),"\n",(0,a.jsx)(e.li,{children:"LIDAR: Distance measurement and mapping"}),"\n",(0,a.jsx)(e.li,{children:"Tactile sensors: Contact detection and force measurement"}),"\n",(0,a.jsx)(e.li,{children:"Microphones: Audio input and speech recognition"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"specialized-sensors",children:"Specialized Sensors"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Force plates: Ground reaction forces"}),"\n",(0,a.jsx)(e.li,{children:"Air pressure sensors: Altitude and weather"}),"\n",(0,a.jsx)(e.li,{children:"Temperature sensors: Environmental monitoring"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"gazebo-sensor-simulation",children:"Gazebo Sensor Simulation"}),"\n",(0,a.jsx)(e.h3,{id:"camera-simulation",children:"Camera Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Cameras are essential for humanoid robots for navigation, object recognition, and human interaction:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<gazebo reference="head_camera">\r\n  <sensor name="camera_sensor" type="camera">\r\n    <always_on>true</always_on>\r\n    <update_rate>30</update_rate>\r\n    <camera name="head_camera">\r\n      <horizontal_fov>1.047</horizontal_fov> \x3c!-- 60 degrees --\x3e\r\n      <image>\r\n        <width>640</width>\r\n        <height>480</height>\r\n        <format>R8G8B8</format>\r\n      </image>\r\n      <clip>\r\n        <near>0.1</near>\r\n        <far>10.0</far>\r\n      </clip>\r\n      <noise>\r\n        <type>gaussian</type>\r\n        <mean>0.0</mean>\r\n        <stddev>0.007</stddev>\r\n      </noise>\r\n    </camera>\r\n    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\r\n      <frame_name>head_camera_optical_frame</frame_name>\r\n      <min_depth>0.1</min_depth>\r\n      <max_depth>10.0</max_depth>\r\n      <update_rate>30.0</update_rate>\r\n    </plugin>\r\n  </sensor>\r\n</gazebo>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"depth-camera-simulation",children:"Depth Camera Simulation"}),"\n",(0,a.jsx)(e.p,{children:"For 3D perception, depth cameras are crucial:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<gazebo reference="depth_camera">\r\n  <sensor name="depth_camera_sensor" type="depth">\r\n    <always_on>true</always_on>\r\n    <update_rate>30</update_rate>\r\n    <camera name="depth_camera">\r\n      <horizontal_fov>1.047</horizontal_fov>\r\n      <image>\r\n        <width>640</width>\r\n        <height>480</height>\r\n        <format>R8G8B8</format>\r\n      </image>\r\n      <clip>\r\n        <near>0.1</near>\r\n        <far>10.0</far>\r\n      </clip>\r\n    </camera>\r\n    <plugin name="depth_camera_controller" filename="libgazebo_ros_openni_kinect.so">\r\n      <baseline>0.2</baseline>\r\n      <distortion_k1>0.0</distortion_k1>\r\n      <distortion_k2>0.0</distortion_k2>\r\n      <distortion_k3>0.0</distortion_k3>\r\n      <distortion_t1>0.0</distortion_t1>\r\n      <distortion_t2>0.0</distortion_t2>\r\n      <point_cloud_cutoff>0.1</point_cloud_cutoff>\r\n      <point_cloud_cutoff_max>10.0</point_cloud_cutoff_max>\r\n      <frame_name>depth_camera_optical_frame</frame_name>\r\n      <point_cloud_transport_hints>raw</point_cloud_transport_hints>\r\n      <update_rate>30.0</update_rate>\r\n    </plugin>\r\n  </sensor>\r\n</gazebo>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"lidar-simulation",children:"LIDAR Simulation"}),"\n",(0,a.jsx)(e.p,{children:"LIDAR sensors provide distance measurements for mapping and navigation:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<gazebo reference="laser_link">\r\n  <sensor name="laser_sensor" type="ray">\r\n    <always_on>true</always_on>\r\n    <update_rate>40</update_rate>\r\n    <ray>\r\n      <scan>\r\n        <horizontal>\r\n          <samples>720</samples>\r\n          <resolution>1</resolution>\r\n          <min_angle>-1.570796</min_angle> \x3c!-- -90 degrees --\x3e\r\n          <max_angle>1.570796</max_angle>   \x3c!-- 90 degrees --\x3e\r\n        </horizontal>\r\n      </scan>\r\n      <range>\r\n        <min>0.1</min>\r\n        <max>30.0</max>\r\n        <resolution>0.01</resolution>\r\n      </range>\r\n      <noise>\r\n        <type>gaussian</type>\r\n        <mean>0.0</mean>\r\n        <stddev>0.01</stddev>\r\n      </noise>\r\n    </ray>\r\n    <plugin name="laser_controller" filename="libgazebo_ros_ray_sensor.so">\r\n      <ros>\r\n        <namespace>/my_humanoid</namespace>\r\n        <remapping>~/out:=scan</remapping>\r\n      </ros>\r\n      <output_type>sensor_msgs/LaserScan</output_type>\r\n      <frame_name>laser_link</frame_name>\r\n    </plugin>\r\n  </sensor>\r\n</gazebo>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,a.jsx)(e.p,{children:"IMUs are critical for balance and orientation in humanoid robots:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<gazebo reference="imu_link">\r\n  <sensor name="imu_sensor" type="imu">\r\n    <always_on>true</always_on>\r\n    <update_rate>100</update_rate>\r\n    <imu>\r\n      <angular_velocity>\r\n        <x>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>0.0017</stddev> \x3c!-- ~0.1 deg/s --\x3e\r\n            <bias_mean>0.0000</bias_mean>\r\n            <bias_stddev>0.0001</bias_stddev>\r\n          </noise>\r\n        </x>\r\n        <y>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>0.0017</stddev>\r\n            <bias_mean>0.0000</bias_mean>\r\n            <bias_stddev>0.0001</bias_stddev>\r\n          </noise>\r\n        </y>\r\n        <z>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>0.0017</stddev>\r\n            <bias_mean>0.0000</bias_mean>\r\n            <bias_stddev>0.0001</bias_stddev>\r\n          </noise>\r\n        </z>\r\n      </angular_velocity>\r\n      <linear_acceleration>\r\n        <x>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>1.7e-2</stddev>\r\n            <bias_mean>0.0</bias_mean>\r\n            <bias_stddev>1.7e-3</bias_stddev>\r\n          </noise>\r\n        </x>\r\n        <y>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>1.7e-2</stddev>\r\n            <bias_mean>0.0</bias_mean>\r\n            <bias_stddev>1.7e-3</bias_stddev>\r\n          </noise>\r\n        </y>\r\n        <z>\r\n          <noise type="gaussian">\r\n            <mean>0.0</mean>\r\n            <stddev>1.7e-2</stddev>\r\n            <bias_mean>0.0</bias_mean>\r\n            <bias_stddev>1.7e-3</bias_stddev>\r\n          </noise>\r\n        </z>\r\n      </linear_acceleration>\r\n    </imu>\r\n    <plugin name="imu_controller" filename="libgazebo_ros_imu.so">\r\n      <ros>\r\n        <namespace>/my_humanoid</namespace>\r\n        <remapping>~/out:=imu</remapping>\r\n      </ros>\r\n      <frame_name>imu_link</frame_name>\r\n      <body_name>torso</body_name>\r\n      <update_rate>100</update_rate>\r\n    </plugin>\r\n  </sensor>\r\n</gazebo>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"forcetorque-sensor-simulation",children:"Force/Torque Sensor Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Force/torque sensors are essential for humanoid balance and manipulation:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<gazebo>\r\n  <plugin name="left_foot_ft_sensor" filename="libgazebo_ros_ft_sensor.so">\r\n    <updateRate>100.0</updateRate>\r\n    <topicName>/my_humanoid/left_foot/force_torque</topicName>\r\n    <jointName>left_ankle_roll</jointName> \x3c!-- Joint to measure forces at --\x3e\r\n  </plugin>\r\n</gazebo>\n'})}),"\n",(0,a.jsx)(e.h2,{id:"sensor-noise-and-realism",children:"Sensor Noise and Realism"}),"\n",(0,a.jsx)(e.h3,{id:"adding-realistic-noise-models",children:"Adding Realistic Noise Models"}),"\n",(0,a.jsx)(e.p,{children:"Real sensors have noise characteristics that must be simulated:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Example of realistic camera noise --\x3e\r\n<camera name="realistic_camera">\r\n  <image>\r\n    <width>640</width>\r\n    <height>480</height>\r\n    <format>R8G8B8</format>\r\n  </image>\r\n  <noise>\r\n    <type>gaussian</type>\r\n    <mean>0.0</mean>\r\n    <stddev>0.01</stddev> \x3c!-- 1% noise --\x3e\r\n  </noise>\r\n</camera>\r\n\r\n\x3c!-- Example of realistic LIDAR noise --\x3e\r\n<ray>\r\n  <scan>\r\n    <horizontal>\r\n      <samples>360</samples>\r\n      <resolution>1</resolution>\r\n      <min_angle>-3.14159</min_angle>\r\n      <max_angle>3.14159</max_angle>\r\n    </horizontal>\r\n  </scan>\r\n  <range>\r\n    <min>0.1</min>\r\n    <max>10.0</max>\r\n    <resolution>0.01</resolution>\r\n  </range>\r\n  <noise>\r\n    <type>gaussian</type>\r\n    <mean>0.0</mean>\r\n    <stddev>0.02</stddev> \x3c!-- 2cm noise at 1m --\x3e\r\n  </noise>\r\n</ray>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"dynamic-noise-models",children:"Dynamic Noise Models"}),"\n",(0,a.jsx)(e.p,{children:"For more realistic simulation, noise characteristics can vary based on environmental conditions:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan\r\nimport numpy as np\r\n\r\nclass DynamicNoiseSimulator(Node):\r\n    def __init__(self):\r\n        super().__init__('dynamic_noise_simulator')\r\n\r\n        # Subscribe to raw sensor data\r\n        self.raw_scan_sub = self.create_subscription(\r\n            LaserScan, '/gazebo/scan', self.raw_scan_callback, 10)\r\n\r\n        # Publish noisy sensor data\r\n        self.noisy_scan_pub = self.create_publisher(\r\n            LaserScan, '/scan', 10)\r\n\r\n    def raw_scan_callback(self, msg):\r\n        \"\"\"Add dynamic noise to laser scan data\"\"\"\r\n        noisy_msg = LaserScan()\r\n        noisy_msg.header = msg.header\r\n        noisy_msg.angle_min = msg.angle_min\r\n        noisy_msg.angle_max = msg.angle_max\r\n        noisy_msg.angle_increment = msg.angle_increment\r\n        noisy_msg.time_increment = msg.time_increment\r\n        noisy_msg.scan_time = msg.scan_time\r\n        noisy_msg.range_min = msg.range_min\r\n        noisy_msg.range_max = msg.range_max\r\n\r\n        # Add distance-dependent noise\r\n        noisy_ranges = []\r\n        for i, original_range in enumerate(msg.ranges):\r\n            if np.isfinite(original_range):\r\n                # Noise increases with distance (beam divergence)\r\n                noise_std = 0.01 + 0.005 * original_range  # 1cm + 0.5% of range\r\n                noise = np.random.normal(0, noise_std)\r\n                noisy_range = max(\r\n                    msg.range_min,\r\n                    min(msg.range_max, original_range + noise)\r\n                )\r\n            else:\r\n                noisy_range = original_range  # Keep inf or nan as is\r\n\r\n            noisy_ranges.append(noisy_range)\r\n\r\n        noisy_msg.ranges = noisy_ranges\r\n        self.noisy_scan_pub.publish(noisy_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = DynamicNoiseSimulator()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"multi-sensor-integration",children:"Multi-Sensor Integration"}),"\n",(0,a.jsx)(e.h3,{id:"sensor-fusion-node",children:"Sensor Fusion Node"}),"\n",(0,a.jsx)(e.p,{children:"Humanoid robots often combine multiple sensors for enhanced perception:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan, Imu, Image\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport tf2_ros\r\nfrom tf2_ros import TransformException\r\n\r\nclass SensorFusionNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'sensor_fusion_node\')\r\n\r\n        # Initialize CV bridge\r\n        self.cv_bridge = CvBridge()\r\n\r\n        # Sensor data storage\r\n        self.laser_data = None\r\n        self.imu_data = None\r\n        self.camera_data = None\r\n\r\n        # Subscribers for different sensors\r\n        self.laser_sub = self.create_subscription(\r\n            LaserScan, \'/scan\', self.laser_callback, 10)\r\n        self.imu_sub = self.create_subscription(\r\n            Imu, \'/imu\', self.imu_callback, 10)\r\n        self.camera_sub = self.create_subscription(\r\n            Image, \'/camera/image_raw\', self.camera_callback, 10)\r\n\r\n        # Publisher for fused perception\r\n        self.perception_pub = self.create_publisher(\r\n            PoseStamped, \'/perception/target_pose\', 10)\r\n\r\n        # TF buffer for coordinate transforms\r\n        self.tf_buffer = tf2_ros.Buffer()\r\n        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self)\r\n\r\n        # Processing timer\r\n        self.fusion_timer = self.create_timer(0.1, self.sensor_fusion_process)\r\n\r\n    def laser_callback(self, msg):\r\n        """Process laser scan data"""\r\n        self.laser_data = msg\r\n\r\n    def imu_callback(self, msg):\r\n        """Process IMU data"""\r\n        self.imu_data = msg\r\n\r\n    def camera_callback(self, msg):\r\n        """Process camera data"""\r\n        try:\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\r\n            self.camera_data = cv_image\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Camera data conversion failed: {e}\')\r\n\r\n    def sensor_fusion_process(self):\r\n        """Process and fuse sensor data"""\r\n        if all([self.laser_data, self.imu_data, self.camera_data]):\r\n            # Example: Combine visual and LIDAR data to detect objects\r\n            target_pose = self.fuse_visual_lidar_data()\r\n\r\n            if target_pose is not None:\r\n                self.perception_pub.publish(target_pose)\r\n\r\n    def fuse_visual_lidar_data(self):\r\n        """Example fusion algorithm"""\r\n        # Convert camera image to detect objects\r\n        # Use LIDAR to get distance to detected objects\r\n        # Use IMU to account for robot orientation\r\n        # Return fused target pose in world coordinates\r\n\r\n        # This is a simplified example\r\n        # Real fusion would involve complex algorithms\r\n        return None\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = SensorFusionNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"calibration-and-validation",children:"Calibration and Validation"}),"\n",(0,a.jsx)(e.h3,{id:"sensor-calibration",children:"Sensor Calibration"}),"\n",(0,a.jsx)(e.p,{children:"Virtual sensors need to be calibrated to match real sensor characteristics:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import CameraInfo\r\nfrom cv2 import FileStorage, FileNode\r\n\r\nclass SensorCalibrator(Node):\r\n    def __init__(self):\r\n        super().__init__('sensor_calibrator')\r\n\r\n        # Publisher for calibrated camera info\r\n        self.camera_info_pub = self.create_publisher(\r\n            CameraInfo, '/camera/camera_info', 10)\r\n\r\n        # Load calibration parameters\r\n        self.calibration_params = self.load_calibration('camera_calibration.yaml')\r\n\r\n        # Timer to publish calibration info\r\n        self.calibration_timer = self.create_timer(1.0, self.publish_calibration)\r\n\r\n    def load_calibration(self, filename):\r\n        \"\"\"Load calibration parameters from file\"\"\"\r\n        # In practice, load from YAML or other calibration file\r\n        params = {\r\n            'camera_matrix': [[525.0, 0.0, 319.5],\r\n                              [0.0, 525.0, 239.5],\r\n                              [0.0, 0.0, 1.0]],\r\n            'distortion_coefficients': [0.0, 0.0, 0.0, 0.0, 0.0],\r\n            'image_width': 640,\r\n            'image_height': 480\r\n        }\r\n        return params\r\n\r\n    def publish_calibration(self):\r\n        \"\"\"Publish camera calibration info\"\"\"\r\n        msg = CameraInfo()\r\n        msg.width = self.calibration_params['image_width']\r\n        msg.height = self.calibration_params['image_height']\r\n        msg.k = self.calibration_params['camera_matrix']\r\n        msg.d = self.calibration_params['distortion_coefficients']\r\n\r\n        # Set other calibration parameters\r\n        msg.header.frame_id = 'camera_optical_frame'\r\n        self.camera_info_pub.publish(msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = SensorCalibrator()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"advanced-sensor-simulation",children:"Advanced Sensor Simulation"}),"\n",(0,a.jsx)(e.h3,{id:"custom-sensor-plugins",children:"Custom Sensor Plugins"}),"\n",(0,a.jsx)(e.p,{children:"For specialized sensors, you can create custom Gazebo plugins:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-cpp",children:'#include <gazebo/gazebo.hh>\r\n#include <gazebo/sensors/sensors.hh>\r\n#include <gazebo/physics/physics.hh>\r\n#include <ros/ros.h>\r\n#include <sensor_msgs/Range.h>\r\n\r\nnamespace gazebo\r\n{\r\n  class TactileSensorPlugin : public SensorPlugin\r\n  {\r\n    public: void Load(sensors::SensorPtr _sensor, sdf::ElementPtr _sdf)\r\n    {\r\n      // Get the range sensor\r\n      this->parentSensor =\r\n        std::dynamic_pointer_cast<sensors::RaySensor>(_sensor);\r\n\r\n      if (!this->parentSensor)\r\n      {\r\n        gzerr << "TactileSensorPlugin not attached to a ray sensor\\n";\r\n        return;\r\n      }\r\n\r\n      // Connect to the sensor update event\r\n      this->updateConnection = this->parentSensor->ConnectUpdated(\r\n          std::bind(&TactileSensorPlugin::OnUpdate, this));\r\n\r\n      // Make sure the parent sensor is active\r\n      this->parentSensor->SetActive(true);\r\n\r\n      // Initialize ROS\r\n      if (!ros::isInitialized())\r\n      {\r\n        int argc = 0;\r\n        char **argv = NULL;\r\n        ros::init(argc, argv, "gazebo_client",\r\n                  ros::init_options::NoSigintHandler);\r\n      }\r\n\r\n      this->rosnode = new ros::NodeHandle("gazebo_client");\r\n      this->pub = this->rosnode->advertise<sensor_msgs::Range>(\r\n          "/tactile_sensor", 1);\r\n    }\r\n\r\n    public: void OnUpdate()\r\n    {\r\n      // Get range data from sensor\r\n      double range = this->parentSensor->Range(0);\r\n\r\n      // Publish tactile sensor data\r\n      sensor_msgs::Range msg;\r\n      msg.header.stamp = ros::Time::now();\r\n      msg.header.frame_id = "tactile_sensor_frame";\r\n      msg.radiation_type = sensor_msgs::Range::ULTRASOUND;\r\n      msg.field_of_view = 0.1;\r\n      msg.min_range = 0.01;\r\n      msg.max_range = 1.0;\r\n      msg.range = range;\r\n\r\n      this->pub.publish(msg);\r\n    }\r\n\r\n    private: sensors::RaySensorPtr parentSensor;\r\n    private: ros::NodeHandle* rosnode;\r\n    private: ros::Publisher pub;\r\n    private: event::ConnectionPtr updateConnection;\r\n  };\r\n\r\n  GZ_REGISTER_SENSOR_PLUGIN(TactileSensorPlugin)\r\n}\n'})}),"\n",(0,a.jsx)(e.h3,{id:"multi-modal-sensor-simulation",children:"Multi-Modal Sensor Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Humanoid robots often require complex multi-modal sensor simulation:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Multi-modal head sensor array --\x3e\r\n<gazebo reference="head_link">\r\n  \x3c!-- RGB Camera --\x3e\r\n  <sensor name="rgb_camera" type="camera">\r\n    <pose>0.1 0 0.05 0 0 0</pose>\r\n    <camera name="head_camera">\r\n      <horizontal_fov>1.047</horizontal_fov>\r\n      <image><width>640</width><height>480</height><format>R8G8B8</format></image>\r\n      <clip><near>0.1</near><far>10</far></clip>\r\n    </camera>\r\n    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\r\n      <frame_name>head_camera_optical_frame</frame_name>\r\n    </plugin>\r\n  </sensor>\r\n\r\n  \x3c!-- Depth Camera --\x3e\r\n  <sensor name="depth_camera" type="depth">\r\n    <pose>0.1 0.05 0.05 0 0 0</pose>\r\n    <camera name="head_depth_camera">\r\n      <horizontal_fov>1.047</horizontal_fov>\r\n      <image><width>320</width><height>240</height><format>L8</format></image>\r\n      <clip><near>0.3</near><far>5</far></clip>\r\n    </camera>\r\n    <plugin name="depth_controller" filename="libgazebo_ros_openni_kinect.so">\r\n      <frame_name>head_depth_optical_frame</frame_name>\r\n    </plugin>\r\n  </sensor>\r\n\r\n  \x3c!-- Microphone Array --\x3e\r\n  <sensor name="microphone_array" type="audio">\r\n    <pose>0.05 0.05 0 0 0 0</pose>\r\n    <always_on>true</always_on>\r\n    <update_rate>44100</update_rate>\r\n  </sensor>\r\n</gazebo>\n'})}),"\n",(0,a.jsx)(e.h2,{id:"sensor-data-processing",children:"Sensor Data Processing"}),"\n",(0,a.jsx)(e.h3,{id:"real-time-sensor-processing-node",children:"Real-time Sensor Processing Node"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan, Image, Imu\r\nfrom std_msgs.msg import Float32MultiArray\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport cv2\r\nfrom collections import deque\r\n\r\nclass SensorProcessingNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'sensor_processing_node\')\r\n\r\n        # Initialize CV bridge\r\n        self.cv_bridge = CvBridge()\r\n\r\n        # Sensor data buffers\r\n        self.scan_buffer = deque(maxlen=10)\r\n        self.imu_buffer = deque(maxlen=100)\r\n        self.image_buffer = deque(maxlen=5)\r\n\r\n        # Subscribers\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan, \'/scan\', self.scan_callback, 10)\r\n        self.imu_sub = self.create_subscription(\r\n            Imu, \'/imu\', self.imu_callback, 10)\r\n        self.image_sub = self.create_subscription(\r\n            Image, \'/camera/image_raw\', self.image_callback, 10)\r\n\r\n        # Publishers for processed data\r\n        self.obstacle_pub = self.create_publisher(\r\n            Float32MultiArray, \'/processed/obstacles\', 10)\r\n        self.balance_pub = self.create_publisher(\r\n            Float32MultiArray, \'/processed/balance\', 10)\r\n\r\n        # Processing timer\r\n        self.process_timer = self.create_timer(0.05, self.process_sensors)\r\n\r\n    def scan_callback(self, msg):\r\n        """Store laser scan data"""\r\n        self.scan_buffer.append(msg)\r\n\r\n    def imu_callback(self, msg):\r\n        """Store IMU data"""\r\n        self.imu_buffer.append(msg)\r\n\r\n    def image_callback(self, msg):\r\n        """Store image data"""\r\n        try:\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\r\n            self.image_buffer.append(cv_image)\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Image conversion failed: {e}\')\r\n\r\n    def process_sensors(self):\r\n        """Process sensor data in real-time"""\r\n        if len(self.scan_buffer) > 0:\r\n            # Process laser scan for obstacle detection\r\n            obstacles = self.detect_obstacles()\r\n            if obstacles is not None:\r\n                obstacle_msg = Float32MultiArray()\r\n                obstacle_msg.data = obstacles\r\n                self.obstacle_pub.publish(obstacle_msg)\r\n\r\n        if len(self.imu_buffer) > 0:\r\n            # Process IMU data for balance\r\n            balance_data = self.calculate_balance()\r\n            if balance_data is not None:\r\n                balance_msg = Float32MultiArray()\r\n                balance_msg.data = balance_data\r\n                self.balance_pub.publish(balance_msg)\r\n\r\n    def detect_obstacles(self):\r\n        """Detect obstacles from laser scan"""\r\n        if len(self.scan_buffer) == 0:\r\n            return None\r\n\r\n        latest_scan = self.scan_buffer[-1]\r\n        ranges = np.array(latest_scan.ranges)\r\n\r\n        # Remove invalid ranges\r\n        valid_ranges = ranges[np.isfinite(ranges)]\r\n\r\n        # Detect obstacles within 1m\r\n        obstacle_distances = valid_ranges[valid_ranges < 1.0]\r\n\r\n        if len(obstacle_distances) > 0:\r\n            # Return minimum distance and count\r\n            return [np.min(obstacle_distances), len(obstacle_distances)]\r\n        else:\r\n            return [float(\'inf\'), 0]\r\n\r\n    def calculate_balance(self):\r\n        """Calculate balance metrics from IMU data"""\r\n        if len(self.imu_buffer) < 10:\r\n            return None\r\n\r\n        # Get recent IMU data\r\n        recent_imu = list(self.imu_buffer)[-10:]\r\n\r\n        # Calculate average orientation\r\n        roll_avg = np.mean([self.quat_to_roll_pitch_yaw(\r\n            imu.orientation)[0] for imu in recent_imu])\r\n        pitch_avg = np.mean([self.quat_to_roll_pitch_yaw(\r\n            imu.orientation)[1] for imu in recent_imu])\r\n\r\n        # Calculate angular velocity\r\n        angular_vel = np.mean([\r\n            np.sqrt(imu.angular_velocity.x**2 +\r\n                   imu.angular_velocity.y**2 +\r\n                   imu.angular_velocity.z**2)\r\n            for imu in recent_imu\r\n        ])\r\n\r\n        return [roll_avg, pitch_avg, angular_vel]\r\n\r\n    def quat_to_roll_pitch_yaw(self, quat):\r\n        """Convert quaternion to roll-pitch-yaw"""\r\n        import math\r\n        w, x, y, z = quat.w, quat.x, quat.y, quat.z\r\n\r\n        # Roll (x-axis rotation)\r\n        sinr_cosp = 2 * (w * x + y * z)\r\n        cosr_cosp = 1 - 2 * (x * x + y * y)\r\n        roll = math.atan2(sinr_cosp, cosr_cosp)\r\n\r\n        # Pitch (y-axis rotation)\r\n        sinp = 2 * (w * y - z * x)\r\n        pitch = math.asin(sinp)\r\n\r\n        # Yaw (z-axis rotation)\r\n        siny_cosp = 2 * (w * z + x * y)\r\n        cosy_cosp = 1 - 2 * (y * y + z * z)\r\n        yaw = math.atan2(siny_cosp, cosy_cosp)\r\n\r\n        return roll, pitch, yaw\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = SensorProcessingNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"sim-to-real-transfer-considerations",children:"Sim-to-Real Transfer Considerations"}),"\n",(0,a.jsx)(e.h3,{id:"handling-the-reality-gap",children:"Handling the Reality Gap"}),"\n",(0,a.jsx)(e.p,{children:'One of the main challenges in sensor simulation is the "reality gap":'}),"\n",(0,a.jsx)(e.h4,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Example: Randomizing sensor parameters during training\r\nclass DomainRandomizationNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'domain_randomization\')\r\n\r\n        # Randomize sensor parameters within realistic bounds\r\n        self.noise_std_range = [0.005, 0.02]  # Different noise levels\r\n        self.bias_range = [-0.001, 0.001]     # Different biases\r\n\r\n    def randomize_sensor_params(self):\r\n        """Randomize sensor parameters for training"""\r\n        noise_std = np.random.uniform(*self.noise_std_range)\r\n        bias = np.random.uniform(*self.bias_range)\r\n        return noise_std, bias\n'})}),"\n",(0,a.jsx)(e.h4,{id:"sensor-simulation-accuracy",children:"Sensor Simulation Accuracy"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Validate simulation against real sensor data"}),"\n",(0,a.jsx)(e.li,{children:"Use real-world calibration parameters"}),"\n",(0,a.jsx)(e.li,{children:"Implement realistic noise models"}),"\n",(0,a.jsx)(e.li,{children:"Test edge cases and failure modes"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsx)(e.h3,{id:"1-sensor-placement",children:"1. Sensor Placement"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Position sensors to match real robot configuration"}),"\n",(0,a.jsx)(e.li,{children:"Consider field of view and sensing range"}),"\n",(0,a.jsx)(e.li,{children:"Avoid occlusions where possible"}),"\n",(0,a.jsx)(e.li,{children:"Account for robot kinematics"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"2-computational-performance",children:"2. Computational Performance"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Balance sensor fidelity with simulation speed"}),"\n",(0,a.jsx)(e.li,{children:"Use appropriate update rates"}),"\n",(0,a.jsx)(e.li,{children:"Consider sensor data compression"}),"\n",(0,a.jsx)(e.li,{children:"Optimize sensor processing pipelines"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"3-validation-and-testing",children:"3. Validation and Testing"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Compare simulated vs. real sensor data"}),"\n",(0,a.jsx)(e.li,{children:"Test sensor fusion algorithms"}),"\n",(0,a.jsx)(e.li,{children:"Validate under various environmental conditions"}),"\n",(0,a.jsx)(e.li,{children:"Test sensor failure scenarios"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"Sensor simulation is a critical component of humanoid robot simulation, enabling safe testing and development of perception and control algorithms. Accurate simulation of various sensor types - cameras, LIDAR, IMU, force/torque sensors - is essential for effective sim-to-real transfer."}),"\n",(0,a.jsx)(e.p,{children:"The key to successful sensor simulation lies in understanding the characteristics of real sensors and implementing appropriate noise models, calibration parameters, and processing pipelines. Multi-sensor fusion and real-time processing add additional complexity but are essential for humanoid robot operation."}),"\n",(0,a.jsx)(e.p,{children:"In the next chapter, we'll explore environment building in Gazebo, focusing on creating realistic environments for humanoid robot testing and development."})]})}function m(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>o,x:()=>t});var a=r(6540);const s={},i=a.createContext(s);function o(n){const e=a.useContext(i);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),a.createElement(i.Provider,{value:e},n.children)}}}]);