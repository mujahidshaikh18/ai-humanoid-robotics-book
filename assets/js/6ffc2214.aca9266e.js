"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[997],{476:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>l});var t=r(4848),i=r(8453);const o={sidebar_position:17,title:"Chapter 17: Voice-to-Action with Whisper"},a="Voice-to-Action with Whisper",s={id:"module-4/chapter-17",title:"Chapter 17: Voice-to-Action with Whisper",description:"Overview",source:"@site/docs/module-4/chapter-17.mdx",sourceDirName:"module-4",slug:"/module-4/chapter-17",permalink:"/ai-humanoid-robotics-book/docs/module-4/chapter-17",draft:!1,unlisted:!1,editUrl:"https://github.com/mujahidshaikh18/ai-humanoid-robotics-book/tree/main/docs/module-4/chapter-17.mdx",tags:[],version:"current",sidebarPosition:17,frontMatter:{sidebar_position:17,title:"Chapter 17: Voice-to-Action with Whisper"},sidebar:"tutorialSidebar",previous:{title:"Chapter 16: Introduction to VLA Systems",permalink:"/ai-humanoid-robotics-book/docs/module-4/chapter-16"},next:{title:"Chapter 18: LLM-Based Cognitive Planning",permalink:"/ai-humanoid-robotics-book/docs/module-4/chapter-18"}},c={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Whisper for Robotics",id:"introduction-to-whisper-for-robotics",level:2},{value:"Whisper Architecture",id:"whisper-architecture",level:3},{value:"Whisper in Robotics Context",id:"whisper-in-robotics-context",level:3},{value:"Whisper Integration Implementation",id:"whisper-integration-implementation",level:2},{value:"Basic Whisper Setup",id:"basic-whisper-setup",level:3},{value:"Voice Command Grammar and Parsing",id:"voice-command-grammar-and-parsing",level:2},{value:"Command Grammar System",id:"command-grammar-system",level:3},{value:"Real-Time Voice Processing",id:"real-time-voice-processing",level:2},{value:"Streaming Voice Recognition",id:"streaming-voice-recognition",level:3},{value:"Voice Command Integration with Robotics",id:"voice-command-integration-with-robotics",level:2},{value:"Robot Action Execution",id:"robot-action-execution",level:3},{value:"Noise Reduction and Audio Enhancement",id:"noise-reduction-and-audio-enhancement",level:2},{value:"Audio Preprocessing for Robotics",id:"audio-preprocessing-for-robotics",level:3},{value:"Integration with Humanoid Robot Systems",id:"integration-with-humanoid-robot-systems",level:2},{value:"Complete Voice-to-Action System",id:"complete-voice-to-action-system",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Real-Time Performance Considerations",id:"real-time-performance-considerations",level:3},{value:"Error Handling and Robustness",id:"error-handling-and-robustness",level:2},{value:"Handling Recognition Errors",id:"handling-recognition-errors",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"voice-to-action-with-whisper",children:"Voice-to-Action with Whisper"}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"Voice-to-action systems enable humanoid robots to understand and execute spoken commands, providing a natural and intuitive interface for human-robot interaction. OpenAI's Whisper model has revolutionized automatic speech recognition (ASR) by providing robust, multilingual transcription capabilities that can be integrated into robotics applications. This chapter explores the implementation of Whisper-based voice-to-action systems for humanoid robots."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand the architecture and capabilities of Whisper for speech recognition"}),"\n",(0,t.jsx)(n.li,{children:"Implement Whisper integration for real-time voice command processing"}),"\n",(0,t.jsx)(n.li,{children:"Design voice command grammars and parsing systems"}),"\n",(0,t.jsx)(n.li,{children:"Handle speech recognition challenges in robotics environments"}),"\n",(0,t.jsx)(n.li,{children:"Integrate voice-to-action with other VLA system components"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-whisper-for-robotics",children:"Introduction to Whisper for Robotics"}),"\n",(0,t.jsx)(n.h3,{id:"whisper-architecture",children:"Whisper Architecture"}),"\n",(0,t.jsx)(n.p,{children:"Whisper is a general-purpose speech recognition model developed by OpenAI that excels at transcribing audio across multiple languages. For robotics applications, Whisper offers several advantages:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robustness"}),": Performs well in noisy environments"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multilingual"}),": Supports multiple languages without retraining"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Zero-shot"}),": Can transcribe without domain-specific fine-tuning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time Capability"}),": Can be optimized for real-time processing"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"whisper-in-robotics-context",children:"Whisper in Robotics Context"}),"\n",(0,t.jsx)(n.p,{children:"In humanoid robotics, Whisper serves as the bridge between human speech and robot action execution:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   Human Voice   \u2502    \u2502   Whisper ASR   \u2502    \u2502   Command       \u2502\r\n\u2502   Commands      \u2502\u2500\u2500\u2500\u25b6\u2502   (Transcribe)  \u2502\u2500\u2500\u2500\u25b6\u2502   Processing    \u2502\r\n\u2502   (Audio)       \u2502    \u2502   (Text)        \u2502    \u2502   (Parse &      \u2502\r\n\u2502                 \u2502    \u2502                 \u2502    \u2502   Execute)      \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n         \u2502                       \u2502                       \u2502\r\n         \u25bc                       \u25bc                       \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   Microphone    \u2502    \u2502   Text Buffer   \u2502    \u2502   Robot Action  \u2502\r\n\u2502   Array         \u2502    \u2502   & Processing  \u2502    \u2502   Execution     \u2502\r\n\u2502   (Capture)     \u2502    \u2502   (Filter &     \u2502    \u2502   (Movement,    \u2502\r\n\u2502                 \u2502    \u2502   Interpret)    \u2502    \u2502   Manipulation) \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h2,{id:"whisper-integration-implementation",children:"Whisper Integration Implementation"}),"\n",(0,t.jsx)(n.h3,{id:"basic-whisper-setup",children:"Basic Whisper Setup"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport whisper\r\nimport numpy as np\r\nimport pyaudio\r\nimport wave\r\nimport threading\r\nimport queue\r\nimport time\r\nfrom dataclasses import dataclass\r\nfrom typing import Optional, Callable, List\r\n\r\n@dataclass\r\nclass VoiceCommand:\r\n    """Structure for processed voice commands"""\r\n    text: str\r\n    confidence: float\r\n    timestamp: float\r\n    command_type: str\r\n    parameters: dict\r\n\r\nclass WhisperVoiceProcessor:\r\n    def __init__(self, model_size="base", device=None):\r\n        """\r\n        Initialize Whisper voice processor\r\n\r\n        Args:\r\n            model_size: Size of Whisper model (\'tiny\', \'base\', \'small\', \'medium\', \'large\')\r\n            device: Device to run model on (\'cpu\', \'cuda\', or None for auto)\r\n        """\r\n        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")\r\n\r\n        # Load Whisper model\r\n        print(f"Loading Whisper {model_size} model on {self.device}...")\r\n        self.model = whisper.load_model(model_size).to(self.device)\r\n\r\n        # Audio parameters\r\n        self.sample_rate = 16000  # Whisper expects 16kHz\r\n        self.chunk_size = 1024   # Audio chunk size for streaming\r\n        self.audio_format = pyaudio.paInt16\r\n        self.channels = 1\r\n\r\n        # Audio recording setup\r\n        self.audio = pyaudio.PyAudio()\r\n        self.stream = None\r\n        self.is_listening = False\r\n\r\n        # Command processing\r\n        self.command_queue = queue.Queue()\r\n        self.audio_buffer = []\r\n        self.min_audio_length = 0.5  # Minimum audio length in seconds\r\n        self.max_audio_length = 10.0  # Maximum audio length in seconds\r\n\r\n        # Callbacks\r\n        self.command_callbacks = []\r\n        self.listening_callbacks = []\r\n\r\n    def start_listening(self):\r\n        """Start audio recording and voice processing"""\r\n        if self.is_listening:\r\n            return\r\n\r\n        self.is_listening = True\r\n\r\n        # Open audio stream\r\n        self.stream = self.audio.open(\r\n            format=self.audio_format,\r\n            channels=self.channels,\r\n            rate=self.sample_rate,\r\n            input=True,\r\n            frames_per_buffer=self.chunk_size\r\n        )\r\n\r\n        # Start recording thread\r\n        self.recording_thread = threading.Thread(target=self._recording_loop)\r\n        self.recording_thread.daemon = True\r\n        self.recording_thread.start()\r\n\r\n        # Start processing thread\r\n        self.processing_thread = threading.Thread(target=self._processing_loop)\r\n        self.processing_thread.daemon = True\r\n        self.processing_thread.start()\r\n\r\n        # Notify listeners\r\n        for callback in self.listening_callbacks:\r\n            callback("started")\r\n\r\n    def stop_listening(self):\r\n        """Stop audio recording and voice processing"""\r\n        self.is_listening = False\r\n\r\n        if self.stream:\r\n            self.stream.stop_stream()\r\n            self.stream.close()\r\n            self.stream = None\r\n\r\n        # Notify listeners\r\n        for callback in self.listening_callbacks:\r\n            callback("stopped")\r\n\r\n    def _recording_loop(self):\r\n        """Continuous audio recording loop"""\r\n        print("Voice recording started...")\r\n\r\n        while self.is_listening:\r\n            try:\r\n                # Read audio chunk\r\n                data = self.stream.read(self.chunk_size, exception_on_overflow=False)\r\n                audio_array = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\r\n\r\n                # Add to buffer\r\n                self.audio_buffer.extend(audio_array)\r\n\r\n                # Limit buffer size to prevent excessive memory usage\r\n                max_buffer_samples = int(self.max_audio_length * self.sample_rate)\r\n                if len(self.audio_buffer) > max_buffer_samples:\r\n                    self.audio_buffer = self.audio_buffer[-max_buffer_samples:]\r\n\r\n            except Exception as e:\r\n                print(f"Recording error: {e}")\r\n                time.sleep(0.1)\r\n\r\n    def _processing_loop(self):\r\n        """Continuous audio processing loop"""\r\n        last_process_time = time.time()\r\n\r\n        while self.is_listening:\r\n            try:\r\n                current_time = time.time()\r\n\r\n                # Process audio if enough time has passed and buffer has sufficient audio\r\n                if (current_time - last_process_time > 0.5 and  # Process every 0.5 seconds\r\n                    len(self.audio_buffer) > self.min_audio_length * self.sample_rate):\r\n\r\n                    # Convert to numpy array\r\n                    audio_np = np.array(self.audio_buffer)\r\n\r\n                    # Check if audio has sufficient energy (not just noise)\r\n                    if self._has_sufficient_energy(audio_np):\r\n                        # Process the audio\r\n                        self._process_audio_chunk(audio_np)\r\n                        # Clear processed audio (keep some overlap for continuity)\r\n                        keep_samples = int(0.5 * self.sample_rate)  # Keep 0.5s overlap\r\n                        self.audio_buffer = self.audio_buffer[-keep_samples:]\r\n                    else:\r\n                        # Clear buffer if it\'s just noise\r\n                        self.audio_buffer = self.audio_buffer[-int(0.5 * self.sample_rate):]\r\n\r\n                    last_process_time = current_time\r\n                else:\r\n                    time.sleep(0.1)\r\n\r\n            except Exception as e:\r\n                print(f"Processing error: {e}")\r\n                time.sleep(0.1)\r\n\r\n    def _has_sufficient_energy(self, audio_data):\r\n        """Check if audio has sufficient energy to be speech"""\r\n        energy = np.mean(np.abs(audio_data))\r\n        return energy > 0.01  # Threshold for speech detection\r\n\r\n    def _process_audio_chunk(self, audio_chunk):\r\n        """Process an audio chunk with Whisper"""\r\n        try:\r\n            # Transcribe audio using Whisper\r\n            result = self.model.transcribe(\r\n                audio_chunk,\r\n                language=\'en\',  # Can be changed or set to None for auto-detection\r\n                fp16=self.device == \'cuda\'\r\n            )\r\n\r\n            if result and result["text"].strip():\r\n                # Calculate confidence based on log probability\r\n                confidence = self._calculate_confidence(result)\r\n\r\n                # Create voice command\r\n                command = VoiceCommand(\r\n                    text=result["text"].strip(),\r\n                    confidence=confidence,\r\n                    timestamp=time.time(),\r\n                    command_type="speech",\r\n                    parameters={}\r\n                )\r\n\r\n                # Add to command queue\r\n                self.command_queue.put(command)\r\n\r\n                # Process command\r\n                self._process_voice_command(command)\r\n\r\n        except Exception as e:\r\n            print(f"Whisper processing error: {e}")\r\n\r\n    def _calculate_confidence(self, result):\r\n        """Calculate confidence score from Whisper result"""\r\n        # Use average log probability if available\r\n        if "avg_logprob" in result:\r\n            # Convert log probability to confidence (higher is better)\r\n            # Log prob is typically negative, so we use its absolute value\r\n            avg_logprob = result["avg_logprob"]\r\n            # Normalize to 0-1 range (this is a simplified approach)\r\n            confidence = max(0, min(1, (avg_logprob + 2.0) / 2.0))  # Adjust range as needed\r\n        else:\r\n            # Fallback confidence\r\n            confidence = 0.8\r\n\r\n        return confidence\r\n\r\n    def _process_voice_command(self, command):\r\n        """Process recognized voice command"""\r\n        if command.confidence > 0.5:  # Only process confident transcriptions\r\n            print(f"Recognized: \'{command.text}\' (confidence: {command.confidence:.2f})")\r\n\r\n            # Notify all command callbacks\r\n            for callback in self.command_callbacks:\r\n                try:\r\n                    callback(command)\r\n                except Exception as e:\r\n                    print(f"Command callback error: {e}")\r\n\r\n    def add_command_callback(self, callback: Callable[[VoiceCommand], None]):\r\n        """Add callback for processed voice commands"""\r\n        self.command_callbacks.append(callback)\r\n\r\n    def add_listening_callback(self, callback: Callable[[str], None]):\r\n        """Add callback for listening state changes"""\r\n        self.listening_callbacks.append(callback)\r\n\r\n    def get_latest_commands(self, max_commands=5) -> List[VoiceCommand]:\r\n        """Get latest voice commands from queue"""\r\n        commands = []\r\n        while not self.command_queue.empty() and len(commands) < max_commands:\r\n            try:\r\n                command = self.command_queue.get_nowait()\r\n                commands.append(command)\r\n            except queue.Empty:\r\n                break\r\n        return commands\n'})}),"\n",(0,t.jsx)(n.h2,{id:"voice-command-grammar-and-parsing",children:"Voice Command Grammar and Parsing"}),"\n",(0,t.jsx)(n.h3,{id:"command-grammar-system",children:"Command Grammar System"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import re\r\nfrom typing import Dict, List, Tuple, Optional\r\nimport json\r\n\r\nclass VoiceCommandGrammar:\r\n    def __init__(self):\r\n        # Define command patterns\r\n        self.command_patterns = {\r\n            # Movement commands\r\n            "move_forward": {\r\n                "patterns": [\r\n                    r"move forward(?: by (\\d+(?:\\.\\d+)?))?",\r\n                    r"go forward(?: by (\\d+(?:\\.\\d+)?))?",\r\n                    r"walk forward(?: by (\\d+(?:\\.\\d+)?))?"\r\n                ],\r\n                "action": "move",\r\n                "params": {"direction": "forward"}\r\n            },\r\n            "move_backward": {\r\n                "patterns": [\r\n                    r"move backward(?: by (\\d+(?:\\.\\d+)?))?",\r\n                    r"go backward(?: by (\\d+(?:\\.\\d+)?))?",\r\n                    r"walk backward(?: by (\\d+(?:\\.\\d+)?))?"\r\n                ],\r\n                "action": "move",\r\n                "params": {"direction": "backward"}\r\n            },\r\n            "turn": {\r\n                "patterns": [\r\n                    r"turn (left|right)(?: by (\\d+(?:\\.\\d+)?))?",\r\n                    r"rotate (left|right)(?: by (\\d+(?:\\.\\d+)?))?",\r\n                    r"pivot (left|right)(?: by (\\d+(?:\\.\\d+)?))?"\r\n                ],\r\n                "action": "turn",\r\n                "params": {}\r\n            },\r\n            "pick_object": {\r\n                "patterns": [\r\n                    r"pick up (?:the )?(.+)",\r\n                    r"grab (?:the )?(.+)",\r\n                    r"take (?:the )?(.+)",\r\n                    r"get (?:the )?(.+)"\r\n                ],\r\n                "action": "pick",\r\n                "params": {}\r\n            },\r\n            "place_object": {\r\n                "patterns": [\r\n                    r"place (?:the )?(.+) (?:on|at|in) (?:the )?(.+)",\r\n                    r"put (?:the )?(.+) (?:on|at|in) (?:the )?(.+)",\r\n                    r"set (?:the )?(.+) (?:on|at|in) (?:the )?(.+)"\r\n                ],\r\n                "action": "place",\r\n                "params": {}\r\n            },\r\n            "look_at": {\r\n                "patterns": [\r\n                    r"look at (?:the )?(.+)",\r\n                    r"face (?:the )?(.+)",\r\n                    r"turn to (?:the )?(.+)",\r\n                    r"find (?:the )?(.+)"\r\n                ],\r\n                "action": "look_at",\r\n                "params": {}\r\n            },\r\n            "stop": {\r\n                "patterns": [\r\n                    r"stop",\r\n                    r"halt",\r\n                    r"freeze",\r\n                    r"wait"\r\n                ],\r\n                "action": "stop",\r\n                "params": {}\r\n            },\r\n            "greet": {\r\n                "patterns": [\r\n                    r"hello",\r\n                    r"hi",\r\n                    r"greetings",\r\n                    r"hey"\r\n                ],\r\n                "action": "greet",\r\n                "params": {}\r\n            }\r\n        }\r\n\r\n    def parse_command(self, text: str) -> Optional[Dict]:\r\n        """Parse voice command text into structured action"""\r\n        text_lower = text.lower().strip()\r\n\r\n        for command_type, config in self.command_patterns.items():\r\n            for pattern in config["patterns"]:\r\n                match = re.search(pattern, text_lower)\r\n                if match:\r\n                    # Extract parameters from regex groups\r\n                    groups = match.groups()\r\n\r\n                    # Create action with base parameters\r\n                    action = {\r\n                        "type": config["action"],\r\n                        "command_type": command_type,\r\n                        "original_text": text,\r\n                        "parameters": config["params"].copy()\r\n                    }\r\n\r\n                    # Add extracted parameters\r\n                    if config["action"] == "turn" and len(groups) >= 1:\r\n                        action["parameters"]["direction"] = groups[0]\r\n                        if len(groups) > 1:\r\n                            try:\r\n                                action["parameters"]["angle"] = float(groups[1])\r\n                            except ValueError:\r\n                                action["parameters"]["angle"] = 90.0  # Default turn angle\r\n                    elif config["action"] in ["pick", "look_at"] and len(groups) >= 1:\r\n                        action["parameters"]["target"] = groups[0]\r\n                    elif config["action"] == "place" and len(groups) >= 2:\r\n                        action["parameters"]["object"] = groups[0]\r\n                        action["parameters"]["location"] = groups[1]\r\n                    elif config["action"] == "move" and len(groups) >= 1:\r\n                        try:\r\n                            action["parameters"]["distance"] = float(groups[0])\r\n                        except (ValueError, IndexError):\r\n                            action["parameters"]["distance"] = 1.0  # Default distance\r\n\r\n                    return action\r\n\r\n        # If no pattern matches, return a general command\r\n        return {\r\n            "type": "general",\r\n            "command_type": "unknown",\r\n            "original_text": text,\r\n            "parameters": {"text": text}\r\n        }\r\n\r\nclass VoiceCommandProcessor:\r\n    def __init__(self):\r\n        self.grammar = VoiceCommandGrammar()\r\n        self.context = {}  # Store conversation context\r\n        self.command_history = []  # Store command history\r\n\r\n    def process_voice_command(self, voice_command: VoiceCommand) -> Optional[Dict]:\r\n        """Process a voice command and return structured action"""\r\n        if voice_command.confidence < 0.6:  # Only process confident commands\r\n            return None\r\n\r\n        # Parse the command using grammar\r\n        action = self.grammar.parse_command(voice_command.text)\r\n\r\n        if action:\r\n            # Add metadata\r\n            action["confidence"] = voice_command.confidence\r\n            action["timestamp"] = voice_command.timestamp\r\n\r\n            # Store in history\r\n            self.command_history.append(action)\r\n\r\n            # Keep only recent commands (last 100)\r\n            if len(self.command_history) > 100:\r\n                self.command_history = self.command_history[-100:]\r\n\r\n            return action\r\n\r\n        return None\r\n\r\n    def handle_command_with_context(self, voice_command: VoiceCommand) -> Optional[Dict]:\r\n        """Process command with context awareness"""\r\n        action = self.process_voice_command(voice_command)\r\n\r\n        if action and "target" in action["parameters"]:\r\n            # Resolve relative references using context\r\n            target = action["parameters"]["target"]\r\n\r\n            # Handle pronouns and relative references\r\n            if target in ["it", "that", "this", "the object", "the item"]:\r\n                # Use last mentioned object from context\r\n                last_object = self.context.get("last_mentioned_object")\r\n                if last_object:\r\n                    action["parameters"]["target"] = last_object\r\n\r\n            # Store target in context for future reference\r\n            self.context["last_mentioned_object"] = action["parameters"]["target"]\r\n\r\n        return action\n'})}),"\n",(0,t.jsx)(n.h2,{id:"real-time-voice-processing",children:"Real-Time Voice Processing"}),"\n",(0,t.jsx)(n.h3,{id:"streaming-voice-recognition",children:"Streaming Voice Recognition"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import asyncio\r\nimport threading\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nimport time\r\n\r\nclass StreamingVoiceRecognizer:\r\n    def __init__(self, whisper_processor: WhisperVoiceProcessor):\r\n        self.whisper_processor = whisper_processor\r\n        self.executor = ThreadPoolExecutor(max_workers=2)\r\n        self.is_streaming = False\r\n        self.voice_activity = False\r\n        self.silence_threshold = 0.01\r\n        self.speech_threshold = 0.05\r\n        self.silence_duration = 0.0\r\n        self.min_speech_duration = 0.3  # Minimum speech duration to trigger recognition\r\n\r\n    def start_streaming(self):\r\n        """Start streaming voice recognition"""\r\n        self.is_streaming = True\r\n        self.whisper_processor.start_listening()\r\n\r\n    def stop_streaming(self):\r\n        """Stop streaming voice recognition"""\r\n        self.is_streaming = False\r\n        self.whisper_processor.stop_listening()\r\n\r\n    def is_voice_active(self, audio_chunk):\r\n        """Detect if voice is active in audio chunk"""\r\n        energy = np.mean(np.abs(audio_chunk))\r\n\r\n        if energy > self.speech_threshold:\r\n            self.silence_duration = 0\r\n            return True\r\n        else:\r\n            self.silence_duration += len(audio_chunk) / self.whisper_processor.sample_rate\r\n            return self.silence_duration < 0.5  # Consider voice active if silence < 0.5s\r\n\r\n    def handle_voice_activity(self, is_active: bool):\r\n        """Handle voice activity changes"""\r\n        if is_active and not self.voice_activity:\r\n            # Voice started - prepare for recognition\r\n            self.voice_activity = True\r\n            print("Voice activity detected - preparing for recognition")\r\n        elif not is_active and self.voice_activity:\r\n            # Voice stopped - trigger recognition\r\n            self.voice_activity = False\r\n            print("Voice activity ended - processing command")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"voice-command-integration-with-robotics",children:"Voice Command Integration with Robotics"}),"\n",(0,t.jsx)(n.h3,{id:"robot-action-execution",children:"Robot Action Execution"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import math\r\nfrom dataclasses import dataclass\r\nfrom typing import Dict, Any, Optional\r\n\r\n@dataclass\r\nclass RobotAction:\r\n    """Structure for robot actions"""\r\n    action_type: str\r\n    parameters: Dict[str, Any]\r\n    priority: int = 1\r\n    timeout: float = 10.0\r\n\r\nclass VoiceToActionController:\r\n    def __init__(self, robot_interface=None):\r\n        self.robot_interface = robot_interface\r\n        self.command_processor = VoiceCommandProcessor()\r\n        self.active_actions = []\r\n        self.action_queue = []\r\n        self.is_executing = False\r\n\r\n    def process_voice_command(self, voice_command: VoiceCommand):\r\n        """Process voice command and queue robot action"""\r\n        # Parse the command\r\n        action = self.command_processor.handle_command_with_context(voice_command)\r\n\r\n        if action:\r\n            # Convert to robot action\r\n            robot_action = self._convert_to_robot_action(action)\r\n\r\n            if robot_action:\r\n                # Add to execution queue\r\n                self.action_queue.append(robot_action)\r\n\r\n                # Start execution if not already running\r\n                if not self.is_executing:\r\n                    self._execute_action_queue()\r\n\r\n    def _convert_to_robot_action(self, action: Dict) -> Optional[RobotAction]:\r\n        """Convert parsed action to robot action"""\r\n        action_type = action["type"]\r\n\r\n        if action_type == "move":\r\n            direction = action["parameters"].get("direction", "forward")\r\n            distance = action["parameters"].get("distance", 1.0)\r\n\r\n            return RobotAction(\r\n                action_type="navigate",\r\n                parameters={\r\n                    "direction": direction,\r\n                    "distance": distance,\r\n                    "speed": 0.5\r\n                }\r\n            )\r\n\r\n        elif action_type == "turn":\r\n            direction = action["parameters"].get("direction", "left")\r\n            angle = action["parameters"].get("angle", 90.0)\r\n\r\n            return RobotAction(\r\n                action_type="rotate",\r\n                parameters={\r\n                    "direction": direction,\r\n                    "angle": math.radians(angle),\r\n                    "speed": 0.5\r\n                }\r\n            )\r\n\r\n        elif action_type == "pick":\r\n            target = action["parameters"].get("target", "")\r\n\r\n            return RobotAction(\r\n                action_type="manipulation",\r\n                parameters={\r\n                    "action": "pick",\r\n                    "target_object": target\r\n                }\r\n            )\r\n\r\n        elif action_type == "place":\r\n            obj = action["parameters"].get("object", "")\r\n            location = action["parameters"].get("location", "default")\r\n\r\n            return RobotAction(\r\n                action_type="manipulation",\r\n                parameters={\r\n                    "action": "place",\r\n                    "object": obj,\r\n                    "location": location\r\n                }\r\n            )\r\n\r\n        elif action_type == "look_at":\r\n            target = action["parameters"].get("target", "")\r\n\r\n            return RobotAction(\r\n                action_type="look_at",\r\n                parameters={\r\n                    "target": target\r\n                }\r\n            )\r\n\r\n        elif action_type == "stop":\r\n            return RobotAction(\r\n                action_type="stop",\r\n                parameters={}\r\n            )\r\n\r\n        elif action_type == "greet":\r\n            return RobotAction(\r\n                action_type="speak",\r\n                parameters={\r\n                    "text": "Hello! How can I help you today?",\r\n                    "voice_pitch": 1.0\r\n                }\r\n            )\r\n\r\n        return None\r\n\r\n    def _execute_action_queue(self):\r\n        """Execute actions in the queue"""\r\n        if not self.action_queue or self.is_executing:\r\n            return\r\n\r\n        self.is_executing = True\r\n\r\n        while self.action_queue and self.is_executing:\r\n            action = self.action_queue.pop(0)\r\n            self.active_actions.append(action)\r\n\r\n            try:\r\n                success = self._execute_single_action(action)\r\n\r\n                if success:\r\n                    print(f"Successfully executed action: {action.action_type}")\r\n                else:\r\n                    print(f"Failed to execute action: {action.action_type}")\r\n\r\n            except Exception as e:\r\n                print(f"Error executing action {action.action_type}: {e}")\r\n            finally:\r\n                # Remove from active actions\r\n                if action in self.active_actions:\r\n                    self.active_actions.remove(action)\r\n\r\n        self.is_executing = False\r\n\r\n    def _execute_single_action(self, action: RobotAction) -> bool:\r\n        """Execute a single robot action"""\r\n        if not self.robot_interface:\r\n            print(f"Simulating action: {action.action_type} with params: {action.parameters}")\r\n            # Simulate action execution\r\n            time.sleep(0.5)  # Simulate execution time\r\n            return True\r\n\r\n        # Execute actual robot action based on type\r\n        try:\r\n            if action.action_type == "navigate":\r\n                return self._execute_navigation(action.parameters)\r\n            elif action.action_type == "rotate":\r\n                return self._execute_rotation(action.parameters)\r\n            elif action.action_type == "manipulation":\r\n                return self._execute_manipulation(action.parameters)\r\n            elif action.action_type == "look_at":\r\n                return self._execute_look_at(action.parameters)\r\n            elif action.action_type == "stop":\r\n                return self._execute_stop()\r\n            elif action.action_type == "speak":\r\n                return self._execute_speak(action.parameters)\r\n            else:\r\n                print(f"Unknown action type: {action.action_type}")\r\n                return False\r\n        except Exception as e:\r\n            print(f"Error executing action: {e}")\r\n            return False\r\n\r\n    def _execute_navigation(self, params: Dict) -> bool:\r\n        """Execute navigation action"""\r\n        # This would interface with the robot\'s navigation system\r\n        direction = params.get("direction", "forward")\r\n        distance = params.get("distance", 1.0)\r\n        speed = params.get("speed", 0.5)\r\n\r\n        print(f"Navigating {direction} for {distance}m at speed {speed}")\r\n        # In real implementation: call navigation API\r\n        return True\r\n\r\n    def _execute_rotation(self, params: Dict) -> bool:\r\n        """Execute rotation action"""\r\n        direction = params.get("direction", "left")\r\n        angle = params.get("angle", math.pi/2)  # Default 90 degrees\r\n        speed = params.get("speed", 0.5)\r\n\r\n        print(f"Rotating {direction} by {math.degrees(angle)} degrees")\r\n        # In real implementation: call rotation API\r\n        return True\r\n\r\n    def _execute_manipulation(self, params: Dict) -> bool:\r\n        """Execute manipulation action"""\r\n        action = params.get("action", "pick")\r\n        target = params.get("target_object", "") or params.get("object", "")\r\n\r\n        print(f"Manipulation: {action} {target}")\r\n        # In real implementation: call manipulation API\r\n        return True\r\n\r\n    def _execute_look_at(self, params: Dict) -> bool:\r\n        """Execute look at action"""\r\n        target = params.get("target", "")\r\n\r\n        print(f"Looking at: {target}")\r\n        # In real implementation: call head/eye control API\r\n        return True\r\n\r\n    def _execute_stop(self) -> bool:\r\n        """Execute stop action"""\r\n        print("Stopping all robot motion")\r\n        # In real implementation: call emergency stop or halt API\r\n        return True\r\n\r\n    def _execute_speak(self, params: Dict) -> bool:\r\n        """Execute speech action"""\r\n        text = params.get("text", "")\r\n        pitch = params.get("voice_pitch", 1.0)\r\n\r\n        print(f"Speaking: {text}")\r\n        # In real implementation: call text-to-speech API\r\n        return True\n'})}),"\n",(0,t.jsx)(n.h2,{id:"noise-reduction-and-audio-enhancement",children:"Noise Reduction and Audio Enhancement"}),"\n",(0,t.jsx)(n.h3,{id:"audio-preprocessing-for-robotics",children:"Audio Preprocessing for Robotics"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import soundfile as sf\r\nimport librosa\r\nfrom scipy import signal\r\nimport webrtcvad  # For voice activity detection\r\n\r\nclass AudioPreprocessor:\r\n    def __init__(self):\r\n        # Initialize WebRTC VAD\r\n        self.vad = webrtcvad.Vad()\r\n        self.vad.set_mode(1)  # Aggressiveness mode (0-3)\r\n\r\n    def denoise_audio(self, audio_data, sample_rate=16000):\r\n        """Apply noise reduction to audio"""\r\n        try:\r\n            # Use librosa for noise reduction\r\n            # This is a simplified approach - real implementation would use more sophisticated methods\r\n            denoised = librosa.effects.percussive(audio_data)\r\n            return denoised\r\n        except:\r\n            # If librosa fails, return original audio\r\n            return audio_data\r\n\r\n    def apply_audio_filters(self, audio_data, sample_rate=16000):\r\n        """Apply various audio filters to improve quality"""\r\n        # Normalize audio\r\n        audio_data = audio_data / np.max(np.abs(audio_data)) if np.max(np.abs(audio_data)) != 0 else audio_data\r\n\r\n        # Apply high-pass filter to remove low-frequency noise\r\n        b, a = signal.butter(4, 100 / (sample_rate / 2), btype=\'high\')\r\n        filtered_audio = signal.filtfilt(b, a, audio_data)\r\n\r\n        return filtered_audio\r\n\r\n    def detect_voice_activity(self, audio_data, sample_rate=16000):\r\n        """Detect voice activity in audio chunk"""\r\n        # Convert to appropriate format for WebRTC VAD\r\n        # WebRTC VAD requires 16kHz, 16-bit PCM\r\n        if sample_rate != 16000:\r\n            audio_data = librosa.resample(audio_data, orig_sr=sample_rate, target_sr=16000)\r\n\r\n        # Convert to 16-bit integers\r\n        audio_int16 = (audio_data * 32767).astype(np.int16)\r\n\r\n        # WebRTC VAD works on 10, 20, or 30 ms frames\r\n        frame_size = int(16000 * 0.02)  # 20ms frame\r\n        frames = [audio_int16[i:i+frame_size] for i in range(0, len(audio_int16), frame_size)]\r\n\r\n        voice_active = False\r\n        for frame in frames:\r\n            if len(frame) == frame_size:\r\n                try:\r\n                    if self.vad.is_speech(frame.tobytes(), 16000):\r\n                        voice_active = True\r\n                        break\r\n                except:\r\n                    continue\r\n\r\n        return voice_active\r\n\r\n    def preprocess_audio_chunk(self, audio_chunk, sample_rate=16000):\r\n        """Complete preprocessing pipeline for audio chunk"""\r\n        # Apply filters\r\n        filtered_audio = self.apply_audio_filters(audio_chunk, sample_rate)\r\n\r\n        # Denoise (simplified)\r\n        denoised_audio = self.denoise_audio(filtered_audio, sample_rate)\r\n\r\n        # Detect voice activity\r\n        has_voice = self.detect_voice_activity(denoised_audio, sample_rate)\r\n\r\n        return denoised_audio, has_voice\n'})}),"\n",(0,t.jsx)(n.h2,{id:"integration-with-humanoid-robot-systems",children:"Integration with Humanoid Robot Systems"}),"\n",(0,t.jsx)(n.h3,{id:"complete-voice-to-action-system",children:"Complete Voice-to-Action System"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class CompleteVoiceToActionSystem:\r\n    def __init__(self, robot_interface=None):\r\n        # Initialize Whisper processor\r\n        self.whisper_processor = WhisperVoiceProcessor(model_size="base")\r\n\r\n        # Initialize audio preprocessor\r\n        self.audio_preprocessor = AudioPreprocessor()\r\n\r\n        # Initialize command processor\r\n        self.command_processor = VoiceCommandProcessor()\r\n\r\n        # Initialize action controller\r\n        self.action_controller = VoiceToActionController(robot_interface)\r\n\r\n        # Setup callbacks\r\n        self.whisper_processor.add_command_callback(self._handle_voice_command)\r\n\r\n        # System state\r\n        self.is_active = False\r\n        self.conversation_context = []\r\n\r\n    def start_system(self):\r\n        """Start the complete voice-to-action system"""\r\n        if not self.is_active:\r\n            self.is_active = True\r\n            self.whisper_processor.start_listening()\r\n            print("Voice-to-Action system started")\r\n\r\n    def stop_system(self):\r\n        """Stop the complete voice-to-action system"""\r\n        if self.is_active:\r\n            self.is_active = False\r\n            self.whisper_processor.stop_listening()\r\n            print("Voice-to-Action system stopped")\r\n\r\n    def _handle_voice_command(self, voice_command: VoiceCommand):\r\n        """Handle incoming voice command"""\r\n        print(f"Processing voice command: \'{voice_command.text}\' (confidence: {voice_command.confidence:.2f})")\r\n\r\n        # Add to conversation context\r\n        self.conversation_context.append({\r\n            "type": "user",\r\n            "text": voice_command.text,\r\n            "timestamp": voice_command.timestamp,\r\n            "confidence": voice_command.confidence\r\n        })\r\n\r\n        # Process with context awareness\r\n        action = self.command_processor.handle_command_with_context(voice_command)\r\n\r\n        if action:\r\n            print(f"Converted to action: {action}")\r\n\r\n            # Execute action\r\n            self.action_controller.process_voice_command(voice_command)\r\n\r\n            # Add to context\r\n            self.conversation_context.append({\r\n                "type": "system",\r\n                "action": action,\r\n                "timestamp": time.time()\r\n            })\r\n        else:\r\n            print("Could not parse command")\r\n\r\n    def get_system_status(self):\r\n        """Get current system status"""\r\n        return {\r\n            "is_active": self.is_active,\r\n            "listening": self.whisper_processor.is_listening,\r\n            "command_queue_size": len(self.action_controller.action_queue),\r\n            "active_actions": len(self.action_controller.active_actions),\r\n            "conversation_length": len(self.conversation_context)\r\n        }\r\n\r\n    def add_command_callback(self, callback):\r\n        """Add callback for processed commands"""\r\n        self.whisper_processor.add_command_callback(callback)\r\n\r\n    def process_direct_command(self, text: str):\r\n        """Process a text command directly (for testing or alternative input)"""\r\n        command = VoiceCommand(\r\n            text=text,\r\n            confidence=1.0,  # Direct input has maximum confidence\r\n            timestamp=time.time(),\r\n            command_type="text",\r\n            parameters={}\r\n        )\r\n\r\n        self._handle_voice_command(command)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"real-time-performance-considerations",children:"Real-Time Performance Considerations"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import psutil\r\nimport GPUtil\r\nfrom collections import deque\r\nimport statistics\r\n\r\nclass VoiceToActionPerformanceMonitor:\r\n    def __init__(self):\r\n        self.processing_times = deque(maxlen=100)\r\n        self.cpu_usage = deque(maxlen=100)\r\n        self.gpu_usage = deque(maxlen=100)\r\n        self.memory_usage = deque(maxlen=100)\r\n\r\n    def start_monitoring(self):\r\n        """Start performance monitoring in background"""\r\n        self.monitoring_active = True\r\n        self.monitoring_thread = threading.Thread(target=self._monitor_loop)\r\n        self.monitoring_thread.daemon = True\r\n        self.monitoring_thread.start()\r\n\r\n    def _monitor_loop(self):\r\n        """Performance monitoring loop"""\r\n        while self.monitoring_active:\r\n            # CPU usage\r\n            cpu_percent = psutil.cpu_percent()\r\n            self.cpu_usage.append(cpu_percent)\r\n\r\n            # Memory usage\r\n            memory_percent = psutil.virtual_memory().percent\r\n            self.memory_usage.append(memory_percent)\r\n\r\n            # GPU usage (if available)\r\n            try:\r\n                gpus = GPUtil.getGPUs()\r\n                if gpus:\r\n                    gpu_percent = gpus[0].load * 100\r\n                    self.gpu_usage.append(gpu_percent)\r\n                else:\r\n                    self.gpu_usage.append(0)\r\n            except:\r\n                self.gpu_usage.append(0)\r\n\r\n            time.sleep(1)  # Monitor every second\r\n\r\n    def record_processing_time(self, processing_time):\r\n        """Record processing time for a command"""\r\n        self.processing_times.append(processing_time)\r\n\r\n    def get_performance_metrics(self):\r\n        """Get current performance metrics"""\r\n        metrics = {\r\n            "avg_processing_time": statistics.mean(self.processing_times) if self.processing_times else 0,\r\n            "max_processing_time": max(self.processing_times) if self.processing_times else 0,\r\n            "cpu_avg": statistics.mean(self.cpu_usage) if self.cpu_usage else 0,\r\n            "memory_avg": statistics.mean(self.memory_usage) if self.memory_usage else 0,\r\n            "gpu_avg": statistics.mean(self.gpu_usage) if self.gpu_usage else 0\r\n        }\r\n        return metrics\n'})}),"\n",(0,t.jsx)(n.h2,{id:"error-handling-and-robustness",children:"Error Handling and Robustness"}),"\n",(0,t.jsx)(n.h3,{id:"handling-recognition-errors",children:"Handling Recognition Errors"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class VoiceCommandErrorHandler:\r\n    def __init__(self):\r\n        self.error_history = deque(maxlen=50)\r\n        self.uncertainty_threshold = 0.5\r\n        self.similarity_threshold = 0.8\r\n\r\n    def handle_recognition_error(self, command_text, confidence, original_command=None):\r\n        """Handle cases where recognition is uncertain"""\r\n        error_record = {\r\n            "command_text": command_text,\r\n            "confidence": confidence,\r\n            "timestamp": time.time(),\r\n            "original_command": original_command\r\n        }\r\n        self.error_history.append(error_record)\r\n\r\n        # If confidence is low, ask for clarification\r\n        if confidence < self.uncertainty_threshold:\r\n            return self._request_clarification(command_text)\r\n        else:\r\n            return command_text\r\n\r\n    def _request_clarification(self, command_text):\r\n        """Request user to clarify the command"""\r\n        suggestions = self._generate_command_suggestions(command_text)\r\n\r\n        if suggestions:\r\n            clarification_prompt = f"I didn\'t quite understand \'{command_text}\'. Did you mean: {\', \'.join(suggestions)}?"\r\n        else:\r\n            clarification_prompt = f"I didn\'t understand \'{command_text}\'. Could you please repeat that?"\r\n\r\n        return clarification_prompt\r\n\r\n    def _generate_command_suggestions(self, command_text):\r\n        """Generate possible command suggestions based on the input"""\r\n        # This would use command similarity or grammar rules\r\n        # For demonstration, returning some common alternatives\r\n        common_commands = [\r\n            "move forward", "turn left", "pick up object",\r\n            "place object", "stop", "look at object"\r\n        ]\r\n\r\n        suggestions = []\r\n        for cmd in common_commands:\r\n            similarity = self._calculate_similarity(command_text.lower(), cmd.lower())\r\n            if similarity > self.similarity_threshold:\r\n                suggestions.append(cmd)\r\n\r\n        return suggestions[:3]  # Return top 3 suggestions\r\n\r\n    def _calculate_similarity(self, text1, text2):\r\n        """Calculate similarity between two texts"""\r\n        # Simple word overlap similarity\r\n        words1 = set(text1.split())\r\n        words2 = set(text2.split())\r\n\r\n        intersection = words1.intersection(words2)\r\n        union = words1.union(words2)\r\n\r\n        return len(intersection) / len(union) if union else 0\n'})}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"Voice-to-action systems using Whisper provide a powerful interface for humanoid robots to understand and execute spoken commands. The integration involves several key components:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Audio Processing"}),": Capturing, filtering, and preprocessing audio input"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speech Recognition"}),": Using Whisper for accurate transcription"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Command Parsing"}),": Converting natural language to structured robot commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Execution"}),": Mapping commands to robot behaviors"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Error Handling"}),": Managing recognition errors and uncertainties"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The success of voice-to-action systems in robotics depends on:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Robust speech recognition that works in noisy environments"}),"\n",(0,t.jsx)(n.li,{children:"Effective command parsing and natural language understanding"}),"\n",(0,t.jsx)(n.li,{children:"Safe and reliable action execution"}),"\n",(0,t.jsx)(n.li,{children:"Continuous learning and adaptation"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"In the next chapter, we'll explore LLM-based cognitive planning, which represents the higher-level reasoning component of VLA systems, enabling humanoid robots to plan complex, multi-step tasks based on natural language commands."})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>s});var t=r(6540);const i={},o=t.createContext(i);function a(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);