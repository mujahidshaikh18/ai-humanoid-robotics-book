"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[545],{1818:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>i,metadata:()=>o,toc:()=>c});var t=r(4848),s=r(8453);const i={sidebar_position:19,title:"Chapter 19: Multimodal Perception"},a="Multimodal Perception",o={id:"module-4/chapter-19",title:"Chapter 19: Multimodal Perception",description:"Overview",source:"@site/docs/module-4/chapter-19.mdx",sourceDirName:"module-4",slug:"/module-4/chapter-19",permalink:"/ai-humanoid-robotics-book/docs/module-4/chapter-19",draft:!1,unlisted:!1,editUrl:"https://github.com/mujahidshaikh18/ai-humanoid-robotics-book/tree/main/docs/module-4/chapter-19.mdx",tags:[],version:"current",sidebarPosition:19,frontMatter:{sidebar_position:19,title:"Chapter 19: Multimodal Perception"},sidebar:"tutorialSidebar",previous:{title:"Chapter 18: LLM-Based Cognitive Planning",permalink:"/ai-humanoid-robotics-book/docs/module-4/chapter-18"},next:{title:"Chapter 20: Capstone Project \u2014 Autonomous Humanoid",permalink:"/ai-humanoid-robotics-book/docs/module-4/chapter-20"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Multimodal Perception",id:"introduction-to-multimodal-perception",level:2},{value:"The Need for Multimodal Perception",id:"the-need-for-multimodal-perception",level:3},{value:"Challenges in Multimodal Perception",id:"challenges-in-multimodal-perception",level:3},{value:"Sensor Fusion Techniques",id:"sensor-fusion-techniques",level:2},{value:"Kalman Filter-Based Fusion",id:"kalman-filter-based-fusion",level:3},{value:"Particle Filter for Non-Linear Fusion",id:"particle-filter-for-non-linear-fusion",level:3},{value:"Visual Perception Integration",id:"visual-perception-integration",level:2},{value:"Deep Learning-Based Visual Processing",id:"deep-learning-based-visual-processing",level:3},{value:"Auditory Perception Integration",id:"auditory-perception-integration",level:2},{value:"Sound Processing and Analysis",id:"sound-processing-and-analysis",level:3},{value:"Tactile and Proprioceptive Integration",id:"tactile-and-proprioceptive-integration",level:2},{value:"Tactile Sensor Processing",id:"tactile-sensor-processing",level:3},{value:"Multimodal Fusion Architecture",id:"multimodal-fusion-architecture",level:2},{value:"Attention-Based Fusion",id:"attention-based-fusion",level:3},{value:"Real-Time Processing and Optimization",id:"real-time-processing-and-optimization",level:2},{value:"Efficient Multimodal Processing",id:"efficient-multimodal-processing",level:3},{value:"Evaluation and Quality Assessment",id:"evaluation-and-quality-assessment",level:2},{value:"Perception Quality Metrics",id:"perception-quality-metrics",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"multimodal-perception",children:"Multimodal Perception"}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"Multimodal perception is the foundation of intelligent humanoid robotics, enabling robots to understand and interact with their environment through the integration of multiple sensory modalities. This chapter explores how humanoid robots can combine visual, auditory, tactile, and other sensory inputs to achieve robust environmental understanding, object recognition, and decision-making capabilities."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand the principles of multimodal perception in robotics"}),"\n",(0,t.jsx)(n.li,{children:"Implement sensor fusion techniques for humanoid robots"}),"\n",(0,t.jsx)(n.li,{children:"Design multimodal perception architectures"}),"\n",(0,t.jsx)(n.li,{children:"Integrate different sensory modalities for enhanced perception"}),"\n",(0,t.jsx)(n.li,{children:"Evaluate and optimize multimodal perception systems"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-multimodal-perception",children:"Introduction to Multimodal Perception"}),"\n",(0,t.jsx)(n.h3,{id:"the-need-for-multimodal-perception",children:"The Need for Multimodal Perception"}),"\n",(0,t.jsx)(n.p,{children:"Humanoid robots operating in human environments require sophisticated perception capabilities that go beyond single-sensor approaches. Multimodal perception provides:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robustness"}),": Redundant information from multiple sensors"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Completeness"}),": Comprehensive environmental understanding"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Context Awareness"}),": Rich contextual information for decision-making"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Adaptability"}),": Ability to function in varied conditions"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   Visual        \u2502    \u2502   Auditory      \u2502    \u2502   Tactile       \u2502\r\n\u2502   Sensors       \u2502    \u2502   Sensors       \u2502    \u2502   Sensors       \u2502\r\n\u2502   (Cameras,     \u2502    \u2502   (Microphones, \u2502    \u2502   (Force/Torque,\u2502\r\n\u2502   Depth)        \u2502    \u2502   Speakers)     \u2502    \u2502   Touch)        \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n         \u2502                       \u2502                       \u2502\r\n         \u25bc                       \u25bc                       \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                    Sensor Fusion Layer                          \u2502\r\n\u2502              (Kalman Filters, Particle Filters,                \u2502\r\n\u2502               Neural Networks, Attention Mechanisms)           \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n         \u2502\r\n         \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   Semantic      \u2502    \u2502   Spatial       \u2502    \u2502   Temporal      \u2502\r\n\u2502   Understanding \u2502    \u2502   Reasoning     \u2502    \u2502   Modeling      \u2502\r\n\u2502   (Objects,     \u2502    \u2502   (Location,    \u2502    \u2502   (Behavior,    \u2502\r\n\u2502   Scenes)       \u2502    \u2502   Navigation)   \u2502    \u2502   Prediction)   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h3,{id:"challenges-in-multimodal-perception",children:"Challenges in Multimodal Perception"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor Calibration"}),": Aligning different sensor coordinate systems"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Temporal Synchronization"}),": Coordinating asynchronous sensor data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Data Association"}),": Matching features across modalities"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Computational Complexity"}),": Processing multiple high-bandwidth streams"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Uncertainty Management"}),": Handling sensor noise and uncertainty"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"sensor-fusion-techniques",children:"Sensor Fusion Techniques"}),"\n",(0,t.jsx)(n.h3,{id:"kalman-filter-based-fusion",children:"Kalman Filter-Based Fusion"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import numpy as np\r\nfrom scipy.linalg import block_diag\r\nimport math\r\n\r\nclass MultimodalKalmanFilter:\r\n    def __init__(self, state_dim=6, control_dim=0):\r\n        \"\"\"\r\n        Initialize Kalman filter for multimodal fusion\r\n        State: [x, y, z, vx, vy, vz] (position + velocity)\r\n        \"\"\"\r\n        self.state_dim = state_dim\r\n        self.control_dim = control_dim\r\n\r\n        # State vector [x, y, z, vx, vy, vz]\r\n        self.x = np.zeros(state_dim)\r\n\r\n        # State covariance matrix\r\n        self.P = np.eye(state_dim) * 1000.0  # Initial uncertainty\r\n\r\n        # Process noise covariance\r\n        self.Q = np.eye(state_dim) * 0.1\r\n\r\n        # Measurement noise covariance (to be updated per sensor)\r\n        self.R = np.eye(3) * 1.0  # Default for 3D position\r\n\r\n        # State transition matrix (constant velocity model)\r\n        self.F = np.eye(state_dim)\r\n        dt = 0.1  # Time step\r\n        for i in range(3):\r\n            self.F[i, i+3] = dt\r\n\r\n        # Control matrix (none for this example)\r\n        self.B = np.zeros((state_dim, control_dim)) if control_dim > 0 else None\r\n\r\n        # Measurement matrix (initially for position only)\r\n        self.H = np.zeros((3, state_dim))\r\n        self.H[0, 0] = 1  # x measurement\r\n        self.H[1, 1] = 1  # y measurement\r\n        self.H[2, 2] = 1  # z measurement\r\n\r\n    def predict(self, u=None):\r\n        \"\"\"Prediction step\"\"\"\r\n        # State prediction\r\n        if u is not None and self.B is not None:\r\n            self.x = self.F @ self.x + self.B @ u\r\n        else:\r\n            self.x = self.F @ self.x\r\n\r\n        # Covariance prediction\r\n        self.P = self.F @ self.P @ self.F.T + self.Q\r\n\r\n    def update(self, z, R=None, H=None):\r\n        \"\"\"Update step with measurement z\"\"\"\r\n        if R is None:\r\n            R = self.R\r\n        if H is None:\r\n            H = self.H\r\n\r\n        # Innovation\r\n        y = z - H @ self.x\r\n\r\n        # Innovation covariance\r\n        S = H @ self.P @ H.T + R\r\n\r\n        # Kalman gain\r\n        K = self.P @ H.T @ np.linalg.inv(S)\r\n\r\n        # State update\r\n        self.x = self.x + K @ y\r\n\r\n        # Covariance update\r\n        I = np.eye(self.P.shape[0])\r\n        self.P = (I - K @ H) @ self.P\r\n\r\n    def fuse_sensor_data(self, sensor_data):\r\n        \"\"\"\r\n        Fuse data from multiple sensors\r\n        sensor_data: dict with keys 'visual', 'lidar', 'imu', etc.\r\n        \"\"\"\r\n        fused_estimate = np.zeros(self.state_dim)\r\n        total_weight = 0\r\n\r\n        for sensor_type, data in sensor_data.items():\r\n            if sensor_type == 'visual':\r\n                # Visual sensor typically provides position\r\n                weight = 1.0 / data.get('uncertainty', 1.0)\r\n                estimate = np.array([data['x'], data['y'], data['z'], 0, 0, 0])\r\n                fused_estimate[:3] += weight * estimate[:3]\r\n                total_weight += weight\r\n\r\n            elif sensor_type == 'lidar':\r\n                # LIDAR provides accurate position\r\n                weight = 2.0 / data.get('uncertainty', 0.5)  # Higher weight for LIDAR\r\n                estimate = np.array([data['x'], data['y'], data['z'], 0, 0, 0])\r\n                fused_estimate[:3] += weight * estimate[:3]\r\n                total_weight += weight\r\n\r\n            elif sensor_type == 'imu':\r\n                # IMU provides velocity/acceleration\r\n                weight = 0.5 / data.get('uncertainty', 2.0)\r\n                estimate = np.array([0, 0, 0, data['vx'], data['vy'], data['vz']])\r\n                fused_estimate[3:] += weight * estimate[3:]\r\n                total_weight += weight\r\n\r\n        if total_weight > 0:\r\n            fused_estimate[:3] /= total_weight\r\n            # For velocity, we might need different normalization\r\n            velocity_weight = sum(w for st, d in sensor_data.items() if st == 'imu')\r\n            if velocity_weight > 0:\r\n                fused_estimate[3:] /= velocity_weight\r\n\r\n        return fused_estimate\r\n\r\nclass MultimodalSensorFusion:\r\n    def __init__(self):\r\n        self.kalman_filter = MultimodalKalmanFilter()\r\n        self.sensor_data_buffer = {}\r\n        self.fusion_weights = {\r\n            'visual': 1.0,\r\n            'lidar': 2.0,\r\n            'imu': 0.5,\r\n            'sonar': 0.3,\r\n            'touch': 3.0  # High weight when contact is made\r\n        }\r\n\r\n    def add_sensor_data(self, sensor_type, data, timestamp):\r\n        \"\"\"Add sensor data to buffer\"\"\"\r\n        if sensor_type not in self.sensor_data_buffer:\r\n            self.sensor_data_buffer[sensor_type] = []\r\n\r\n        self.sensor_data_buffer[sensor_type].append({\r\n            'data': data,\r\n            'timestamp': timestamp,\r\n            'uncertainty': data.get('uncertainty', 1.0)\r\n        })\r\n\r\n        # Keep only recent data (last 10 readings)\r\n        if len(self.sensor_data_buffer[sensor_type]) > 10:\r\n            self.sensor_data_buffer[sensor_type] = self.sensor_data_buffer[sensor_type][-10:]\r\n\r\n    def get_fused_estimate(self):\r\n        \"\"\"Get fused estimate from all available sensors\"\"\"\r\n        if not self.sensor_data_buffer:\r\n            return None\r\n\r\n        # Prepare data for fusion\r\n        fusion_data = {}\r\n        for sensor_type, readings in self.sensor_data_buffer.items():\r\n            if readings:\r\n                latest_reading = readings[-1]  # Use most recent\r\n                fusion_data[sensor_type] = {\r\n                    **latest_reading['data'],\r\n                    'uncertainty': latest_reading['uncertainty']\r\n                }\r\n\r\n        # Perform fusion\r\n        fused_estimate = self.kalman_filter.fuse_sensor_data(fusion_data)\r\n        return fused_estimate\n"})}),"\n",(0,t.jsx)(n.h3,{id:"particle-filter-for-non-linear-fusion",children:"Particle Filter for Non-Linear Fusion"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class MultimodalParticleFilter:\r\n    def __init__(self, num_particles=1000, state_dim=6):\r\n        self.num_particles = num_particles\r\n        self.state_dim = state_dim\r\n\r\n        # Initialize particles\r\n        self.particles = np.random.normal(0, 1, (num_particles, state_dim))\r\n        self.weights = np.ones(num_particles) / num_particles\r\n\r\n    def predict(self, control_input=None, noise_std=0.1):\r\n        """Predict particle states"""\r\n        if control_input is not None:\r\n            self.particles += control_input + np.random.normal(0, noise_std, self.particles.shape)\r\n        else:\r\n            # Add process noise\r\n            self.particles += np.random.normal(0, noise_std, self.particles.shape)\r\n\r\n    def update(self, measurements):\r\n        """Update particle weights based on measurements"""\r\n        for i, particle in enumerate(self.particles):\r\n            weight = 1.0\r\n\r\n            # Update weight based on each sensor measurement\r\n            for sensor_type, measurement in measurements.items():\r\n                predicted_measurement = self.predict_sensor_reading(particle, sensor_type)\r\n                measurement_error = np.linalg.norm(measurement - predicted_measurement)\r\n\r\n                # Calculate likelihood (Gaussian)\r\n                likelihood = np.exp(-0.5 * (measurement_error ** 2) / (measurement.get(\'uncertainty\', 1.0) ** 2))\r\n                weight *= likelihood\r\n\r\n            self.weights[i] *= weight\r\n\r\n        # Normalize weights\r\n        self.weights += 1e-300  # Avoid zero weights\r\n        self.weights /= np.sum(self.weights)\r\n\r\n    def predict_sensor_reading(self, state, sensor_type):\r\n        """Predict what sensor should read given state"""\r\n        if sensor_type in [\'visual\', \'lidar\', \'camera\']:\r\n            # Position sensors read position directly\r\n            return state[:3]\r\n        elif sensor_type == \'imu\':\r\n            # IMU reads velocity\r\n            return state[3:6]\r\n        else:\r\n            return state[:3]  # Default to position\r\n\r\n    def resample(self):\r\n        """Resample particles based on weights"""\r\n        # Systematic resampling\r\n        indices = self.systematic_resample()\r\n        self.particles = self.particles[indices]\r\n        self.weights.fill(1.0 / self.num_particles)\r\n\r\n    def systematic_resample(self):\r\n        """Systematic resampling algorithm"""\r\n        indices = np.zeros(self.num_particles, dtype=int)\r\n        cumulative_sum = np.cumsum(self.weights)\r\n\r\n        u = np.random.uniform(0, 1/self.num_particles)\r\n        i, j = 0, 0\r\n\r\n        while i < self.num_particles:\r\n            while cumulative_sum[j] < u:\r\n                j += 1\r\n            indices[i] = j\r\n            u += 1/self.num_particles\r\n            i += 1\r\n\r\n        return indices\r\n\r\n    def estimate(self):\r\n        """Get state estimate from particles"""\r\n        return np.average(self.particles, weights=self.weights, axis=0)\r\n\r\n    def process_multimodal_data(self, sensor_data):\r\n        """Process multimodal sensor data"""\r\n        # Prediction step\r\n        self.predict()\r\n\r\n        # Update step\r\n        self.update(sensor_data)\r\n\r\n        # Resampling if effective sample size is low\r\n        neff = 1.0 / np.sum(self.weights ** 2)\r\n        if neff < self.num_particles / 2.0:\r\n            self.resample()\r\n\r\n        return self.estimate()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"visual-perception-integration",children:"Visual Perception Integration"}),"\n",(0,t.jsx)(n.h3,{id:"deep-learning-based-visual-processing",children:"Deep Learning-Based Visual Processing"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\nimport torchvision.transforms as transforms\r\nfrom torchvision.models import resnet50\r\nimport cv2\r\n\r\nclass MultimodalVisualProcessor:\r\n    def __init__(self, device=\'cuda\' if torch.cuda.is_available() else \'cpu\'):\r\n        self.device = device\r\n\r\n        # Load pre-trained models\r\n        self.feature_extractor = resnet50(pretrained=True)\r\n        self.feature_extractor.fc = nn.Identity()  # Remove classification layer\r\n        self.feature_extractor = self.feature_extractor.to(device)\r\n        self.feature_extractor.eval()\r\n\r\n        # Object detection model (YOLO or similar)\r\n        self.object_detector = self.load_object_detector()\r\n\r\n        # Image preprocessing\r\n        self.transform = transforms.Compose([\r\n            transforms.ToTensor(),\r\n            transforms.Resize((224, 224)),\r\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\r\n                               std=[0.229, 0.224, 0.225])\r\n        ])\r\n\r\n    def load_object_detector(self):\r\n        """Load object detection model"""\r\n        # In practice, this would load YOLO, Detectron2, or similar\r\n        # For this example, we\'ll use a placeholder\r\n        class DummyDetector:\r\n            def detect(self, image):\r\n                # Placeholder detection - in reality, this would use a real detector\r\n                height, width = image.shape[:2]\r\n                return [{\r\n                    \'label\': \'person\',\r\n                    \'confidence\': 0.9,\r\n                    \'bbox\': [width//4, height//4, 3*width//4, 3*height//4],\r\n                    \'center\': [width//2, height//2]\r\n                }]\r\n        return DummyDetector()\r\n\r\n    def extract_visual_features(self, image):\r\n        """Extract deep visual features from image"""\r\n        # Convert image to tensor\r\n        if isinstance(image, np.ndarray):\r\n            image_tensor = self.transform(image).unsqueeze(0)\r\n        else:\r\n            image_tensor = image.unsqueeze(0)\r\n\r\n        image_tensor = image_tensor.to(self.device)\r\n\r\n        with torch.no_grad():\r\n            features = self.feature_extractor(image_tensor)\r\n\r\n        return features.cpu().numpy()\r\n\r\n    def detect_objects(self, image):\r\n        """Detect objects in image"""\r\n        detections = self.object_detector.detect(image)\r\n        return detections\r\n\r\n    def extract_scene_context(self, image):\r\n        """Extract scene-level context information"""\r\n        # Get object detections\r\n        objects = self.detect_objects(image)\r\n\r\n        # Extract visual features\r\n        features = self.extract_visual_features(image)\r\n\r\n        # Analyze scene composition\r\n        scene_context = {\r\n            \'objects\': objects,\r\n            \'features\': features,\r\n            \'scene_type\': self.classify_scene_type(image),\r\n            \'spatial_relations\': self.compute_spatial_relations(objects),\r\n            \'color_palette\': self.extract_color_palette(image)\r\n        }\r\n\r\n        return scene_context\r\n\r\n    def classify_scene_type(self, image):\r\n        """Classify the type of scene"""\r\n        # This would use a scene classification model\r\n        # For now, using simple heuristics\r\n        height, width = image.shape[:2]\r\n        if width > height * 1.5:\r\n            return "indoor_wide"\r\n        else:\r\n            return "indoor_closeup"\r\n\r\n    def compute_spatial_relations(self, objects):\r\n        """Compute spatial relationships between objects"""\r\n        relations = []\r\n        for i, obj1 in enumerate(objects):\r\n            for j, obj2 in enumerate(objects):\r\n                if i != j:\r\n                    center1 = obj1[\'center\']\r\n                    center2 = obj2[\'center\']\r\n\r\n                    dx = center2[0] - center1[0]\r\n                    dy = center2[1] - center1[1]\r\n\r\n                    # Determine spatial relation\r\n                    if abs(dx) > abs(dy):\r\n                        relation = "left" if dx < 0 else "right"\r\n                    else:\r\n                        relation = "above" if dy < 0 else "below"\r\n\r\n                    relations.append({\r\n                        \'subject\': obj1[\'label\'],\r\n                        \'relation\': relation,\r\n                        \'object\': obj2[\'label\']\r\n                    })\r\n\r\n        return relations\r\n\r\n    def extract_color_palette(self, image):\r\n        """Extract dominant colors from image"""\r\n        # Convert to HSV for better color analysis\r\n        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\r\n\r\n        # Use k-means clustering to find dominant colors\r\n        pixels = hsv.reshape(-1, 3)\r\n        pixels = np.float32(pixels)\r\n\r\n        # For simplicity, return average color\r\n        avg_color = np.mean(pixels, axis=0)\r\n        return avg_color\n'})}),"\n",(0,t.jsx)(n.h2,{id:"auditory-perception-integration",children:"Auditory Perception Integration"}),"\n",(0,t.jsx)(n.h3,{id:"sound-processing-and-analysis",children:"Sound Processing and Analysis"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import librosa\r\nimport numpy as np\r\nfrom scipy import signal\r\nimport soundfile as sf\r\n\r\nclass MultimodalAuditoryProcessor:\r\n    def __init__(self):\r\n        self.sample_rate = 16000\r\n        self.frame_length = 2048\r\n        self.hop_length = 512\r\n        self.n_mels = 128\r\n\r\n    def extract_audio_features(self, audio_data, sample_rate=16000):\r\n        """Extract comprehensive audio features"""\r\n        # Resample if necessary\r\n        if sample_rate != self.sample_rate:\r\n            audio_data = librosa.resample(audio_data, orig_sr=sample_rate, target_sr=self.sample_rate)\r\n\r\n        # Compute various audio features\r\n        features = {\r\n            \'mfcc\': self.compute_mfcc(audio_data),\r\n            \'spectral_centroid\': self.compute_spectral_centroid(audio_data),\r\n            \'zero_crossing_rate\': self.compute_zero_crossing_rate(audio_data),\r\n            \'chroma\': self.compute_chroma(audio_data),\r\n            \'mel_spectrogram\': self.compute_mel_spectrogram(audio_data),\r\n            \'rms_energy\': self.compute_rms_energy(audio_data)\r\n        }\r\n\r\n        return features\r\n\r\n    def compute_mfcc(self, audio_data):\r\n        """Compute Mel-Frequency Cepstral Coefficients"""\r\n        mfccs = librosa.feature.mfcc(\r\n            y=audio_data,\r\n            sr=self.sample_rate,\r\n            n_mfcc=13,\r\n            n_fft=self.frame_length,\r\n            hop_length=self.hop_length\r\n        )\r\n        return mfccs\r\n\r\n    def compute_spectral_centroid(self, audio_data):\r\n        """Compute spectral centroid (brightness of sound)"""\r\n        spectral_centroids = librosa.feature.spectral_centroid(\r\n            y=audio_data,\r\n            sr=self.sample_rate,\r\n            n_fft=self.frame_length,\r\n            hop_length=self.hop_length\r\n        )\r\n        return spectral_centroids\r\n\r\n    def compute_zero_crossing_rate(self, audio_data):\r\n        """Compute zero crossing rate"""\r\n        zcr = librosa.feature.zero_crossing_rate(\r\n            y=audio_data,\r\n            hop_length=self.hop_length\r\n        )\r\n        return zcr\r\n\r\n    def compute_chroma(self, audio_data):\r\n        """Compute chroma features (pitch class profiles)"""\r\n        chroma = librosa.feature.chroma_stft(\r\n            y=audio_data,\r\n            sr=self.sample_rate,\r\n            n_fft=self.frame_length,\r\n            hop_length=self.hop_length\r\n        )\r\n        return chroma\r\n\r\n    def compute_mel_spectrogram(self, audio_data):\r\n        """Compute mel-spectrogram"""\r\n        mel_spec = librosa.feature.melspectrogram(\r\n            y=audio_data,\r\n            sr=self.sample_rate,\r\n            n_mels=self.n_mels,\r\n            n_fft=self.frame_length,\r\n            hop_length=self.hop_length\r\n        )\r\n        return librosa.power_to_db(mel_spec)\r\n\r\n    def compute_rms_energy(self, audio_data):\r\n        """Compute root mean square energy"""\r\n        rms = librosa.feature.rms(\r\n            y=audio_data,\r\n            frame_length=self.frame_length,\r\n            hop_length=self.hop_length\r\n        )\r\n        return rms\r\n\r\n    def detect_sound_events(self, audio_data):\r\n        """Detect sound events in audio"""\r\n        # Compute onset strength\r\n        onset_envelope = librosa.onset.onset_strength(\r\n            y=audio_data,\r\n            sr=self.sample_rate,\r\n            hop_length=self.hop_length\r\n        )\r\n\r\n        # Detect onset events\r\n        onset_frames = librosa.onset.onset_detect(\r\n            onset_envelope=onset_envelope,\r\n            sr=self.sample_rate,\r\n            hop_length=self.hop_length,\r\n            units=\'time\'\r\n        )\r\n\r\n        # Classify sound types (simplified)\r\n        sound_events = []\r\n        for onset_time in onset_frames:\r\n            # In a real system, this would use sound classification models\r\n            sound_type = self.classify_sound_type(audio_data)\r\n            sound_events.append({\r\n                \'time\': onset_time,\r\n                \'type\': sound_type,\r\n                \'intensity\': self.get_sound_intensity(audio_data, int(onset_time * self.sample_rate))\r\n            })\r\n\r\n        return sound_events\r\n\r\n    def classify_sound_type(self, audio_data):\r\n        """Classify type of sound (simplified)"""\r\n        # Compute features to distinguish sound types\r\n        rms_energy = np.mean(self.compute_rms_energy(audio_data))\r\n        zcr = np.mean(self.compute_zero_crossing_rate(audio_data))\r\n\r\n        if rms_energy > 0.01 and zcr > 0.05:\r\n            return "speech"\r\n        elif rms_energy > 0.005 and zcr < 0.02:\r\n            return "music"\r\n        elif rms_energy > 0.02:\r\n            return "environmental"\r\n        else:\r\n            return "silence"\r\n\r\n    def get_sound_intensity(self, audio_data, start_sample, window_size=4096):\r\n        """Get intensity of sound at specific time"""\r\n        end_sample = min(start_sample + window_size, len(audio_data))\r\n        window = audio_data[start_sample:end_sample]\r\n        return np.sqrt(np.mean(window ** 2))\r\n\r\n    def localize_sound_source(self, audio_data_multichannel):\r\n        """Localize sound source using multichannel audio"""\r\n        # This would implement beamforming or other localization techniques\r\n        # For now, returning placeholder\r\n        return {\r\n            \'azimuth\': 0.0,\r\n            \'elevation\': 0.0,\r\n            \'distance\': 1.0,\r\n            \'confidence\': 0.8\r\n        }\n'})}),"\n",(0,t.jsx)(n.h2,{id:"tactile-and-proprioceptive-integration",children:"Tactile and Proprioceptive Integration"}),"\n",(0,t.jsx)(n.h3,{id:"tactile-sensor-processing",children:"Tactile Sensor Processing"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class MultimodalTactileProcessor:\r\n    def __init__(self):\r\n        self.tactile_threshold = 0.1\r\n        self.force_calibration = 1.0\r\n        self.touch_history = []\r\n\r\n    def process_tactile_data(self, tactile_sensors_data):\r\n        \"\"\"Process data from tactile sensors\"\"\"\r\n        processed_data = {\r\n            'contact_points': [],\r\n            'force_distribution': [],\r\n            'slip_detection': [],\r\n            'texture_analysis': []\r\n        }\r\n\r\n        for sensor_id, sensor_data in tactile_sensors_data.items():\r\n            # Check for contact\r\n            if sensor_data['force'] > self.tactile_threshold:\r\n                contact_point = {\r\n                    'sensor_id': sensor_id,\r\n                    'position': sensor_data.get('position', [0, 0, 0]),\r\n                    'force': sensor_data['force'] * self.force_calibration,\r\n                    'timestamp': sensor_data.get('timestamp', 0)\r\n                }\r\n                processed_data['contact_points'].append(contact_point)\r\n\r\n                # Analyze force distribution\r\n                processed_data['force_distribution'].append({\r\n                    'magnitude': sensor_data['force'],\r\n                    'direction': sensor_data.get('force_direction', [0, 0, 1]),\r\n                    'area': sensor_data.get('contact_area', 0.01)\r\n                })\r\n\r\n                # Detect slip\r\n                slip_detected = self.detect_slip(sensor_data)\r\n                if slip_detected:\r\n                    processed_data['slip_detection'].append({\r\n                        'sensor_id': sensor_id,\r\n                        'slip_force': sensor_data.get('shear_force', 0),\r\n                        'timestamp': sensor_data['timestamp']\r\n                    })\r\n\r\n        return processed_data\r\n\r\n    def detect_slip(self, sensor_data):\r\n        \"\"\"Detect slip based on tactile sensor readings\"\"\"\r\n        # Simple slip detection based on shear forces\r\n        shear_force = sensor_data.get('shear_force', 0)\r\n        normal_force = sensor_data.get('force', 1e-6)  # Avoid division by zero\r\n        friction_coefficient = 0.5  # Typical for many surfaces\r\n\r\n        # If shear force exceeds friction limit, slip is detected\r\n        max_friction_force = friction_coefficient * normal_force\r\n        return shear_force > max_friction_force\r\n\r\n    def analyze_texture(self, tactile_sequence):\r\n        \"\"\"Analyze texture from tactile sensor sequence\"\"\"\r\n        if len(tactile_sequence) < 10:\r\n            return {'roughness': 0.0, 'pattern': 'unknown'}\r\n\r\n        # Analyze frequency content of tactile signal\r\n        forces = [data['force'] for data in tactile_sequence]\r\n        power_spectrum = np.abs(np.fft.fft(forces))**2\r\n\r\n        # Calculate roughness based on high-frequency content\r\n        total_power = np.sum(power_spectrum)\r\n        if total_power == 0:\r\n            return {'roughness': 0.0, 'pattern': 'smooth'}\r\n\r\n        high_freq_power = np.sum(power_spectrum[len(power_spectrum)//2:])\r\n        roughness = high_freq_power / total_power\r\n\r\n        return {\r\n            'roughness': roughness,\r\n            'pattern': self.classify_texture_pattern(power_spectrum)\r\n        }\r\n\r\n    def classify_texture_pattern(self, power_spectrum):\r\n        \"\"\"Classify texture pattern from power spectrum\"\"\"\r\n        # Simple classification based on dominant frequencies\r\n        dominant_freq_idx = np.argmax(power_spectrum[1:]) + 1  # Skip DC component\r\n        normalized_idx = dominant_freq_idx / len(power_spectrum)\r\n\r\n        if normalized_idx < 0.1:\r\n            return 'smooth'\r\n        elif normalized_idx < 0.3:\r\n            return 'patterned'\r\n        else:\r\n            return 'rough'\r\n\r\nclass ProprioceptiveProcessor:\r\n    def __init__(self):\r\n        self.joint_limits = {}\r\n        self.joint_velocities = {}\r\n        self.balance_threshold = 0.1\r\n\r\n    def process_proprioceptive_data(self, joint_states, imu_data):\r\n        \"\"\"Process proprioceptive data from joints and IMU\"\"\"\r\n        processed_data = {\r\n            'joint_positions': joint_states.get('positions', []),\r\n            'joint_velocities': joint_states.get('velocities', []),\r\n            'joint_efforts': joint_states.get('efforts', []),\r\n            'balance_state': self.assess_balance(imu_data),\r\n            'posture_analysis': self.analyze_posture(joint_states),\r\n            'collision_detection': self.detect_self_collision(joint_states)\r\n        }\r\n\r\n        return processed_data\r\n\r\n    def assess_balance(self, imu_data):\r\n        \"\"\"Assess robot balance based on IMU data\"\"\"\r\n        # Extract orientation and acceleration\r\n        orientation = imu_data.get('orientation', [0, 0, 0, 1])  # quaternion\r\n        linear_acceleration = imu_data.get('linear_acceleration', [0, 0, 9.81])\r\n\r\n        # Convert quaternion to roll/pitch angles\r\n        roll = math.atan2(2 * (orientation[0]*orientation[1] + orientation[2]*orientation[3]),\r\n                         1 - 2*(orientation[1]**2 + orientation[2]**2))\r\n        pitch = math.asin(2 * (orientation[0]*orientation[2] - orientation[3]*orientation[1]))\r\n\r\n        # Assess balance based on angles\r\n        balance_score = 1.0 - min(abs(roll), abs(pitch)) / (math.pi/3)  # Assuming 60-degree limit\r\n        balance_score = max(0, balance_score)\r\n\r\n        return {\r\n            'roll_angle': roll,\r\n            'pitch_angle': pitch,\r\n            'balance_score': balance_score,\r\n            'is_balanced': balance_score > self.balance_threshold,\r\n            'center_of_mass': self.estimate_com(joint_states) if 'joint_states' in locals() else [0, 0, 0]\r\n        }\r\n\r\n    def analyze_posture(self, joint_states):\r\n        \"\"\"Analyze robot posture\"\"\"\r\n        # This would use inverse kinematics and posture models\r\n        # For now, providing basic analysis\r\n        joint_positions = joint_states.get('positions', [])\r\n\r\n        posture_analysis = {\r\n            'posture_type': self.classify_posture(joint_positions),\r\n            'joint_ranges': self.check_joint_ranges(joint_positions),\r\n            'energy_efficiency': self.estimate_energy_efficiency(joint_positions)\r\n        }\r\n\r\n        return posture_analysis\r\n\r\n    def classify_posture(self, joint_positions):\r\n        \"\"\"Classify current posture\"\"\"\r\n        # Simplified posture classification\r\n        if len(joint_positions) > 6:  # Assuming humanoid has at least 6 joints\r\n            if abs(joint_positions[0]) < 0.1 and abs(joint_positions[1]) < 0.1:\r\n                return 'standing'\r\n            elif joint_positions[2] > 1.0:\r\n                return 'sitting'\r\n            else:\r\n                return 'walking'\r\n        return 'unknown'\r\n\r\n    def check_joint_ranges(self, joint_positions):\r\n        \"\"\"Check if joints are within safe ranges\"\"\"\r\n        safe_ranges = []\r\n        for i, position in enumerate(joint_positions):\r\n            # Assuming default safe range of [-pi, pi] for all joints\r\n            is_safe = -math.pi <= position <= math.pi\r\n            safe_ranges.append({\r\n                'joint_id': i,\r\n                'position': position,\r\n                'is_safe': is_safe,\r\n                'safety_margin': min(math.pi - abs(position), math.pi) if is_safe else 0\r\n            })\r\n        return safe_ranges\r\n\r\n    def estimate_energy_efficiency(self, joint_positions):\r\n        \"\"\"Estimate energy efficiency of current posture\"\"\"\r\n        # Simplified energy efficiency based on joint angles\r\n        total_deviation = sum(abs(angle) for angle in joint_positions)\r\n        max_possible_deviation = len(joint_positions) * math.pi\r\n        efficiency = 1.0 - (total_deviation / max_possible_deviation)\r\n        return efficiency\r\n\r\n    def detect_self_collision(self, joint_states):\r\n        \"\"\"Detect potential self-collision\"\"\"\r\n        # This would use forward kinematics and collision checking\r\n        # For now, providing placeholder\r\n        return {\r\n            'collision_risk': False,\r\n            'risk_level': 0.0,\r\n            'affected_joints': []\r\n        }\n"})}),"\n",(0,t.jsx)(n.h2,{id:"multimodal-fusion-architecture",children:"Multimodal Fusion Architecture"}),"\n",(0,t.jsx)(n.h3,{id:"attention-based-fusion",children:"Attention-Based Fusion"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass MultimodalAttentionFusion(nn.Module):\r\n    def __init__(self, visual_dim=2048, auditory_dim=128, tactile_dim=64, output_dim=512):\r\n        super(MultimodalAttentionFusion, self).__init__()\r\n\r\n        # Input dimension adapters\r\n        self.visual_adapter = nn.Linear(visual_dim, output_dim)\r\n        self.auditory_adapter = nn.Linear(auditory_dim, output_dim)\r\n        self.tactile_adapter = nn.Linear(tactile_dim, output_dim)\r\n\r\n        # Attention mechanisms for each modality\r\n        self.visual_attention = nn.MultiheadAttention(output_dim, num_heads=8)\r\n        self.auditory_attention = nn.MultiheadAttention(output_dim, num_heads=8)\r\n        self.tactile_attention = nn.MultiheadAttention(output_dim, num_heads=8)\r\n\r\n        # Cross-modal attention\r\n        self.cross_attention = nn.MultiheadAttention(output_dim, num_heads=8)\r\n\r\n        # Output processing\r\n        self.output_layer = nn.Sequential(\r\n            nn.Linear(output_dim * 3, output_dim),  # Concatenated modalities\r\n            nn.ReLU(),\r\n            nn.Linear(output_dim, output_dim)\r\n        )\r\n\r\n        # Confidence prediction for each modality\r\n        self.confidence_predictor = nn.Linear(output_dim, 3)  # One for each modality\r\n\r\n    def forward(self, visual_features, auditory_features, tactile_features):\r\n        # Adapt input dimensions\r\n        visual_adapted = F.relu(self.visual_adapter(visual_features))\r\n        auditory_adapted = F.relu(self.auditory_adapter(auditory_features))\r\n        tactile_adapted = F.relu(self.tactile_adapter(tactile_features))\r\n\r\n        # Self-attention within each modality\r\n        visual_attn, _ = self.visual_attention(visual_adapted, visual_adapted, visual_adapted)\r\n        auditory_attn, _ = self.auditory_attention(auditory_adapted, auditory_adapted, auditory_adapted)\r\n        tactile_attn, _ = self.tactile_attention(tactile_adapted, tactile_adapted, tactile_adapted)\r\n\r\n        # Cross-modal attention\r\n        # Visual attending to auditory and tactile\r\n        visual_cross, _ = self.cross_attention(visual_attn, auditory_attn, tactile_attn)\r\n        # Auditory attending to visual and tactile\r\n        auditory_cross, _ = self.cross_attention(auditory_attn, visual_attn, tactile_attn)\r\n        # Tactile attending to visual and auditory\r\n        tactile_cross, _ = self.cross_attention(tactile_attn, visual_attn, auditory_attn)\r\n\r\n        # Concatenate and process\r\n        concatenated = torch.cat([visual_cross, auditory_cross, tactile_cross], dim=-1)\r\n        output = self.output_layer(concatenated)\r\n\r\n        # Predict confidence for each modality\r\n        confidence_scores = torch.softmax(self.confidence_predictor(output.mean(dim=1)), dim=-1)\r\n\r\n        return output, confidence_scores\r\n\r\nclass MultimodalPerceptionSystem:\r\n    def __init__(self):\r\n        # Initialize modality-specific processors\r\n        self.visual_processor = MultimodalVisualProcessor()\r\n        self.auditory_processor = MultimodalAuditoryProcessor()\r\n        self.tactile_processor = MultimodalTactileProcessor()\r\n        self.proprioceptive_processor = ProprioceptiveProcessor()\r\n\r\n        # Initialize fusion module\r\n        self.fusion_module = MultimodalAttentionFusion()\r\n\r\n        # Initialize sensor fusion\r\n        self.sensor_fusion = MultimodalSensorFusion()\r\n        self.particle_filter = MultimodalParticleFilter()\r\n\r\n        # System state\r\n        self.perception_buffer = {\r\n            'visual': [],\r\n            'auditory': [],\r\n            'tactile': [],\r\n            'proprioceptive': []\r\n        }\r\n        self.fusion_result = None\r\n\r\n    def process_sensor_data(self, sensor_data):\r\n        \"\"\"Process multimodal sensor data\"\"\"\r\n        results = {}\r\n\r\n        # Process visual data\r\n        if 'visual' in sensor_data:\r\n            visual_result = self.visual_processor.extract_scene_context(\r\n                sensor_data['visual']['image']\r\n            )\r\n            results['visual'] = visual_result\r\n            self.perception_buffer['visual'].append(visual_result)\r\n\r\n        # Process auditory data\r\n        if 'auditory' in sensor_data:\r\n            auditory_result = self.auditory_processor.extract_audio_features(\r\n                sensor_data['auditory']['audio'],\r\n                sensor_data['auditory'].get('sample_rate', 16000)\r\n            )\r\n            results['auditory'] = auditory_result\r\n            self.perception_buffer['auditory'].append(auditory_result)\r\n\r\n        # Process tactile data\r\n        if 'tactile' in sensor_data:\r\n            tactile_result = self.tactile_processor.process_tactile_data(\r\n                sensor_data['tactile']\r\n            )\r\n            results['tactile'] = tactile_result\r\n            self.perception_buffer['tactile'].append(tactile_result)\r\n\r\n        # Process proprioceptive data\r\n        if 'proprioceptive' in sensor_data:\r\n            proprioceptive_result = self.proprioceptive_processor.process_proprioceptive_data(\r\n                sensor_data['proprioceptive']['joint_states'],\r\n                sensor_data['proprioceptive']['imu_data']\r\n            )\r\n            results['proprioceptive'] = proprioceptive_result\r\n            self.perception_buffer['proprioceptive'].append(proprioceptive_result)\r\n\r\n        # Perform multimodal fusion\r\n        self.fusion_result = self.perform_fusion(results)\r\n\r\n        return results\r\n\r\n    def perform_fusion(self, modality_results):\r\n        \"\"\"Perform multimodal fusion\"\"\"\r\n        # Prepare features for neural fusion\r\n        visual_features = self.extract_visual_features_for_fusion(modality_results.get('visual'))\r\n        auditory_features = self.extract_auditory_features_for_fusion(modality_results.get('auditory'))\r\n        tactile_features = self.extract_tactile_features_for_fusion(modality_results.get('tactile'))\r\n\r\n        # Perform attention-based fusion\r\n        with torch.no_grad():\r\n            fused_output, confidence_scores = self.fusion_module(\r\n                visual_features,\r\n                auditory_features,\r\n                tactile_features\r\n            )\r\n\r\n        # Combine with traditional fusion methods\r\n        traditional_fusion = self.sensor_fusion.get_fused_estimate()\r\n        particle_estimate = self.particle_filter.process_multimodal_data(modality_results)\r\n\r\n        # Final fusion result\r\n        fusion_result = {\r\n            'neural_fusion': fused_output,\r\n            'confidence_scores': confidence_scores,\r\n            'traditional_estimate': traditional_fusion,\r\n            'particle_estimate': particle_estimate,\r\n            'final_estimate': self.combine_estimates(fused_output, traditional_fusion, particle_estimate)\r\n        }\r\n\r\n        return fusion_result\r\n\r\n    def extract_visual_features_for_fusion(self, visual_result):\r\n        \"\"\"Extract visual features in format suitable for neural fusion\"\"\"\r\n        if visual_result is None:\r\n            return torch.zeros(1, 2048)  # Default visual features\r\n\r\n        # Extract features from visual processing\r\n        features = visual_result.get('features', np.zeros((1, 2048)))\r\n        return torch.tensor(features, dtype=torch.float32)\r\n\r\n    def extract_auditory_features_for_fusion(self, auditory_result):\r\n        \"\"\"Extract auditory features in format suitable for neural fusion\"\"\"\r\n        if auditory_result is None:\r\n            return torch.zeros(1, 128)  # Default auditory features\r\n\r\n        # Use MFCCs as primary features\r\n        mfccs = auditory_result.get('mfcc', np.zeros((13, 100)))  # Assuming 100 time steps\r\n        # Take mean across time dimension\r\n        avg_mfccs = np.mean(mfccs, axis=1)\r\n        return torch.tensor(avg_mfccs[:128], dtype=torch.float32).unsqueeze(0)\r\n\r\n    def extract_tactile_features_for_fusion(self, tactile_result):\r\n        \"\"\"Extract tactile features in format suitable for neural fusion\"\"\"\r\n        if tactile_result is None:\r\n            return torch.zeros(1, 64)  # Default tactile features\r\n\r\n        # Extract relevant tactile features\r\n        contact_points = tactile_result.get('contact_points', [])\r\n        if contact_points:\r\n            # Use force magnitudes as features\r\n            forces = [cp['force'] for cp in contact_points]\r\n            forces = np.array(forces[:64]) if len(forces) >= 64 else np.pad(forces, (0, max(0, 64-len(forces))))\r\n        else:\r\n            forces = np.zeros(64)\r\n\r\n        return torch.tensor(forces, dtype=torch.float32).unsqueeze(0)\r\n\r\n    def combine_estimates(self, neural, traditional, particle):\r\n        \"\"\"Combine different fusion estimates\"\"\"\r\n        # This would implement weighted combination based on confidence\r\n        # For now, returning a simple combination\r\n        if traditional is not None:\r\n            return {\r\n                'position': traditional[:3] if isinstance(traditional, np.ndarray) else [0, 0, 0],\r\n                'confidence': 0.8  # Placeholder\r\n            }\r\n        else:\r\n            return {'position': [0, 0, 0], 'confidence': 0.5}\n"})}),"\n",(0,t.jsx)(n.h2,{id:"real-time-processing-and-optimization",children:"Real-Time Processing and Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"efficient-multimodal-processing",children:"Efficient Multimodal Processing"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import threading\r\nimport queue\r\nimport time\r\nfrom concurrent.futures import ThreadPoolExecutor\r\n\r\nclass RealTimeMultimodalProcessor:\r\n    def __init__(self, max_workers=4):\r\n        self.executor = ThreadPoolExecutor(max_workers=max_workers)\r\n        self.processing_queue = queue.Queue()\r\n        self.result_queue = queue.Queue()\r\n        self.is_running = False\r\n        self.processing_times = []\r\n\r\n    def start_processing(self):\r\n        """Start real-time processing loop"""\r\n        self.is_running = True\r\n        self.processing_thread = threading.Thread(target=self._processing_loop)\r\n        self.processing_thread.daemon = True\r\n        self.processing_thread.start()\r\n\r\n    def stop_processing(self):\r\n        """Stop real-time processing"""\r\n        self.is_running = False\r\n\r\n    def submit_sensor_data(self, sensor_data, timestamp):\r\n        """Submit sensor data for processing"""\r\n        self.processing_queue.put((sensor_data, timestamp))\r\n\r\n    def _processing_loop(self):\r\n        """Main processing loop"""\r\n        while self.is_running:\r\n            try:\r\n                if not self.processing_queue.empty():\r\n                    sensor_data, timestamp = self.processing_queue.get_nowait()\r\n\r\n                    start_time = time.time()\r\n                    result = self._process_multimodal_data(sensor_data)\r\n                    processing_time = time.time() - start_time\r\n\r\n                    self.processing_times.append(processing_time)\r\n\r\n                    # Keep only recent processing times\r\n                    if len(self.processing_times) > 100:\r\n                        self.processing_times = self.processing_times[-100:]\r\n\r\n                    # Add result to queue\r\n                    self.result_queue.put((result, timestamp, processing_time))\r\n                else:\r\n                    time.sleep(0.001)  # Small delay to prevent busy waiting\r\n            except queue.Empty:\r\n                time.sleep(0.001)\r\n            except Exception as e:\r\n                print(f"Processing error: {e}")\r\n\r\n    def _process_multimodal_data(self, sensor_data):\r\n        """Process multimodal data efficiently"""\r\n        # Use the multimodal perception system\r\n        perception_system = MultimodalPerceptionSystem()\r\n        result = perception_system.process_sensor_data(sensor_data)\r\n        return result\r\n\r\n    def get_latest_results(self):\r\n        """Get latest processing results"""\r\n        results = []\r\n        while not self.result_queue.empty():\r\n            try:\r\n                result = self.result_queue.get_nowait()\r\n                results.append(result)\r\n            except queue.Empty:\r\n                break\r\n        return results\r\n\r\n    def get_performance_metrics(self):\r\n        """Get performance metrics"""\r\n        if not self.processing_times:\r\n            return {\r\n                \'avg_processing_time\': 0,\r\n                \'min_processing_time\': 0,\r\n                \'max_processing_time\': 0,\r\n                \'processing_rate\': 0\r\n            }\r\n\r\n        avg_time = sum(self.processing_times) / len(self.processing_times)\r\n        min_time = min(self.processing_times)\r\n        max_time = max(self.processing_times)\r\n\r\n        # Estimate processing rate (assuming 30 FPS input)\r\n        avg_processing_rate = 1.0 / avg_time if avg_time > 0 else 0\r\n\r\n        return {\r\n            \'avg_processing_time\': avg_time,\r\n            \'min_processing_time\': min_time,\r\n            \'max_processing_time\': max_time,\r\n            \'processing_rate\': avg_processing_rate\r\n        }\n'})}),"\n",(0,t.jsx)(n.h2,{id:"evaluation-and-quality-assessment",children:"Evaluation and Quality Assessment"}),"\n",(0,t.jsx)(n.h3,{id:"perception-quality-metrics",children:"Perception Quality Metrics"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import numpy as np\r\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\r\n\r\nclass MultimodalPerceptionEvaluator:\r\n    def __init__(self):\r\n        self.metrics = {\r\n            'accuracy': [],\r\n            'precision': [],\r\n            'recall': [],\r\n            'f1_score': [],\r\n            'confidence_calibration': [],\r\n            'multimodal_consistency': []\r\n        }\r\n\r\n    def evaluate_perception(self, ground_truth, perception_result):\r\n        \"\"\"Evaluate perception accuracy\"\"\"\r\n        metrics = {}\r\n\r\n        # Object detection accuracy\r\n        if 'objects' in ground_truth and 'objects' in perception_result:\r\n            gt_objects = ground_truth['objects']\r\n            pred_objects = perception_result['objects']\r\n            metrics['detection_accuracy'] = self.calculate_detection_accuracy(gt_objects, pred_objects)\r\n\r\n        # Localization accuracy\r\n        if 'position' in ground_truth and 'position' in perception_result:\r\n            gt_pos = np.array(ground_truth['position'])\r\n            pred_pos = np.array(perception_result['position'])\r\n            localization_error = np.linalg.norm(gt_pos - pred_pos)\r\n            metrics['localization_error'] = localization_error\r\n\r\n        # Classification accuracy\r\n        if 'class' in ground_truth and 'class' in perception_result:\r\n            gt_class = ground_truth['class']\r\n            pred_class = perception_result['class']\r\n            metrics['classification_accuracy'] = 1.0 if gt_class == pred_class else 0.0\r\n\r\n        return metrics\r\n\r\n    def calculate_detection_accuracy(self, gt_objects, pred_objects):\r\n        \"\"\"Calculate object detection accuracy\"\"\"\r\n        if not gt_objects and not pred_objects:\r\n            return 1.0  # Both empty, perfect match\r\n        if not gt_objects or not pred_objects:\r\n            return 0.0  # One empty, no match\r\n\r\n        # Calculate IoU for each predicted object\r\n        ious = []\r\n        for pred_obj in pred_objects:\r\n            best_iou = 0\r\n            for gt_obj in gt_objects:\r\n                iou = self.calculate_iou(pred_obj.get('bbox', []), gt_obj.get('bbox', []))\r\n                best_iou = max(best_iou, iou)\r\n            ious.append(best_iou)\r\n\r\n        # Average IoU above threshold\r\n        threshold = 0.5\r\n        accurate_detections = sum(1 for iou in ious if iou > threshold)\r\n        accuracy = accurate_detections / len(pred_objects) if pred_objects else 0\r\n\r\n        return accuracy\r\n\r\n    def calculate_iou(self, bbox1, bbox2):\r\n        \"\"\"Calculate Intersection over Union\"\"\"\r\n        if len(bbox1) < 4 or len(bbox2) < 4:\r\n            return 0.0\r\n\r\n        # Unpack bounding boxes [x1, y1, x2, y2]\r\n        x1_1, y1_1, x2_1, y2_1 = bbox1[:4]\r\n        x1_2, y1_2, x2_2, y2_2 = bbox2[:4]\r\n\r\n        # Calculate intersection\r\n        xi1 = max(x1_1, x1_2)\r\n        yi1 = max(y1_1, y1_2)\r\n        xi2 = min(x2_1, x2_2)\r\n        yi2 = min(y2_1, y2_2)\r\n\r\n        if xi2 < xi1 or yi2 < yi1:\r\n            return 0.0\r\n\r\n        intersection = (xi2 - xi1) * (yi2 - yi1)\r\n\r\n        # Calculate union\r\n        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\r\n        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\r\n        union = area1 + area2 - intersection\r\n\r\n        return intersection / union if union > 0 else 0.0\r\n\r\n    def evaluate_multimodal_consistency(self, modality_results):\r\n        \"\"\"Evaluate consistency across modalities\"\"\"\r\n        consistency_score = 0.0\r\n        consistency_checks = 0\r\n\r\n        # Check visual-auditory consistency (e.g., sound source localization)\r\n        if 'visual' in modality_results and 'auditory' in modality_results:\r\n            visual_objects = modality_results['visual'].get('objects', [])\r\n            auditory_events = modality_results['auditory'].get('sound_events', [])\r\n\r\n            # Check if auditory events correspond to visual objects\r\n            for event in auditory_events:\r\n                for obj in visual_objects:\r\n                    # Simplified consistency check\r\n                    consistency_score += 0.5  # Placeholder\r\n                    consistency_checks += 1\r\n\r\n        # Check tactile-visual consistency\r\n        if 'tactile' in modality_results and 'visual' in modality_results:\r\n            tactile_contacts = modality_results['tactile'].get('contact_points', [])\r\n            visual_objects = modality_results['visual'].get('objects', [])\r\n\r\n            for contact in tactile_contacts:\r\n                for obj in visual_objects:\r\n                    # Check spatial consistency\r\n                    consistency_score += 0.3  # Placeholder\r\n                    consistency_checks += 1\r\n\r\n        return consistency_score / consistency_checks if consistency_checks > 0 else 0.0\r\n\r\n    def assess_confidence_calibration(self, perception_result):\r\n        \"\"\"Assess how well confidence scores match actual accuracy\"\"\"\r\n        # This would compare confidence scores with actual accuracy over time\r\n        # For now, returning placeholder\r\n        return 0.8  # Well-calibrated\r\n\r\n    def generate_evaluation_report(self, test_results):\r\n        \"\"\"Generate comprehensive evaluation report\"\"\"\r\n        report = {\r\n            'overall_metrics': {},\r\n            'modality_specific_metrics': {},\r\n            'consistency_analysis': {},\r\n            'recommendations': []\r\n        }\r\n\r\n        # Calculate overall metrics\r\n        if test_results:\r\n            avg_accuracy = np.mean([r.get('accuracy', 0) for r in test_results])\r\n            avg_precision = np.mean([r.get('precision', 0) for r in test_results])\r\n            avg_recall = np.mean([r.get('recall', 0) for r in test_results])\r\n            avg_f1 = np.mean([r.get('f1_score', 0) for r in test_results])\r\n\r\n            report['overall_metrics'] = {\r\n                'average_accuracy': avg_accuracy,\r\n                'average_precision': avg_precision,\r\n                'average_recall': avg_recall,\r\n                'average_f1_score': avg_f1\r\n            }\r\n\r\n        return report\n"})}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"Multimodal perception is essential for humanoid robots to operate effectively in complex, dynamic environments. By integrating visual, auditory, tactile, and proprioceptive information, robots can achieve robust environmental understanding that surpasses what any single sensor modality can provide."}),"\n",(0,t.jsx)(n.p,{children:"The key components of effective multimodal perception systems include:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor Fusion"}),": Combining information from multiple sensors using techniques like Kalman filters, particle filters, and attention mechanisms"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time Processing"}),": Efficient algorithms that can process multiple sensor streams in real-time"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Quality Assessment"}),": Continuous evaluation of perception accuracy and confidence"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Adaptive Integration"}),": Adjusting fusion strategies based on sensor reliability and environmental conditions"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The success of multimodal perception in humanoid robotics depends on:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Proper sensor calibration and synchronization"}),"\n",(0,t.jsx)(n.li,{children:"Appropriate fusion algorithms for the task"}),"\n",(0,t.jsx)(n.li,{children:"Real-time processing capabilities"}),"\n",(0,t.jsx)(n.li,{children:"Continuous learning and adaptation"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"In the next chapter, we'll explore the capstone project that brings together all the concepts from this module to create an autonomous humanoid robot system capable of complex tasks through vision, language, and action integration."})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>o});var t=r(6540);const s={},i=t.createContext(s);function a(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);