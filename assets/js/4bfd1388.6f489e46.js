"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[707],{5250:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>t,metadata:()=>o,toc:()=>c});var a=r(4848),s=r(8453);const t={sidebar_position:5,title:"Chapter 5: Integrating AI Agents with ROS (rclpy)"},i="Integrating AI Agents with ROS (rclpy)",o={id:"module-1/chapter-5",title:"Chapter 5: Integrating AI Agents with ROS (rclpy)",description:"Overview",source:"@site/docs/module-1/chapter-5.mdx",sourceDirName:"module-1",slug:"/module-1/chapter-5",permalink:"/ai-humanoid-robotics-book/docs/module-1/chapter-5",draft:!1,unlisted:!1,editUrl:"https://github.com/mujahidshaikh18/ai-humanoid-robotics-book/tree/main/docs/module-1/chapter-5.mdx",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5,title:"Chapter 5: Integrating AI Agents with ROS (rclpy)"},sidebar:"tutorialSidebar",previous:{title:"Chapter 4: URDF & Humanoid Robot Description",permalink:"/ai-humanoid-robotics-book/docs/module-1/chapter-4"},next:{title:"Chapter 6: The Digital Twin Concept",permalink:"/ai-humanoid-robotics-book/docs/module-2/chapter-6"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"AI in Robotics Context",id:"ai-in-robotics-context",level:2},{value:"AI-ROS Integration Architecture",id:"ai-ros-integration-architecture",level:3},{value:"Setting Up AI Libraries with ROS 2",id:"setting-up-ai-libraries-with-ros-2",level:2},{value:"Installing Required Libraries",id:"installing-required-libraries",level:3},{value:"Basic AI Node Structure",id:"basic-ai-node-structure",level:3},{value:"Machine Learning Model Integration",id:"machine-learning-model-integration",level:2},{value:"TensorFlow/Keras Integration",id:"tensorflowkeras-integration",level:3},{value:"Reinforcement Learning Integration",id:"reinforcement-learning-integration",level:2},{value:"Basic RL Agent Node",id:"basic-rl-agent-node",level:3},{value:"Computer Vision Integration",id:"computer-vision-integration",level:2},{value:"Image Processing Node",id:"image-processing-node",level:3},{value:"Planning and Decision Making",id:"planning-and-decision-making",level:2},{value:"AI Planning Node",id:"ai-planning-node",level:3},{value:"Model Training Integration",id:"model-training-integration",level:2},{value:"Training Data Collection Node",id:"training-data-collection-node",level:3},{value:"Best Practices for AI-ROS Integration",id:"best-practices-for-ai-ros-integration",level:2},{value:"1. Performance Considerations",id:"1-performance-considerations",level:3},{value:"2. Error Handling and Fallbacks",id:"2-error-handling-and-fallbacks",level:3},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h1,{id:"integrating-ai-agents-with-ros-rclpy",children:"Integrating AI Agents with ROS (rclpy)"}),"\n",(0,a.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(e.p,{children:"This chapter explores the integration of AI agents with ROS 2 using Python. We'll cover how to connect machine learning models, decision-making systems, and AI algorithms with ROS 2's communication infrastructure."}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Integrate AI models with ROS 2 nodes"}),"\n",(0,a.jsx)(e.li,{children:"Design AI-based decision-making systems"}),"\n",(0,a.jsx)(e.li,{children:"Handle sensor data for AI processing"}),"\n",(0,a.jsx)(e.li,{children:"Implement cognitive architectures for robotics"}),"\n",(0,a.jsx)(e.li,{children:"Use ROS 2 for AI model training and inference"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"ai-in-robotics-context",children:"AI in Robotics Context"}),"\n",(0,a.jsx)(e.p,{children:"Artificial Intelligence in robotics encompasses perception, decision-making, and action execution. The integration of AI with ROS 2 enables robots to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Process sensor data using machine learning models"}),"\n",(0,a.jsx)(e.li,{children:"Make intelligent decisions based on environmental understanding"}),"\n",(0,a.jsx)(e.li,{children:"Plan and execute complex behaviors"}),"\n",(0,a.jsx)(e.li,{children:"Learn from experience and adapt to new situations"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"ai-ros-integration-architecture",children:"AI-ROS Integration Architecture"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   AI Agent      \u2502    \u2502   ROS 2         \u2502    \u2502   Robot         \u2502\r\n\u2502                 \u2502\u25c4\u2500\u2500\u25ba\u2502   Infrastructure\u2502\u25c4\u2500\u2500\u25ba\u2502   Hardware      \u2502\r\n\u2502  - Perception   \u2502    \u2502  - Topics       \u2502    \u2502  - Sensors      \u2502\r\n\u2502  - Planning     \u2502    \u2502  - Services     \u2502    \u2502  - Actuators    \u2502\r\n\u2502  - Control      \u2502    \u2502  - Actions      \u2502    \u2502                 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,a.jsx)(e.h2,{id:"setting-up-ai-libraries-with-ros-2",children:"Setting Up AI Libraries with ROS 2"}),"\n",(0,a.jsx)(e.h3,{id:"installing-required-libraries",children:"Installing Required Libraries"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"pip install tensorflow torch torchvision scikit-learn\r\n# Or using apt for system packages\r\nsudo apt install python3-tensorflow python3-torch\n"})}),"\n",(0,a.jsx)(e.h3,{id:"basic-ai-node-structure",children:"Basic AI Node Structure"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nimport numpy as np\r\nimport tensorflow as tf  # Example AI library\r\n\r\nclass AIAgentNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'ai_agent_node\')\r\n\r\n        # Initialize AI model\r\n        self.model = self.initialize_model()\r\n\r\n        # Create subscribers for sensor data\r\n        self.sensor_sub = self.create_subscription(\r\n            # Define your sensor message type\r\n            \'sensor_msgs/msg/LaserScan\',\r\n            \'scan\',\r\n            self.sensor_callback,\r\n            10\r\n        )\r\n\r\n        # Create publishers for AI decisions\r\n        self.action_pub = self.create_publisher(\r\n            # Define your action message type\r\n            \'geometry_msgs/msg/Twist\',\r\n            \'cmd_vel\',\r\n            10\r\n        )\r\n\r\n        # Create timer for AI processing\r\n        self.ai_timer = self.create_timer(0.1, self.ai_processing_loop)\r\n\r\n        self.get_logger().info(\'AI Agent Node initialized\')\r\n\r\n    def initialize_model(self):\r\n        """Initialize and return your AI model"""\r\n        # Example: Load a pre-trained model\r\n        # model = tf.keras.models.load_model(\'path/to/model\')\r\n        # For this example, we\'ll create a simple placeholder\r\n        return "placeholder_model"\r\n\r\n    def sensor_callback(self, msg):\r\n        """Process incoming sensor data"""\r\n        # Convert ROS message to format suitable for AI model\r\n        sensor_data = self.process_sensor_data(msg)\r\n\r\n        # Store data for AI processing\r\n        self.current_sensor_data = sensor_data\r\n\r\n    def process_sensor_data(self, msg):\r\n        """Convert ROS sensor message to AI-friendly format"""\r\n        # Implementation depends on message type\r\n        return np.array([])  # Placeholder\r\n\r\n    def ai_processing_loop(self):\r\n        """Main AI processing loop"""\r\n        if hasattr(self, \'current_sensor_data\'):\r\n            # Run AI inference\r\n            action = self.run_inference(self.current_sensor_data)\r\n\r\n            # Publish AI decision\r\n            self.publish_action(action)\r\n\r\n    def run_inference(self, sensor_data):\r\n        """Run AI model inference"""\r\n        # Placeholder implementation\r\n        return np.array([0.0, 0.0])  # [linear_velocity, angular_velocity]\r\n\r\n    def publish_action(self, action):\r\n        """Publish AI decision to robot"""\r\n        from geometry_msgs.msg import Twist\r\n        msg = Twist()\r\n        msg.linear.x = float(action[0])\r\n        msg.angular.z = float(action[1])\r\n        self.action_pub.publish(msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    ai_agent_node = AIAgentNode()\r\n\r\n    try:\r\n        rclpy.spin(ai_agent_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        ai_agent_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"machine-learning-model-integration",children:"Machine Learning Model Integration"}),"\n",(0,a.jsx)(e.h3,{id:"tensorflowkeras-integration",children:"TensorFlow/Keras Integration"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom sensor_msgs.msg import LaserScan\r\nfrom geometry_msgs.msg import Twist\r\n\r\nclass MLNavigationNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'ml_navigation_node\')\r\n\r\n        # Load pre-trained navigation model\r\n        self.navigation_model = self.load_navigation_model()\r\n\r\n        # Subscribers and publishers\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan,\r\n            \'scan\',\r\n            self.scan_callback,\r\n            10\r\n        )\r\n\r\n        self.cmd_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\r\n\r\n        # Store latest sensor data\r\n        self.latest_scan = None\r\n\r\n        # Processing timer\r\n        self.process_timer = self.create_timer(0.1, self.process_data)\r\n\r\n    def load_navigation_model(self):\r\n        """Load pre-trained navigation model"""\r\n        try:\r\n            # Load your trained model\r\n            # model = tf.keras.models.load_model(\'navigation_model.h5\')\r\n            # For this example, we\'ll create a simple model\r\n            model = tf.keras.Sequential([\r\n                tf.keras.layers.Dense(64, activation=\'relu\', input_shape=(360,)),\r\n                tf.keras.layers.Dense(32, activation=\'relu\'),\r\n                tf.keras.layers.Dense(2, activation=\'tanh\')  # [linear, angular]\r\n            ])\r\n            return model\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Failed to load model: {e}\')\r\n            return None\r\n\r\n    def scan_callback(self, msg):\r\n        """Process laser scan data"""\r\n        # Convert scan ranges to numpy array\r\n        ranges = np.array(msg.ranges)\r\n        # Handle infinite/nan values\r\n        ranges = np.nan_to_num(ranges, nan=10.0, posinf=10.0, neginf=0.0)\r\n\r\n        # Normalize ranges to [0, 1]\r\n        ranges = np.clip(ranges, 0.0, 10.0) / 10.0\r\n\r\n        self.latest_scan = ranges\r\n\r\n    def process_data(self):\r\n        """Process sensor data with ML model"""\r\n        if self.latest_scan is not None and self.navigation_model is not None:\r\n            # Ensure scan has correct shape\r\n            if len(self.latest_scan) == 360:  # Assuming 360-degree scan\r\n                # Add batch dimension\r\n                input_data = np.expand_dims(self.latest_scan, axis=0)\r\n\r\n                # Run inference\r\n                try:\r\n                    output = self.navigation_model.predict(input_data, verbose=0)\r\n\r\n                    # Extract velocities [linear, angular]\r\n                    linear_vel = float(output[0][0] * 0.5)  # Scale to reasonable speed\r\n                    angular_vel = float(output[0][1] * 1.0)  # Scale to reasonable rotation\r\n\r\n                    # Publish command\r\n                    self.publish_command(linear_vel, angular_vel)\r\n\r\n                except Exception as e:\r\n                    self.get_logger().error(f\'Model inference failed: {e}\')\r\n\r\n    def publish_command(self, linear, angular):\r\n        """Publish velocity command"""\r\n        msg = Twist()\r\n        msg.linear.x = linear\r\n        msg.angular.z = angular\r\n        self.cmd_pub.publish(msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = MLNavigationNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"reinforcement-learning-integration",children:"Reinforcement Learning Integration"}),"\n",(0,a.jsx)(e.h3,{id:"basic-rl-agent-node",children:"Basic RL Agent Node"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom sensor_msgs.msg import LaserScan\r\nfrom geometry_msgs.msg import Twist\r\nfrom std_msgs.msg import Float32\r\n\r\nclass RLAgentNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'rl_agent_node\')\r\n\r\n        # Initialize RL components\r\n        self.state_size = 360  # For laser scan\r\n        self.action_size = 2   # linear and angular velocity\r\n        self.learning_rate = 0.001\r\n\r\n        # Neural network for policy\r\n        self.policy_network = self.create_policy_network()\r\n        self.optimizer = optim.Adam(self.policy_network.parameters(),\r\n                                   lr=self.learning_rate)\r\n\r\n        # Subscribers and publishers\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan, \'scan\', self.scan_callback, 10)\r\n        self.cmd_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\r\n        self.reward_pub = self.create_publisher(Float32, \'reward\', 10)\r\n\r\n        # Episode management\r\n        self.current_state = None\r\n        self.previous_state = None\r\n        self.episode_step = 0\r\n        self.max_episode_steps = 1000\r\n\r\n        # Learning timer\r\n        self.learning_timer = self.create_timer(0.1, self.learning_step)\r\n\r\n    def create_policy_network(self):\r\n        """Create neural network for policy"""\r\n        class PolicyNetwork(nn.Module):\r\n            def __init__(self, state_size, action_size):\r\n                super(PolicyNetwork, self).__init__()\r\n                self.fc1 = nn.Linear(state_size, 128)\r\n                self.fc2 = nn.Linear(128, 64)\r\n                self.fc3 = nn.Linear(64, action_size)\r\n                self.tanh = nn.Tanh()\r\n\r\n            def forward(self, x):\r\n                x = torch.relu(self.fc1(x))\r\n                x = torch.relu(self.fc2(x))\r\n                x = self.tanh(self.fc3(x))  # Output in [-1, 1] range\r\n                return x\r\n\r\n        return PolicyNetwork(self.state_size, self.action_size)\r\n\r\n    def scan_callback(self, msg):\r\n        """Process laser scan and update state"""\r\n        ranges = np.array(msg.ranges)\r\n        ranges = np.nan_to_num(ranges, nan=10.0, posinf=10.0, neginf=0.0)\r\n        ranges = np.clip(ranges, 0.0, 10.0) / 10.0\r\n\r\n        self.previous_state = self.current_state\r\n        self.current_state = ranges\r\n\r\n    def calculate_reward(self, current_scan, action):\r\n        """Calculate reward based on current state and action"""\r\n        # Example reward function: avoid obstacles and move forward\r\n        min_distance = np.min(current_scan)\r\n\r\n        # Reward for moving forward when safe\r\n        forward_reward = 0.0\r\n        if action[0] > 0.3 and min_distance > 0.5:  # Moving forward safely\r\n            forward_reward = 0.1\r\n\r\n        # Penalty for being close to obstacles\r\n        obstacle_penalty = 0.0\r\n        if min_distance < 0.3:\r\n            obstacle_penalty = -1.0\r\n        elif min_distance < 0.5:\r\n            obstacle_penalty = -0.3\r\n\r\n        # Penalty for turning too much when not needed\r\n        turn_penalty = -0.01 * abs(action[1])\r\n\r\n        total_reward = forward_reward + obstacle_penalty + turn_penalty\r\n        return total_reward\r\n\r\n    def learning_step(self):\r\n        """Perform one step of learning"""\r\n        if (self.current_state is not None and\r\n            self.previous_state is not None):\r\n\r\n            # Convert to torch tensors\r\n            state_tensor = torch.FloatTensor(self.previous_state).unsqueeze(0)\r\n\r\n            # Get action from policy\r\n            with torch.no_grad():\r\n                action_tensor = self.policy_network(state_tensor)\r\n                action = action_tensor.numpy()[0]\r\n\r\n            # Calculate reward\r\n            reward = self.calculate_reward(self.current_state, action)\r\n\r\n            # Publish reward for monitoring\r\n            reward_msg = Float32()\r\n            reward_msg.data = reward\r\n            self.reward_pub.publish(reward_msg)\r\n\r\n            # Prepare for training (this is a simplified example)\r\n            # In a real implementation, you would store experiences\r\n            # and train on batches\r\n            self.train_step(self.previous_state, action, reward)\r\n\r\n            # Publish action to robot\r\n            self.publish_action(action)\r\n\r\n            self.episode_step += 1\r\n\r\n            # Reset if episode is done\r\n            if self.episode_step >= self.max_episode_steps:\r\n                self.episode_step = 0\r\n\r\n    def train_step(self, state, action, reward):\r\n        """Perform one training step"""\r\n        # This is a simplified example\r\n        # In practice, you would implement proper RL algorithms\r\n        # like DQN, PPO, or A2C\r\n\r\n        # Convert to tensors\r\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\r\n        action_tensor = torch.FloatTensor(action).unsqueeze(0)\r\n        reward_tensor = torch.FloatTensor([reward])\r\n\r\n        # Simple policy gradient update (conceptual)\r\n        self.optimizer.zero_grad()\r\n\r\n        # Get log probability of action (simplified)\r\n        predicted_action = self.policy_network(state_tensor)\r\n\r\n        # Calculate loss (simplified - real implementation would be more complex)\r\n        loss = -torch.mean(reward_tensor * torch.sum(\r\n            (predicted_action - action_tensor) ** 2, dim=1))\r\n\r\n        loss.backward()\r\n        self.optimizer.step()\r\n\r\n    def publish_action(self, action):\r\n        """Publish action to robot"""\r\n        msg = Twist()\r\n        msg.linear.x = float(action[0] * 0.5)  # Scale linear velocity\r\n        msg.angular.z = float(action[1] * 1.0)  # Scale angular velocity\r\n        self.cmd_pub.publish(msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = RLAgentNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"computer-vision-integration",children:"Computer Vision Integration"}),"\n",(0,a.jsx)(e.h3,{id:"image-processing-node",children:"Image Processing Node"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\nfrom geometry_msgs.msg import Twist\r\nfrom std_msgs.msg import String\r\n\r\nclass VisionNavigationNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'vision_navigation_node\')\r\n\r\n        # Initialize OpenCV bridge\r\n        self.bridge = CvBridge()\r\n\r\n        # Subscribers and publishers\r\n        self.image_sub = self.create_subscription(\r\n            Image, \'camera/image_raw\', self.image_callback, 10)\r\n        self.cmd_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\r\n        self.vision_pub = self.create_publisher(String, \'vision_output\', 10)\r\n\r\n        # Processing timer\r\n        self.process_timer = self.create_timer(0.1, self.process_vision)\r\n\r\n        # Store latest image\r\n        self.latest_image = None\r\n        self.processed_result = None\r\n\r\n    def image_callback(self, msg):\r\n        """Process incoming image"""\r\n        try:\r\n            # Convert ROS Image message to OpenCV image\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\r\n\r\n            # Store for processing\r\n            self.latest_image = cv_image\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Image conversion failed: {e}\')\r\n\r\n    def process_vision(self):\r\n        """Process vision data and make navigation decisions"""\r\n        if self.latest_image is not None:\r\n            # Example: Simple color-based navigation\r\n            # Detect blue objects (could represent targets)\r\n            hsv = cv2.cvtColor(self.latest_image, cv2.COLOR_BGR2HSV)\r\n\r\n            # Define range for blue color\r\n            lower_blue = np.array([100, 50, 50])\r\n            upper_blue = np.array([130, 255, 255])\r\n\r\n            # Create mask for blue objects\r\n            mask = cv2.inRange(hsv, lower_blue, upper_blue)\r\n\r\n            # Find contours of blue objects\r\n            contours, _ = cv2.findContours(\r\n                mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n\r\n            if contours:\r\n                # Find the largest contour\r\n                largest_contour = max(contours, key=cv2.contourArea)\r\n\r\n                # Get center of contour\r\n                M = cv2.moments(largest_contour)\r\n                if M["m00"] != 0:\r\n                    cx = int(M["m10"] / M["m00"])\r\n                    cy = int(M["m01"] / M["m00"])\r\n\r\n                    # Calculate steering based on object position\r\n                    image_center = self.latest_image.shape[1] // 2\r\n                    error = cx - image_center\r\n\r\n                    # Convert to angular velocity\r\n                    angular_vel = -error * 0.005  # Scale factor\r\n                    linear_vel = 0.3  # Move forward at constant speed\r\n\r\n                    # Publish navigation command\r\n                    self.publish_navigation_command(linear_vel, angular_vel)\r\n\r\n                    # Publish vision result\r\n                    result_msg = String()\r\n                    result_msg.data = f"Target detected at ({cx}, {cy})"\r\n                    self.vision_pub.publish(result_msg)\r\n                else:\r\n                    # No clear target, stop or search\r\n                    self.publish_navigation_command(0.0, 0.2)  # Turn slowly\r\n            else:\r\n                # No target found, search\r\n                self.publish_navigation_command(0.1, 0.3)  # Move forward and turn\r\n\r\n    def publish_navigation_command(self, linear, angular):\r\n        """Publish navigation command"""\r\n        msg = Twist()\r\n        msg.linear.x = linear\r\n        msg.angular.z = angular\r\n        self.cmd_pub.publish(msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VisionNavigationNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"planning-and-decision-making",children:"Planning and Decision Making"}),"\n",(0,a.jsx)(e.h3,{id:"ai-planning-node",children:"AI Planning Node"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import PoseStamped, Point\r\nfrom nav_msgs.msg import OccupancyGrid\r\nfrom std_msgs.msg import String\r\nimport numpy as np\r\nimport heapq\r\n\r\nclass AIPlanningNode(Node):\r\n    def __init__(self):\r\n        super().__init__('ai_planning_node')\r\n\r\n        # Subscribers and publishers\r\n        self.map_sub = self.create_subscription(\r\n            OccupancyGrid, 'map', self.map_callback, 10)\r\n        self.goal_sub = self.create_subscription(\r\n            PoseStamped, 'goal_pose', self.goal_callback, 10)\r\n        self.plan_pub = self.create_publisher(\r\n            String, 'planning_status', 10)\r\n\r\n        # Store map and goals\r\n        self.map_data = None\r\n        self.current_goal = None\r\n        self.planning_timer = self.create_timer(1.0, self.plan_if_needed)\r\n\r\n    def map_callback(self, msg):\r\n        \"\"\"Process occupancy grid map\"\"\"\r\n        self.map_data = {\r\n            'data': np.array(msg.data).reshape(msg.info.height, msg.info.width),\r\n            'info': msg.info\r\n        }\r\n\r\n    def goal_callback(self, msg):\r\n        \"\"\"Receive new goal\"\"\"\r\n        self.current_goal = msg\r\n        self.get_logger().info(f'New goal received: ({msg.pose.position.x}, {msg.pose.position.y})')\r\n\r\n    def plan_if_needed(self):\r\n        \"\"\"Plan path if goal is set and map is available\"\"\"\r\n        if self.current_goal is not None and self.map_data is not None:\r\n            # Convert goal position to map coordinates\r\n            goal_x = int((self.current_goal.pose.position.x - self.map_data['info'].origin.position.x) /\r\n                        self.map_data['info'].resolution)\r\n            goal_y = int((self.current_goal.pose.position.y - self.map_data['info'].origin.position.y) /\r\n                        self.map_data['info'].resolution)\r\n\r\n            # Get current position (simplified - in real system, get from TF or odometry)\r\n            current_x, current_y = 50, 50  # Example starting position\r\n\r\n            # Plan path using A* algorithm\r\n            path = self.a_star_plan(current_x, current_y, goal_x, goal_y)\r\n\r\n            if path:\r\n                self.get_logger().info(f'Path found with {len(path)} waypoints')\r\n                # In a real system, you would publish the path to a path follower\r\n            else:\r\n                self.get_logger().warn('No path found to goal')\r\n\r\n    def a_star_plan(self, start_x, start_y, goal_x, goal_y):\r\n        \"\"\"A* path planning algorithm\"\"\"\r\n        if (start_x < 0 or start_x >= self.map_data['info'].width or\r\n            start_y < 0 or start_y >= self.map_data['info'].height or\r\n            goal_x < 0 or goal_x >= self.map_data['info'].width or\r\n            goal_y < 0 or goal_y >= self.map_data['info'].height):\r\n            return None\r\n\r\n        # Check if start or goal is occupied\r\n        if (self.map_data['data'][start_y, start_x] > 50 or  # Occupied threshold\r\n            self.map_data['data'][goal_y, goal_x] > 50):\r\n            return None\r\n\r\n        # A* algorithm implementation\r\n        open_set = [(0, start_x, start_y)]\r\n        came_from = {}\r\n        g_score = {(start_x, start_y): 0}\r\n        f_score = {(start_x, start_y): self.heuristic(start_x, start_y, goal_x, goal_y)}\r\n\r\n        while open_set:\r\n            current = heapq.heappop(open_set)[1:]\r\n\r\n            if current == (goal_x, goal_y):\r\n                # Reconstruct path\r\n                path = []\r\n                while current in came_from:\r\n                    path.append(current)\r\n                    current = came_from[current]\r\n                path.append((start_x, start_y))\r\n                path.reverse()\r\n                return path\r\n\r\n            for neighbor in self.get_neighbors(current[0], current[1]):\r\n                if self.map_data['data'][neighbor[1], neighbor[0]] > 50:  # Occupied\r\n                    continue\r\n\r\n                tentative_g_score = g_score[current] + self.distance(current, neighbor)\r\n\r\n                if neighbor not in g_score or tentative_g_score < g_score[neighbor]:\r\n                    came_from[neighbor] = current\r\n                    g_score[neighbor] = tentative_g_score\r\n                    f_score[neighbor] = tentative_g_score + self.heuristic(neighbor[0], neighbor[1], goal_x, goal_y)\r\n                    heapq.heappush(open_set, (f_score[neighbor], neighbor[0], neighbor[1]))\r\n\r\n        return None  # No path found\r\n\r\n    def heuristic(self, x1, y1, x2, y2):\r\n        \"\"\"Heuristic function (Manhattan distance)\"\"\"\r\n        return abs(x1 - x2) + abs(y1 - y2)\r\n\r\n    def get_neighbors(self, x, y):\r\n        \"\"\"Get 8-connected neighbors\"\"\"\r\n        neighbors = []\r\n        for dx in [-1, 0, 1]:\r\n            for dy in [-1, 0, 1]:\r\n                if dx == 0 and dy == 0:\r\n                    continue\r\n                nx, ny = x + dx, y + dy\r\n                if (0 <= nx < self.map_data['info'].width and\r\n                    0 <= ny < self.map_data['info'].height):\r\n                    neighbors.append((nx, ny))\r\n        return neighbors\r\n\r\n    def distance(self, pos1, pos2):\r\n        \"\"\"Calculate distance between two positions\"\"\"\r\n        dx = pos1[0] - pos2[0]\r\n        dy = pos1[1] - pos2[1]\r\n        return np.sqrt(dx*dx + dy*dy)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = AIPlanningNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"model-training-integration",children:"Model Training Integration"}),"\n",(0,a.jsx)(e.h3,{id:"training-data-collection-node",children:"Training Data Collection Node"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan, Image\r\nfrom geometry_msgs.msg import Twist\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport json\r\nimport os\r\nfrom datetime import datetime\r\n\r\nclass TrainingDataCollectorNode(Node):\r\n    def __init__(self):\r\n        super().__init__('training_data_collector')\r\n\r\n        # Initialize data collection\r\n        self.bridge = CvBridge()\r\n        self.data_buffer = []\r\n        self.max_buffer_size = 1000\r\n\r\n        # Create directory for data\r\n        self.data_dir = 'training_data'\r\n        os.makedirs(self.data_dir, exist_ok=True)\r\n\r\n        # Subscribers\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan, 'scan', self.scan_callback, 10)\r\n        self.image_sub = self.create_subscription(\r\n            Image, 'camera/image_raw', self.image_callback, 10)\r\n        self.cmd_sub = self.create_subscription(\r\n            Twist, 'cmd_vel', self.command_callback, 10)\r\n\r\n        # Store latest data\r\n        self.latest_scan = None\r\n        self.latest_image = None\r\n        self.latest_command = None\r\n\r\n        # Data collection timer\r\n        self.collection_timer = self.create_timer(0.2, self.collect_data)\r\n\r\n        # Data saving timer\r\n        self.save_timer = self.create_timer(5.0, self.save_data)\r\n\r\n    def scan_callback(self, msg):\r\n        \"\"\"Store laser scan data\"\"\"\r\n        self.latest_scan = {\r\n            'ranges': list(msg.ranges),\r\n            'intensities': list(msg.intensities),\r\n            'header': {\r\n                'stamp': msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9,\r\n                'frame_id': msg.header.frame_id\r\n            }\r\n        }\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Store image data\"\"\"\r\n        try:\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\r\n            # Store a simplified version of the image (downsampled)\r\n            small_image = cv2.resize(cv_image, (64, 64))\r\n            self.latest_image = {\r\n                'image': small_image.tolist(),  # Convert to list for JSON serialization\r\n                'header': {\r\n                    'stamp': msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9,\r\n                    'frame_id': msg.header.frame_id\r\n                }\r\n            }\r\n        except Exception as e:\r\n            self.get_logger().error(f'Image processing failed: {e}')\r\n\r\n    def command_callback(self, msg):\r\n        \"\"\"Store command data\"\"\"\r\n        self.latest_command = {\r\n            'linear_x': msg.linear.x,\r\n            'angular_z': msg.angular.z,\r\n            'timestamp': self.get_clock().now().nanoseconds * 1e-9\r\n        }\r\n\r\n    def collect_data(self):\r\n        \"\"\"Collect synchronized data samples\"\"\"\r\n        if (self.latest_scan is not None and\r\n            self.latest_image is not None and\r\n            self.latest_command is not None):\r\n\r\n            # Create data sample\r\n            sample = {\r\n                'sensor_data': {\r\n                    'scan': self.latest_scan,\r\n                    'image': self.latest_image\r\n                },\r\n                'action': self.latest_command,\r\n                'timestamp': datetime.now().isoformat()\r\n            }\r\n\r\n            # Add to buffer\r\n            self.data_buffer.append(sample)\r\n\r\n            # Keep buffer size manageable\r\n            if len(self.data_buffer) > self.max_buffer_size:\r\n                self.data_buffer.pop(0)\r\n\r\n            self.latest_scan = None\r\n            self.latest_image = None\r\n            self.latest_command = None\r\n\r\n    def save_data(self):\r\n        \"\"\"Save collected data to file\"\"\"\r\n        if self.data_buffer:\r\n            filename = f\"{self.data_dir}/training_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\r\n            with open(filename, 'w') as f:\r\n                json.dump(self.data_buffer, f)\r\n\r\n            self.get_logger().info(f'Saved {len(self.data_buffer)} samples to {filename}')\r\n            self.data_buffer.clear()  # Clear buffer after saving\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = TrainingDataCollectorNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        # Save remaining data on shutdown\r\n        node.save_data()\r\n        pass\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"best-practices-for-ai-ros-integration",children:"Best Practices for AI-ROS Integration"}),"\n",(0,a.jsx)(e.h3,{id:"1-performance-considerations",children:"1. Performance Considerations"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Use threading for heavy AI computations\r\nimport threading\r\nfrom concurrent.futures import ThreadPoolExecutor\r\n\r\nclass OptimizedAINode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'optimized_ai_node\')\r\n\r\n        # Use thread pool for AI inference\r\n        self.executor = ThreadPoolExecutor(max_workers=2)\r\n\r\n        # Throttle AI processing to avoid overwhelming the system\r\n        self.ai_process_rate = 10  # Hz\r\n        self.ai_timer = self.create_timer(\r\n            1.0/self.ai_process_rate, self.throttled_ai_process)\r\n\r\n    def throttled_ai_process(self):\r\n        """Process AI tasks with rate limiting"""\r\n        # Submit AI task to thread pool\r\n        future = self.executor.submit(self.run_ai_inference)\r\n        # Handle result asynchronously\r\n        future.add_done_callback(self.ai_result_callback)\r\n\r\n    def ai_result_callback(self, future):\r\n        """Handle AI inference results"""\r\n        try:\r\n            result = future.result()\r\n            # Process result\r\n            self.publish_ai_output(result)\r\n        except Exception as e:\r\n            self.get_logger().error(f\'AI inference failed: {e}\')\n'})}),"\n",(0,a.jsx)(e.h3,{id:"2-error-handling-and-fallbacks",children:"2. Error Handling and Fallbacks"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class RobustAINode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'robust_ai_node\')\r\n\r\n        # Initialize with fallback behavior\r\n        self.ai_active = True\r\n        self.fallback_behavior = "stop"  # or "wander", "return_home", etc.\r\n\r\n    def run_ai_inference(self):\r\n        """Run AI inference with error handling"""\r\n        try:\r\n            if self.ai_model is not None:\r\n                result = self.ai_model.predict(self.current_sensor_data)\r\n                return result\r\n            else:\r\n                self.get_logger().warn(\'AI model not loaded, using fallback\')\r\n                return self.fallback_action()\r\n        except Exception as e:\r\n            self.get_logger().error(f\'AI inference error: {e}\')\r\n            return self.fallback_action()\r\n\r\n    def fallback_action(self):\r\n        """Define safe fallback behavior"""\r\n        if self.fallback_behavior == "stop":\r\n            return [0.0, 0.0]  # Stop\r\n        elif self.fallback_behavior == "wander":\r\n            import random\r\n            return [0.2, random.uniform(-0.5, 0.5)]  # Gentle random movement\n'})}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"This chapter covered the integration of AI agents with ROS 2 using Python. We explored various approaches to connect machine learning models, reinforcement learning agents, computer vision systems, and planning algorithms with ROS 2's communication infrastructure. Key takeaways include:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Model Integration"}),": How to load and run AI models within ROS 2 nodes"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Data Flow"}),": Managing sensor data input and AI decision output"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Real-time Processing"}),": Techniques for handling real-time AI inference"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Safety Considerations"}),": Implementing fallback behaviors and error handling"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Training Integration"}),": Collecting data for model training"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"The integration of AI with ROS 2 enables powerful robotic applications that can perceive, reason, and act intelligently in complex environments. This foundation will be essential as we move to more advanced topics in the subsequent modules of this book."}),"\n",(0,a.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(e.p,{children:"In Module 2, we'll explore how to simulate these AI-powered robots in digital twin environments using Gazebo and Unity, allowing us to test and refine our AI algorithms in safe, controlled virtual environments before deploying them on physical robots."})]})}function m(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>i,x:()=>o});var a=r(6540);const s={},t=a.createContext(s);function i(n){const e=a.useContext(t);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:i(n.components),a.createElement(t.Provider,{value:e},n.children)}}}]);