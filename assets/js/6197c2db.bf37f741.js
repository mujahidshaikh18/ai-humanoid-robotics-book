"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[825],{3739:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>m,frontMatter:()=>i,metadata:()=>o,toc:()=>c});var s=r(4848),a=r(8453);const i={sidebar_position:15,title:"Chapter 15: Reinforcement Learning & Sim-to-Real"},t="Reinforcement Learning & Sim-to-Real",o={id:"module-3/chapter-15",title:"Chapter 15: Reinforcement Learning & Sim-to-Real",description:"Overview",source:"@site/docs/module-3/chapter-15.mdx",sourceDirName:"module-3",slug:"/module-3/chapter-15",permalink:"/ai-humanoid-robotics-book/docs/module-3/chapter-15",draft:!1,unlisted:!1,editUrl:"https://github.com/mujahidshaikh18/ai-humanoid-robotics-book/tree/main/docs/module-3/chapter-15.mdx",tags:[],version:"current",sidebarPosition:15,frontMatter:{sidebar_position:15,title:"Chapter 15: Reinforcement Learning & Sim-to-Real"},sidebar:"tutorialSidebar",previous:{title:"Chapter 14: Path Planning with Nav2",permalink:"/ai-humanoid-robotics-book/docs/module-3/chapter-14"},next:{title:"Chapter 16: Introduction to VLA Systems",permalink:"/ai-humanoid-robotics-book/docs/module-4/chapter-16"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Reinforcement Learning Fundamentals for Robotics",id:"reinforcement-learning-fundamentals-for-robotics",level:2},{value:"RL in Robotics Context",id:"rl-in-robotics-context",level:3},{value:"Key RL Concepts for Robotics",id:"key-rl-concepts-for-robotics",level:3},{value:"Challenges in Robotic RL",id:"challenges-in-robotic-rl",level:3},{value:"Isaac RL Framework",id:"isaac-rl-framework",level:2},{value:"Isaac Gym Integration",id:"isaac-gym-integration",level:3},{value:"Deep Reinforcement Learning Implementation",id:"deep-reinforcement-learning-implementation",level:2},{value:"Actor-Critic Network Architecture",id:"actor-critic-network-architecture",level:3},{value:"Domain Randomization for Sim-to-Real Transfer",id:"domain-randomization-for-sim-to-real-transfer",level:2},{value:"Physics Parameter Randomization",id:"physics-parameter-randomization",level:3},{value:"Visual Domain Randomization",id:"visual-domain-randomization",level:3},{value:"Isaac RL Training Loop",id:"isaac-rl-training-loop",level:2},{value:"Complete Training Implementation",id:"complete-training-implementation",level:3},{value:"Sim-to-Real Transfer Techniques",id:"sim-to-real-transfer-techniques",level:2},{value:"System Identification and Modeling",id:"system-identification-and-modeling",level:3},{value:"Robust Control Design",id:"robust-control-design",level:3},{value:"Transfer Validation and Testing",id:"transfer-validation-and-testing",level:2},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Practical Implementation Guidelines",id:"practical-implementation-guidelines",level:2},{value:"Best Practices for Sim-to-Real Transfer",id:"best-practices-for-sim-to-real-transfer",level:3},{value:"Troubleshooting and Debugging",id:"troubleshooting-and-debugging",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"reinforcement-learning--sim-to-real",children:"Reinforcement Learning & Sim-to-Real"}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Reinforcement Learning (RL) has emerged as a powerful approach for training complex robotic behaviors, particularly for humanoid robots that require sophisticated control strategies. NVIDIA Isaac provides extensive support for RL training in simulation with the goal of transferring learned policies to real robots. This chapter explores the principles and practical implementation of RL in Isaac, focusing on sim-to-real transfer techniques."}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand reinforcement learning fundamentals for robotics"}),"\n",(0,s.jsx)(n.li,{children:"Implement RL environments in Isaac for humanoid robots"}),"\n",(0,s.jsx)(n.li,{children:"Apply domain randomization techniques for sim-to-real transfer"}),"\n",(0,s.jsx)(n.li,{children:"Train humanoid robot policies using Isaac's RL framework"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate and validate sim-to-real transfer performance"}),"\n",(0,s.jsx)(n.li,{children:"Address challenges in transferring learned behaviors"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"reinforcement-learning-fundamentals-for-robotics",children:"Reinforcement Learning Fundamentals for Robotics"}),"\n",(0,s.jsx)(n.h3,{id:"rl-in-robotics-context",children:"RL in Robotics Context"}),"\n",(0,s.jsx)(n.p,{children:"Reinforcement Learning in robotics involves an agent (the robot) learning to perform tasks by interacting with an environment to maximize cumulative rewards. For humanoid robots, this can include tasks like walking, balancing, manipulation, and navigation."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   Humanoid      \u2502    \u2502   Environment   \u2502    \u2502   RL Algorithm  \u2502\r\n\u2502   Robot         \u2502    \u2502   (Simulation   \u2502    \u2502   (Policy       \u2502\r\n\u2502   (Agent)       \u2502\u25c4\u2500\u2500\u25ba\u2502   or Real World)\u2502\u25c4\u2500\u2500\u25ba\u2502   Network)      \u2502\r\n\u2502                 \u2502    \u2502                 \u2502    \u2502                 \u2502\r\n\u2502 - Actions       \u2502    \u2502 - States        \u2502    \u2502 - Rewards       \u2502\r\n\u2502 - Observations  \u2502    \u2502 - Transitions   \u2502    \u2502 - Updates       \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h3,{id:"key-rl-concepts-for-robotics",children:"Key RL Concepts for Robotics"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"State Space"}),": Robot configuration, sensor readings, environment state"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Space"}),": Joint commands, velocities, torques"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reward Function"}),": Task-specific rewards for desired behaviors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Policy"}),": Mapping from states to actions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Environment"}),": Physical or simulated robot and surroundings"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"challenges-in-robotic-rl",children:"Challenges in Robotic RL"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Continuous Action Spaces"}),": High-dimensional control problems"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sample Efficiency"}),": Expensive real-world training"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety"}),": Preventing damage during learning"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sim-to-Real Gap"}),": Differences between simulation and reality"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hardware Constraints"}),": Real-world limitations and noise"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"isaac-rl-framework",children:"Isaac RL Framework"}),"\n",(0,s.jsx)(n.h3,{id:"isaac-gym-integration",children:"Isaac Gym Integration"}),"\n",(0,s.jsx)(n.p,{children:"Isaac provides RL capabilities through Isaac Gym, which offers GPU-accelerated parallel environments:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport numpy as np\r\nfrom isaacgym import gymapi, gymtorch\r\nfrom isaacgym.torch_utils import *\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\n\r\nclass IsaacHumanoidRLEnv:\r\n    def __init__(self, cfg):\r\n        self.cfg = cfg\r\n        self.device = cfg.device\r\n        self.num_envs = cfg.num_envs\r\n        self.num_obs = cfg.num_obs\r\n        self.num_acts = cfg.num_acts\r\n        self.max_episode_length = cfg.max_episode_length\r\n\r\n        # Initialize gym\r\n        self.gym = gymapi.acquire_gym()\r\n        self.sim = None\r\n        self.envs = []\r\n        self.actor_handles = []\r\n\r\n        # RL parameters\r\n        self.reset_buf = torch.zeros(self.num_envs, device=self.device, dtype=torch.long)\r\n        self.progress_buf = torch.zeros(self.num_envs, device=self.device, dtype=torch.long)\r\n        self.extras = {}\r\n\r\n        # Initialize environments\r\n        self._create_envs()\r\n\r\n    def _create_envs(self):\r\n        """Create simulation environments"""\r\n        # Create simulation\r\n        self.sim = self.gym.create_sim(\r\n            device_id=0,\r\n            gpu_device_id=0,\r\n            pipeline=\'graphics\'\r\n        )\r\n\r\n        # Configure simulation\r\n        sim_params = gymapi.SimParams()\r\n        sim_params.up_axis = gymapi.UP_AXIS_Z\r\n        sim_params.gravity = gymapi.Vec3(0.0, 0.0, -9.81)\r\n\r\n        # Set physics parameters\r\n        sim_params.physx.solver_type = 1\r\n        sim_params.physx.num_position_iterations = 8\r\n        sim_params.physx.num_velocity_iterations = 1\r\n        sim_params.physx.max_gpu_contact_pairs = 2**23\r\n        sim_params.physx.num_threads = 4\r\n        sim_params.physx.rest_offset = 0.0\r\n        sim_params.physx.contact_offset = 0.02\r\n        sim_params.physx.friction_offset_threshold = 0.04\r\n\r\n        self.gym.set_sim_params(self.sim, sim_params)\r\n\r\n        # Create environments\r\n        spacing = self.cfg.env_spacing\r\n        lower = gymapi.Vec3(-spacing, -spacing, 0.0)\r\n        upper = gymapi.Vec3(spacing, spacing, spacing)\r\n\r\n        for i in range(self.num_envs):\r\n            env = self.gym.create_env(self.sim, lower, upper, 1)\r\n            self.envs.append(env)\r\n\r\n            # Load humanoid robot asset\r\n            asset_root = self.cfg.asset_root\r\n            asset_file = self.cfg.asset_file\r\n\r\n            asset_options = gymapi.AssetOptions()\r\n            asset_options.fix_base_link = False\r\n            asset_options.default_dof_drive_mode = gymapi.DOF_MODE_EFFORT\r\n            asset_options.set_collision_filter(1, 1)\r\n            asset_options.override_inertia = True\r\n\r\n            humanoid_asset = self.gym.load_asset(\r\n                self.sim, asset_root, asset_file, asset_options\r\n            )\r\n\r\n            # Create actor\r\n            pose = gymapi.Transform()\r\n            pose.p = gymapi.Vec3(0.0, 0.0, 1.0)\r\n            pose.r = gymapi.Quat(0.0, 0.0, 0.0, 1.0)\r\n\r\n            actor = self.gym.create_actor(\r\n                env, humanoid_asset, pose, "humanoid", i, 1, 1\r\n            )\r\n            self.actor_handles.append(actor)\r\n\r\n            # Set DOF properties\r\n            dof_props = self.gym.get_actor_dof_properties(env, actor)\r\n            dof_props[\'driveMode\'].fill(gymapi.DOF_MODE_EFFORT)\r\n            dof_props[\'stiffness\'] = self.cfg.dof_stiffness\r\n            dof_props[\'damping\'] = self.cfg.dof_damping\r\n            self.gym.set_actor_dof_properties(env, actor, dof_props)\r\n\r\n    def reset_idx(self, env_ids):\r\n        """Reset environments with specific indices"""\r\n        positions = 0.1 * (torch.rand((len(env_ids), self.num_acts), device=self.device) - 0.5)\r\n        velocities = 0.1 * (torch.rand((len(env_ids), self.num_acts), device=self.device) - 0.5)\r\n\r\n        # Set new DOF states\r\n        self.root_tensor[env_ids, 7:13] = 0.0  # Zero velocities\r\n        self.dof_state_tensor[env_ids, :self.num_acts] = positions\r\n        self.dof_state_tensor[env_ids, self.num_acts:] = velocities\r\n\r\n        # Reset buffers\r\n        self.reset_buf[env_ids] = 0\r\n        self.progress_buf[env_ids] = 0\r\n\r\n    def pre_physics_step(self, actions):\r\n        """Apply actions before physics simulation"""\r\n        # Scale actions\r\n        actions_tensor = torch.clamp(actions, -1.0, 1.0)\r\n\r\n        # Apply actions to DOF actuators\r\n        self.forces = actions_tensor * self.cfg.action_scale\r\n\r\n        # Set DOF efforts\r\n        self.gym.set_dof_actuation_force_tensor(\r\n            self.sim, gymtorch.unwrap_tensor(self.forces)\r\n        )\r\n\r\n    def post_physics_step(self):\r\n        """Process physics simulation results"""\r\n        self.gym.fetch_results(self.sim, True)\r\n        self.gym.step_graphics(self.sim)\r\n        self.gym.render_all_camera_sensors(self.sim)\r\n        self.gym.start_access_image_tensors(self.sim)\r\n\r\n        # Get state information\r\n        self.gym.refresh_dof_state_tensor(self.sim)\r\n        self.gym.refresh_actor_root_state_tensor(self.sim)\r\n        self.gym.refresh_force_sensor_tensor(self.sim)\r\n        self.gym.refresh_dof_force_tensor(self.sim)\r\n\r\n        # Compute observations, rewards, resets\r\n        self.compute_observations()\r\n        self.compute_rewards()\r\n        self.compute_resets()\r\n\r\n        self.gym.end_access_image_tensors(self.sim)\r\n\r\n    def compute_observations(self):\r\n        """Compute observations for all environments"""\r\n        # Example observation: joint positions, velocities, root pose\r\n        joint_pos = self.dof_state_tensor[:, :self.num_acts, 0]\r\n        joint_vel = self.dof_state_tensor[:, :self.num_acts, 1]\r\n        root_pos = self.root_tensor[:, :3]\r\n        root_rot = self.root_tensor[:, 3:7]\r\n        root_vel = self.root_tensor[:, 7:10]\r\n        root_ang_vel = self.root_tensor[:, 10:13]\r\n\r\n        # Normalize and concatenate observations\r\n        obs = torch.cat([\r\n            joint_pos,\r\n            joint_vel * 0.01,  # Scale velocities\r\n            root_rot,\r\n            root_vel * 0.01,  # Scale velocities\r\n            root_ang_vel * 0.01  # Scale angular velocities\r\n        ], dim=-1)\r\n\r\n        self.obs_buf = obs\r\n\r\n    def compute_rewards(self):\r\n        """Compute rewards for all environments"""\r\n        # Example reward: forward velocity + alive bonus - action penalty\r\n        root_vel = self.root_tensor[:, 7]\r\n        alive_reward = torch.ones_like(root_vel) * 2.0\r\n        forward_reward = root_vel * 3.0\r\n        action_penalty = torch.sum(torch.square(self.last_actions), dim=-1) * 0.01\r\n\r\n        rewards = alive_reward + forward_reward - action_penalty\r\n        self.rew_buf = rewards\r\n\r\n    def compute_resets(self):\r\n        """Compute resets for all environments"""\r\n        resets = torch.zeros(self.num_envs, device=self.device, dtype=torch.bool)\r\n\r\n        # Reset if humanoid falls\r\n        root_pos_z = self.root_tensor[:, 2]\r\n        fall_threshold = 0.3\r\n        resets = root_pos_z < fall_threshold\r\n\r\n        # Reset if episode length exceeded\r\n        resets = resets | (self.progress_buf >= self.max_episode_length - 1)\r\n\r\n        self.reset_buf = resets\r\n\r\n    def step(self, actions):\r\n        """Execute one simulation step"""\r\n        self.pre_physics_step(actions)\r\n\r\n        # Step simulation\r\n        for _ in range(self.cfg.sim_steps_per_control_step):\r\n            self.gym.simulate(self.sim)\r\n\r\n        self.post_physics_step()\r\n\r\n        # Update buffers\r\n        self.last_actions = actions.clone()\r\n        self.progress_buf += 1\r\n\r\n        return self.obs_buf, self.rew_buf, self.reset_buf, self.extras\r\n\r\n    def reset(self):\r\n        """Reset all environments"""\r\n        env_ids = torch.arange(self.num_envs, device=self.device, dtype=torch.long)\r\n        self.reset_idx(env_ids)\r\n        return self.obs_buf\n'})}),"\n",(0,s.jsx)(n.h2,{id:"deep-reinforcement-learning-implementation",children:"Deep Reinforcement Learning Implementation"}),"\n",(0,s.jsx)(n.h3,{id:"actor-critic-network-architecture",children:"Actor-Critic Network Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass HumanoidActorCritic(nn.Module):\r\n    def __init__(self, num_obs, num_acts, actor_hidden_dims=[512, 256, 128],\r\n                 critic_hidden_dims=[512, 256, 128], activation=\'elu\'):\r\n        super(HumanoidActorCritic, self).__init__()\r\n\r\n        # Activation function\r\n        if activation == \'elu\':\r\n            activation_fn = nn.ELU\r\n        elif activation == \'tanh\':\r\n            activation_fn = nn.Tanh\r\n        elif activation == \'relu\':\r\n            activation_fn = nn.ReLU\r\n        else:\r\n            raise ValueError(f"Unknown activation function: {activation}")\r\n\r\n        # Actor network\r\n        actor_layers = []\r\n        actor_layers.append(nn.Linear(num_obs, actor_hidden_dims[0]))\r\n        actor_layers.append(activation_fn())\r\n\r\n        for i in range(len(actor_hidden_dims) - 1):\r\n            actor_layers.append(nn.Linear(actor_hidden_dims[i], actor_hidden_dims[i+1]))\r\n            actor_layers.append(activation_fn())\r\n\r\n        actor_layers.append(nn.Linear(actor_hidden_dims[-1], num_acts))\r\n        actor_layers.append(nn.Tanh())  # Output in [-1, 1] range\r\n\r\n        self.actor = nn.Sequential(*actor_layers)\r\n\r\n        # Critic network\r\n        critic_layers = []\r\n        critic_layers.append(nn.Linear(num_obs, critic_hidden_dims[0]))\r\n        critic_layers.append(activation_fn())\r\n\r\n        for i in range(len(critic_hidden_dims) - 1):\r\n            critic_layers.append(nn.Linear(critic_hidden_dims[i], critic_hidden_dims[i+1]))\r\n            critic_layers.append(activation_fn())\r\n\r\n        critic_layers.append(nn.Linear(critic_hidden_dims[-1], 1))\r\n\r\n        self.critic = nn.Sequential(*critic_layers)\r\n\r\n        # Initialize network weights\r\n        self.init_weights()\r\n\r\n    def init_weights(self):\r\n        """Initialize network weights"""\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Linear):\r\n                nn.init.orthogonal_(m.weight, gain=0.01)\r\n                nn.init.zeros_(m.bias)\r\n\r\n    def forward(self, obs):\r\n        """Forward pass for both actor and critic"""\r\n        return self.actor(obs), self.critic(obs)\r\n\r\n    def get_actions(self, obs):\r\n        """Get actions from actor network"""\r\n        return self.actor(obs)\r\n\r\n    def get_value(self, obs):\r\n        """Get value from critic network"""\r\n        return self.critic(obs)\r\n\r\nclass HumanoidPPOAgent:\r\n    def __init__(self, obs_dim, act_dim, lr=3e-4, gamma=0.99, lam=0.95,\r\n                 clip_param=0.2, epochs=10, mini_batch_size=64):\r\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\r\n\r\n        # Networks\r\n        self.actor_critic = HumanoidActorCritic(obs_dim, act_dim).to(self.device)\r\n        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\r\n\r\n        # Hyperparameters\r\n        self.gamma = gamma\r\n        self.lam = lam\r\n        self.clip_param = clip_param\r\n        self.epochs = epochs\r\n        self.mini_batch_size = mini_batch_size\r\n\r\n    def update(self, buffer):\r\n        """Update policy using PPO algorithm"""\r\n        # Get data from buffer\r\n        obs_batch = buffer[\'obs\'].to(self.device)\r\n        act_batch = buffer[\'actions\'].to(self.device)\r\n        ret_batch = buffer[\'returns\'].to(self.device)\r\n        adv_batch = buffer[\'advantages\'].to(self.device)\r\n        old_log_prob_batch = buffer[\'log_probs\'].to(self.device)\r\n\r\n        # Normalize advantages\r\n        adv_batch = (adv_batch - adv_batch.mean()) / (adv_batch.std() + 1e-8)\r\n\r\n        for _ in range(self.epochs):\r\n            # Get current policy values\r\n            mu, values = self.actor_critic(obs_batch)\r\n\r\n            # Calculate new log probabilities\r\n            dist = torch.distributions.Normal(mu, 1.0)  # Fixed std for simplicity\r\n            new_log_prob = dist.log_prob(act_batch).sum(dim=-1)\r\n\r\n            # Calculate ratio\r\n            ratio = torch.exp(new_log_prob - old_log_prob_batch)\r\n\r\n            # Calculate surrogate objectives\r\n            surr1 = ratio * adv_batch\r\n            surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * adv_batch\r\n            actor_loss = -torch.min(surr1, surr2).mean()\r\n\r\n            # Value loss\r\n            value_loss = F.mse_loss(values.squeeze(), ret_batch)\r\n\r\n            # Total loss\r\n            total_loss = actor_loss + 0.5 * value_loss\r\n\r\n            # Update networks\r\n            self.optimizer.zero_grad()\r\n            total_loss.backward()\r\n            # Gradient clipping\r\n            torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), 0.5)\r\n            self.optimizer.step()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"domain-randomization-for-sim-to-real-transfer",children:"Domain Randomization for Sim-to-Real Transfer"}),"\n",(0,s.jsx)(n.h3,{id:"physics-parameter-randomization",children:"Physics Parameter Randomization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import numpy as np\r\n\r\nclass DomainRandomizer:\r\n    def __init__(self, env):\r\n        self.env = env\r\n        self.randomization_params = {\r\n            'mass': {'range': [0.8, 1.2], 'distribution': 'uniform'},\r\n            'friction': {'range': [0.5, 1.5], 'distribution': 'uniform'},\r\n            'restitution': {'range': [0.0, 0.2], 'distribution': 'uniform'},\r\n            'damping': {'range': [0.8, 1.2], 'distribution': 'uniform'},\r\n            'stiffness': {'range': [0.8, 1.2], 'distribution': 'uniform'},\r\n            'gravity': {'range': [-9.81*1.1, -9.81*0.9], 'distribution': 'uniform'},\r\n            'delay': {'range': [0, 2], 'distribution': 'uniform'},  # Control delay in steps\r\n        }\r\n\r\n    def randomize_env(self, env_id):\r\n        \"\"\"Randomize physics parameters for a specific environment\"\"\"\r\n        # Randomize robot mass\r\n        mass_range = self.randomization_params['mass']['range']\r\n        mass_ratio = np.random.uniform(mass_range[0], mass_range[1])\r\n\r\n        # Get actor properties and modify mass\r\n        props = self.env.gym.get_actor_rigid_body_properties(self.env.envs[env_id], self.env.actor_handles[env_id])\r\n        for prop in props:\r\n            prop.mass *= mass_ratio\r\n        self.env.gym.set_actor_rigid_body_properties(self.env.envs[env_id], self.env.actor_handles[env_id], props)\r\n\r\n        # Randomize joint damping\r\n        dof_props = self.env.gym.get_actor_dof_properties(self.env.envs[env_id], self.env.actor_handles[env_id])\r\n        damping_range = self.randomization_params['damping']['range']\r\n        for i in range(len(dof_props['damping'])):\r\n            random_damping = dof_props['damping'][i] * np.random.uniform(damping_range[0], damping_range[1])\r\n            dof_props['damping'][i] = random_damping\r\n        self.env.gym.set_actor_dof_properties(self.env.envs[env_id], self.env.actor_handles[env_id], dof_props)\r\n\r\n        # Randomize joint stiffness\r\n        stiffness_range = self.randomization_params['stiffness']['range']\r\n        for i in range(len(dof_props['stiffness'])):\r\n            random_stiffness = dof_props['stiffness'][i] * np.random.uniform(stiffness_range[0], stiffness_range[1])\r\n            dof_props['stiffness'][i] = random_stiffness\r\n        self.env.gym.set_actor_dof_properties(self.env.envs[env_id], self.env.actor_handles[env_id], dof_props)\r\n\r\n    def randomize_sensor_noise(self):\r\n        \"\"\"Add random sensor noise\"\"\"\r\n        noise_scale = np.random.uniform(0.0, 0.1)  # Scale of noise\r\n        return noise_scale\r\n\r\n    def randomize_control_delay(self):\r\n        \"\"\"Randomize control delay\"\"\"\r\n        delay_steps = np.random.uniform(\r\n            self.randomization_params['delay']['range'][0],\r\n            self.randomization_params['delay']['range'][1]\r\n        )\r\n        return int(delay_steps)\r\n\r\n    def apply_randomization(self, env_ids):\r\n        \"\"\"Apply randomization to specified environments\"\"\"\r\n        for env_id in env_ids:\r\n            self.randomize_env(env_id)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"visual-domain-randomization",children:"Visual Domain Randomization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn.functional as F\r\n\r\nclass VisualDomainRandomizer:\r\n    def __init__(self):\r\n        self.visual_params = {\r\n            'brightness': {'range': [0.5, 1.5], 'prob': 0.5},\r\n            'contrast': {'range': [0.8, 1.2], 'prob': 0.5},\r\n            'saturation': {'range': [0.8, 1.2], 'prob': 0.5},\r\n            'hue': {'range': [-0.1, 0.1], 'prob': 0.3},\r\n            'noise': {'range': [0.0, 0.1], 'prob': 0.5},\r\n            'blur': {'range': [0.0, 2.0], 'prob': 0.3},\r\n        }\r\n\r\n    def randomize_image(self, image_tensor):\r\n        \"\"\"Apply visual domain randomization to image tensor\"\"\"\r\n        # Random brightness\r\n        if torch.rand(1) < self.visual_params['brightness']['prob']:\r\n            brightness_factor = torch.rand(1) * (\r\n                self.visual_params['brightness']['range'][1] -\r\n                self.visual_params['brightness']['range'][0]\r\n            ) + self.visual_params['brightness']['range'][0]\r\n            image_tensor = image_tensor * brightness_factor\r\n\r\n        # Random contrast\r\n        if torch.rand(1) < self.visual_params['contrast']['prob']:\r\n            # Convert to grayscale to calculate mean\r\n            mean = torch.mean(image_tensor, dim=[1, 2, 3], keepdim=True)\r\n            contrast_factor = torch.rand(1) * (\r\n                self.visual_params['contrast']['range'][1] -\r\n                self.visual_params['contrast']['range'][0]\r\n            ) + self.visual_params['contrast']['range'][0]\r\n            image_tensor = (image_tensor - mean) * contrast_factor + mean\r\n\r\n        # Random saturation\r\n        if torch.rand(1) < self.visual_params['saturation']['prob']:\r\n            # Convert to grayscale\r\n            gray = torch.mean(image_tensor, dim=1, keepdim=True)\r\n            saturation_factor = torch.rand(1) * (\r\n                self.visual_params['saturation']['range'][1] -\r\n                self.visual_params['saturation']['range'][0]\r\n            ) + self.visual_params['saturation']['range'][0]\r\n            image_tensor = gray + (image_tensor - gray) * saturation_factor\r\n\r\n        # Random noise\r\n        if torch.rand(1) < self.visual_params['noise']['prob']:\r\n            noise_factor = torch.rand(1) * (\r\n                self.visual_params['noise']['range'][1] -\r\n                self.visual_params['noise']['range'][0]\r\n            ) + self.visual_params['noise']['range'][0]\r\n            noise = torch.randn_like(image_tensor) * noise_factor\r\n            image_tensor = torch.clamp(image_tensor + noise, 0, 1)\r\n\r\n        # Clamp values\r\n        image_tensor = torch.clamp(image_tensor, 0, 1)\r\n        return image_tensor\n"})}),"\n",(0,s.jsx)(n.h2,{id:"isaac-rl-training-loop",children:"Isaac RL Training Loop"}),"\n",(0,s.jsx)(n.h3,{id:"complete-training-implementation",children:"Complete Training Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import os\r\nimport pickle\r\nimport torch\r\nimport numpy as np\r\nfrom collections import deque\r\n\r\nclass IsaacRLTrainer:\r\n    def __init__(self, env, agent, max_iterations=1000):\r\n        self.env = env\r\n        self.agent = agent\r\n        self.max_iterations = max_iterations\r\n\r\n        # Training statistics\r\n        self.episode_rewards = deque(maxlen=100)\r\n        self.episode_lengths = deque(maxlen=100)\r\n        self.mean_reward = 0\r\n        self.mean_length = 0\r\n\r\n        # Checkpoint directory\r\n        self.checkpoint_dir = "checkpoints"\r\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\r\n\r\n    def train(self):\r\n        """Main training loop"""\r\n        obs = self.env.reset()\r\n        episode_reward = 0\r\n        episode_length = 0\r\n\r\n        for iteration in range(self.max_iterations):\r\n            # Collect trajectories\r\n            trajectory = self.collect_trajectory(obs)\r\n\r\n            # Update policy\r\n            self.agent.update(trajectory)\r\n\r\n            # Update statistics\r\n            self.update_statistics(trajectory)\r\n\r\n            # Log progress\r\n            if iteration % 10 == 0:\r\n                self.log_progress(iteration)\r\n\r\n            # Save checkpoint\r\n            if iteration % 100 == 0:\r\n                self.save_checkpoint(iteration)\r\n\r\n            # Reset environment if needed\r\n            obs = self.env.reset()\r\n\r\n    def collect_trajectory(self, initial_obs):\r\n        """Collect trajectory data for training"""\r\n        obs_buffer = []\r\n        act_buffer = []\r\n        rew_buffer = []\r\n        done_buffer = []\r\n        log_prob_buffer = []\r\n\r\n        obs = initial_obs\r\n        for _ in range(self.env.max_episode_length):\r\n            # Get action from policy\r\n            with torch.no_grad():\r\n                action = self.agent.actor_critic.get_actions(obs)\r\n                # Add noise for exploration\r\n                action = action + torch.randn_like(action) * 0.1\r\n                action = torch.clamp(action, -1, 1)\r\n\r\n                # Get log probability\r\n                dist = torch.distributions.Normal(\r\n                    self.agent.actor_critic.actor(obs), 1.0\r\n                )\r\n                log_prob = dist.log_prob(action).sum(dim=-1)\r\n\r\n            # Store data\r\n            obs_buffer.append(obs.clone())\r\n            act_buffer.append(action.clone())\r\n            log_prob_buffer.append(log_prob.clone())\r\n\r\n            # Step environment\r\n            obs, reward, done, info = self.env.step(action)\r\n\r\n            rew_buffer.append(reward.clone())\r\n            done_buffer.append(done.clone())\r\n\r\n            if torch.any(done):\r\n                break\r\n\r\n        # Convert to tensors\r\n        trajectory = {\r\n            \'obs\': torch.stack(obs_buffer),\r\n            \'actions\': torch.stack(act_buffer),\r\n            \'rewards\': torch.stack(rew_buffer),\r\n            \'dones\': torch.stack(done_buffer),\r\n            \'log_probs\': torch.stack(log_prob_buffer)\r\n        }\r\n\r\n        # Compute returns and advantages\r\n        trajectory = self.compute_returns_advantages(trajectory)\r\n\r\n        return trajectory\r\n\r\n    def compute_returns_advantages(self, trajectory):\r\n        """Compute returns and advantages for PPO"""\r\n        rewards = trajectory[\'rewards\']\r\n        dones = trajectory[\'dones\']\r\n        values = []\r\n\r\n        # Get values from critic\r\n        with torch.no_grad():\r\n            for obs in trajectory[\'obs\']:\r\n                value = self.agent.actor_critic.get_value(obs)\r\n                values.append(value)\r\n\r\n        values = torch.stack(values).squeeze(-1)\r\n\r\n        # Compute advantages and returns\r\n        advantages = torch.zeros_like(rewards)\r\n        returns = torch.zeros_like(rewards)\r\n\r\n        gae = 0\r\n        for t in reversed(range(len(rewards))):\r\n            if t == len(rewards) - 1:\r\n                next_value = 0  # Terminal state\r\n            else:\r\n                next_value = values[t + 1]\r\n\r\n            # Compute TD error\r\n            delta = rewards[t] + self.agent.gamma * next_value * (1 - dones[t]) - values[t]\r\n\r\n            # Compute GAE\r\n            gae = delta + self.agent.gamma * self.agent.lam * (1 - dones[t]) * gae\r\n            advantages[t] = gae\r\n\r\n            # Compute return\r\n            returns[t] = gae + values[t]\r\n\r\n        trajectory[\'advantages\'] = advantages\r\n        trajectory[\'returns\'] = returns\r\n\r\n        return trajectory\r\n\r\n    def update_statistics(self, trajectory):\r\n        """Update training statistics"""\r\n        rewards = trajectory[\'rewards\'].sum(dim=0)  # Sum over time steps\r\n        lengths = torch.sum(1 - trajectory[\'dones\'].float(), dim=0)  # Episode lengths\r\n\r\n        self.episode_rewards.extend(rewards.cpu().numpy())\r\n        self.episode_lengths.extend(lengths.cpu().numpy())\r\n\r\n        if len(self.episode_rewards) > 0:\r\n            self.mean_reward = np.mean(self.episode_rewards)\r\n            self.mean_length = np.mean(self.episode_lengths)\r\n\r\n    def log_progress(self, iteration):\r\n        """Log training progress"""\r\n        print(f"Iteration {iteration}: "\r\n              f"Mean Reward: {self.mean_reward:.2f}, "\r\n              f"Mean Length: {self.mean_length:.2f}")\r\n\r\n    def save_checkpoint(self, iteration):\r\n        """Save model checkpoint"""\r\n        checkpoint_path = os.path.join(\r\n            self.checkpoint_dir, f"checkpoint_{iteration}.pth"\r\n        )\r\n\r\n        torch.save({\r\n            \'iteration\': iteration,\r\n            \'model_state_dict\': self.agent.actor_critic.state_dict(),\r\n            \'optimizer_state_dict\': self.agent.optimizer.state_dict(),\r\n            \'mean_reward\': self.mean_reward,\r\n        }, checkpoint_path)\r\n\r\n        print(f"Checkpoint saved: {checkpoint_path}")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"sim-to-real-transfer-techniques",children:"Sim-to-Real Transfer Techniques"}),"\n",(0,s.jsx)(n.h3,{id:"system-identification-and-modeling",children:"System Identification and Modeling"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\nfrom scipy import optimize\r\n\r\nclass SystemIdentifier:\r\n    def __init__(self, robot_model):\r\n        self.robot_model = robot_model\r\n        self.parameters = {}\r\n\r\n    def identify_dynamics(self, real_data, sim_data):\r\n        """Identify dynamics parameters by fitting simulation to real data"""\r\n        # Define objective function to minimize difference between real and sim\r\n        def objective(params):\r\n            # Set simulation parameters\r\n            self.set_simulation_params(params)\r\n\r\n            # Run simulation with same inputs as real experiment\r\n            sim_output = self.run_simulation(real_data[\'inputs\'])\r\n\r\n            # Calculate error between real and simulated outputs\r\n            error = np.mean((real_data[\'outputs\'] - sim_output) ** 2)\r\n            return error\r\n\r\n        # Initial parameter guess\r\n        initial_params = self.get_initial_params()\r\n\r\n        # Optimize parameters\r\n        result = optimize.minimize(objective, initial_params, method=\'BFGS\')\r\n\r\n        self.parameters = result.x\r\n        return result.x\r\n\r\n    def get_initial_params(self):\r\n        """Get initial parameter estimates"""\r\n        # Return initial estimates for masses, inertias, friction, etc.\r\n        return np.array([1.0, 0.5, 0.1])  # Placeholder values\r\n\r\n    def set_simulation_params(self, params):\r\n        """Set simulation parameters"""\r\n        # Update simulation with identified parameters\r\n        pass\r\n\r\n    def run_simulation(self, inputs):\r\n        """Run simulation with given inputs"""\r\n        # Execute simulation and return outputs\r\n        return np.zeros_like(inputs)  # Placeholder\r\n\r\nclass AdaptiveController:\r\n    def __init__(self, nominal_controller, system_identifier):\r\n        self.nominal_controller = nominal_controller\r\n        self.system_identifier = system_identifier\r\n        self.adaptation_gain = 0.01\r\n\r\n    def compute_control(self, state, reference, real_data=None):\r\n        """Compute control with adaptation"""\r\n        # Compute nominal control\r\n        nominal_control = self.nominal_controller.compute(state, reference)\r\n\r\n        # If real data is available, adapt controller\r\n        if real_data is not None:\r\n            adaptation_term = self.adapt_control_with_real_data(state, real_data)\r\n            final_control = nominal_control + adaptation_term\r\n        else:\r\n            final_control = nominal_control\r\n\r\n        return final_control\r\n\r\n    def adapt_control_with_real_data(self, state, real_data):\r\n        """Adapt control based on real-world data"""\r\n        # Implement adaptive control law\r\n        # This could be model reference adaptive control, etc.\r\n        return np.zeros_like(state)  # Placeholder\n'})}),"\n",(0,s.jsx)(n.h3,{id:"robust-control-design",children:"Robust Control Design"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import numpy as np\r\nfrom scipy import linalg\r\n\r\nclass RobustController:\r\n    def __init__(self, nominal_model, uncertainty_bounds):\r\n        self.nominal_model = nominal_model\r\n        self.uncertainty_bounds = uncertainty_bounds\r\n\r\n    def design_robust_controller(self):\r\n        \"\"\"Design robust controller using H-infinity or mu-synthesis\"\"\"\r\n        # This would implement advanced robust control design\r\n        # For simplicity, implementing a basic robust PID controller\r\n\r\n        # Robust PID gains based on uncertainty bounds\r\n        self.kp = self.calculate_robust_gain('P')\r\n        self.ki = self.calculate_robust_gain('I')\r\n        self.kd = self.calculate_robust_gain('D')\r\n\r\n    def calculate_robust_gain(self, type):\r\n        \"\"\"Calculate robust gain based on uncertainty\"\"\"\r\n        # Simplified robust gain calculation\r\n        if type == 'P':\r\n            return 1.0 / (1 + self.uncertainty_bounds['parametric'])\r\n        elif type == 'I':\r\n            return 0.1 / (1 + self.uncertainty_bounds['parametric'])\r\n        elif type == 'D':\r\n            return 0.01 / (1 + self.uncertainty_bounds['parametric'])\r\n\r\n    def compute_control(self, error, error_integral, error_derivative):\r\n        \"\"\"Compute robust control output\"\"\"\r\n        p_term = self.kp * error\r\n        i_term = self.ki * error_integral\r\n        d_term = self.kd * error_derivative\r\n\r\n        return p_term + i_term + d_term\r\n\r\nclass GainScheduledController:\r\n    def __init__(self):\r\n        self.gain_schedule = self.create_gain_schedule()\r\n\r\n    def create_gain_schedule(self):\r\n        \"\"\"Create gain schedule based on operating conditions\"\"\"\r\n        # Define different gain sets for different conditions\r\n        # e.g., different walking speeds, terrain types, etc.\r\n        schedule = {\r\n            'slow_walking': {'kp': 1.0, 'ki': 0.1, 'kd': 0.05},\r\n            'fast_walking': {'kp': 1.5, 'ki': 0.2, 'kd': 0.1},\r\n            'standing': {'kp': 2.0, 'ki': 0.3, 'kd': 0.15}\r\n        }\r\n        return schedule\r\n\r\n    def get_gains(self, condition):\r\n        \"\"\"Get gains for current operating condition\"\"\"\r\n        return self.gain_schedule.get(condition, self.gain_schedule['standing'])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"transfer-validation-and-testing",children:"Transfer Validation and Testing"}),"\n",(0,s.jsx)(n.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nclass TransferValidator:\r\n    def __init__(self):\r\n        self.metrics = {}\r\n\r\n    def validate_transfer(self, sim_policy, real_robot):\r\n        """Validate policy transfer from simulation to reality"""\r\n        # Collect performance data from both sim and real\r\n        sim_performance = self.evaluate_policy(sim_policy, \'simulation\')\r\n        real_performance = self.evaluate_policy(sim_policy, \'real\')\r\n\r\n        # Calculate sim-to-real gap metrics\r\n        self.metrics[\'reward_gap\'] = real_performance[\'reward\'] - sim_performance[\'reward\']\r\n        self.metrics[\'success_rate_gap\'] = real_performance[\'success_rate\'] - sim_performance[\'success_rate\']\r\n        self.metrics[\'stability_gap\'] = real_performance[\'stability\'] - sim_performance[\'stability\']\r\n\r\n        return self.metrics\r\n\r\n    def evaluate_policy(self, policy, environment):\r\n        """Evaluate policy in specified environment"""\r\n        if environment == \'simulation\':\r\n            # Evaluate in Isaac simulation\r\n            performance = self.evaluate_in_simulation(policy)\r\n        else:\r\n            # Evaluate on real robot\r\n            performance = self.evaluate_on_real_robot(policy)\r\n\r\n        return performance\r\n\r\n    def evaluate_in_simulation(self, policy):\r\n        """Evaluate policy in Isaac simulation"""\r\n        # Run multiple episodes and collect statistics\r\n        rewards = []\r\n        success_rates = []\r\n        stability_metrics = []\r\n\r\n        for episode in range(100):  # Run 100 episodes\r\n            episode_reward = 0\r\n            steps = 0\r\n            success = True\r\n\r\n            # Reset environment\r\n            obs = self.reset_simulation()\r\n\r\n            for step in range(1000):  # Max 1000 steps per episode\r\n                action = policy.get_action(obs)\r\n                obs, reward, done, info = self.step_simulation(action)\r\n\r\n                episode_reward += reward\r\n                steps += 1\r\n\r\n                # Check for failure conditions\r\n                if self.check_failure_conditions(info):\r\n                    success = False\r\n                    break\r\n\r\n                if done:\r\n                    break\r\n\r\n            rewards.append(episode_reward)\r\n            success_rates.append(1.0 if success else 0.0)\r\n            stability_metrics.append(self.calculate_stability(obs))\r\n\r\n        return {\r\n            \'reward\': np.mean(rewards),\r\n            \'success_rate\': np.mean(success_rates),\r\n            \'stability\': np.mean(stability_metrics)\r\n        }\r\n\r\n    def evaluate_on_real_robot(self, policy):\r\n        """Evaluate policy on real robot (simplified)"""\r\n        # This would interface with the real robot\r\n        # For simulation, return dummy values\r\n        return {\r\n            \'reward\': 0.0,  # Placeholder\r\n            \'success_rate\': 0.0,  # Placeholder\r\n            \'stability\': 0.0  # Placeholder\r\n        }\r\n\r\n    def check_failure_conditions(self, info):\r\n        """Check if episode should end due to failure"""\r\n        # Check for robot falling, joint limits, etc.\r\n        return False  # Placeholder\r\n\r\n    def calculate_stability(self, obs):\r\n        """Calculate stability metric from observations"""\r\n        # Calculate based on robot state (orientation, velocity, etc.)\r\n        return 1.0  # Placeholder\r\n\r\nclass TransferAnalyzer:\r\n    def __init__(self):\r\n        self.analysis_results = {}\r\n\r\n    def analyze_transfer_gap(self, sim_data, real_data):\r\n        """Analyze the gap between simulation and real performance"""\r\n        # Compare distributions\r\n        self.compare_distributions(sim_data, real_data)\r\n\r\n        # Analyze specific failure modes\r\n        self.analyze_failure_modes(sim_data, real_data)\r\n\r\n        # Identify key factors causing transfer gap\r\n        self.identify_transfer_factors(sim_data, real_data)\r\n\r\n        return self.analysis_results\r\n\r\n    def compare_distributions(self, sim_data, real_data):\r\n        """Compare state/action distributions between sim and real"""\r\n        # Use statistical tests to compare distributions\r\n        from scipy import stats\r\n\r\n        # Compare state distributions\r\n        state_sim = np.array(sim_data[\'states\'])\r\n        state_real = np.array(real_data[\'states\'])\r\n\r\n        # Perform Kolmogorov-Smirnov test\r\n        ks_stat, p_value = stats.ks_2samp(state_sim.flatten(), state_real.flatten())\r\n\r\n        self.analysis_results[\'state_distribution_similarity\'] = {\r\n            \'ks_statistic\': ks_stat,\r\n            \'p_value\': p_value,\r\n            \'similar\': p_value > 0.05  # If p > 0.05, distributions are similar\r\n        }\r\n\r\n    def analyze_failure_modes(self, sim_data, real_data):\r\n        """Analyze different failure modes in sim vs real"""\r\n        sim_failures = self.extract_failure_modes(sim_data)\r\n        real_failures = self.extract_failure_modes(real_data)\r\n\r\n        self.analysis_results[\'failure_mode_analysis\'] = {\r\n            \'sim_failures\': sim_failures,\r\n            \'real_failures\': real_failures,\r\n            \'key_differences\': self.compare_failure_modes(sim_failures, real_failures)\r\n        }\r\n\r\n    def extract_failure_modes(self, data):\r\n        """Extract failure modes from data"""\r\n        # Analyze data to identify common failure patterns\r\n        return []  # Placeholder\r\n\r\n    def compare_failure_modes(self, sim_failures, real_failures):\r\n        """Compare failure modes between sim and real"""\r\n        # Identify key differences in failure patterns\r\n        return {}  # Placeholder\n'})}),"\n",(0,s.jsx)(n.h2,{id:"practical-implementation-guidelines",children:"Practical Implementation Guidelines"}),"\n",(0,s.jsx)(n.h3,{id:"best-practices-for-sim-to-real-transfer",children:"Best Practices for Sim-to-Real Transfer"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class TransferBestPractices:\r\n    def __init__(self):\r\n        self.guidelines = self.get_guidelines()\r\n\r\n    def get_guidelines(self):\r\n        \"\"\"Get comprehensive guidelines for sim-to-real transfer\"\"\"\r\n        return {\r\n            'simulation_fidelity': {\r\n                'description': 'Ensure simulation captures essential dynamics',\r\n                'recommendations': [\r\n                    'Model contact dynamics accurately',\r\n                    'Include sensor noise and delays',\r\n                    'Match control frequencies',\r\n                    'Validate basic behaviors in sim'\r\n                ]\r\n            },\r\n            'domain_randomization': {\r\n                'description': 'Use broad randomization to improve robustness',\r\n                'recommendations': [\r\n                    'Randomize physics parameters widely',\r\n                    'Include visual domain randomization',\r\n                    'Randomize environment conditions',\r\n                    'Test robustness to parameter changes'\r\n                ]\r\n            },\r\n            'policy_robustness': {\r\n                'description': 'Design policies that are robust to uncertainties',\r\n                'recommendations': [\r\n                    'Use conservative reward shaping',\r\n                    'Include safety constraints',\r\n                    'Test on diverse environments',\r\n                    'Implement failure recovery'\r\n                ]\r\n            },\r\n            'validation_protocol': {\r\n                'description': 'Systematic validation before real deployment',\r\n                'recommendations': [\r\n                    'Test on multiple simulated robots',\r\n                    'Validate on hardware-in-the-loop',\r\n                    'Start with safe behaviors',\r\n                    'Gradually increase complexity'\r\n                ]\r\n            }\r\n        }\r\n\r\n    def apply_guidelines(self, rl_agent, env):\r\n        \"\"\"Apply best practices to RL agent and environment\"\"\"\r\n        # Implement conservative reward shaping\r\n        self.conservative_reward_shaping(rl_agent)\r\n\r\n        # Add safety constraints\r\n        self.add_safety_constraints(rl_agent)\r\n\r\n        # Enable domain randomization\r\n        self.enable_domain_randomization(env)\r\n\r\n        # Implement monitoring\r\n        self.setup_monitoring(rl_agent, env)\r\n\r\n    def conservative_reward_shaping(self, agent):\r\n        \"\"\"Implement conservative reward shaping for safety\"\"\"\r\n        # Modify reward function to penalize aggressive actions\r\n        # Encourage stable, safe behaviors\r\n        pass\r\n\r\n    def add_safety_constraints(self, agent):\r\n        \"\"\"Add safety constraints to policy\"\"\"\r\n        # Implement constraint-based policy optimization\r\n        # or use control barrier functions\r\n        pass\r\n\r\n    def enable_domain_randomization(self, env):\r\n        \"\"\"Enable comprehensive domain randomization\"\"\"\r\n        # Set up randomization ranges for all relevant parameters\r\n        pass\r\n\r\n    def setup_monitoring(self, agent, env):\r\n        \"\"\"Set up monitoring for safe deployment\"\"\"\r\n        # Implement safety monitoring and intervention\r\n        pass\n"})}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting-and-debugging",children:"Troubleshooting and Debugging"}),"\n",(0,s.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class TransferTroubleshooter:\r\n    def __init__(self):\r\n        self.issues = self.get_common_issues()\r\n\r\n    def get_common_issues(self):\r\n        \"\"\"Get common sim-to-real transfer issues\"\"\"\r\n        return {\r\n            'reality_gap': {\r\n                'symptoms': [\r\n                    'Policy works in sim but fails on real robot',\r\n                    'Performance degrades significantly',\r\n                    'Different failure modes observed'\r\n                ],\r\n                'causes': [\r\n                    'Inaccurate simulation models',\r\n                    'Missing physics effects',\r\n                    'Sensor discrepancies',\r\n                    'Actuator limitations'\r\n                ],\r\n                'solutions': [\r\n                    'Improve simulation fidelity',\r\n                    'Increase domain randomization',\r\n                    'System identification',\r\n                    'Robust control design'\r\n                ]\r\n            },\r\n            'sample_efficiency': {\r\n                'symptoms': [\r\n                    'Long training times',\r\n                    'Poor data utilization',\r\n                    'Overfitting to simulation'\r\n                ],\r\n                'causes': [\r\n                    'Large sim-to-real gap',\r\n                    'Inefficient exploration',\r\n                    'Poor reward design'\r\n                ],\r\n                'solutions': [\r\n                    'Use transfer learning',\r\n                    'Implement curriculum learning',\r\n                    'Improve reward shaping',\r\n                    'Use model-based RL'\r\n                ]\r\n            },\r\n            'safety_concerns': {\r\n                'symptoms': [\r\n                    'Unsafe behaviors during learning',\r\n                    'Damage to robot/hardware',\r\n                    'Unpredictable actions'\r\n                ],\r\n                'causes': [\r\n                    'Lack of safety constraints',\r\n                    'Aggressive exploration',\r\n                    'Simulation errors'\r\n                ],\r\n                'solutions': [\r\n                    'Implement safety barriers',\r\n                    'Use safe exploration methods',\r\n                    'Gradual deployment',\r\n                    'Human oversight'\r\n                ]\r\n            }\r\n        }\r\n\r\n    def diagnose_issue(self, symptoms):\r\n        \"\"\"Diagnose issue based on symptoms\"\"\"\r\n        for issue_name, issue_info in self.issues.items():\r\n            matching_symptoms = set(symptoms) & set(issue_info['symptoms'])\r\n            if len(matching_symptoms) > len(issue_info['symptoms']) * 0.3:  # 30% match\r\n                return {\r\n                    'issue': issue_name,\r\n                    'confidence': len(matching_symptoms) / len(issue_info['symptoms']),\r\n                    'solutions': issue_info['solutions']\r\n                }\r\n        return None\n"})}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"Reinforcement Learning with Isaac provides a powerful framework for training complex humanoid robot behaviors in simulation with the goal of transferring these capabilities to real robots. The key to successful sim-to-real transfer lies in:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"High-fidelity simulation"}),": Creating accurate models that capture essential robot dynamics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Domain randomization"}),": Training policies to be robust across various conditions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"System identification"}),": Understanding and modeling the differences between sim and real"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robust control design"}),": Implementing controllers that can handle uncertainties"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gradual deployment"}),": Systematically validating and transferring learned behaviors"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The Isaac platform's GPU-accelerated simulation capabilities enable efficient training of complex humanoid behaviors that would be impractical to learn directly on hardware. By combining advanced RL algorithms with comprehensive domain randomization and careful validation protocols, it's possible to achieve successful sim-to-real transfer for sophisticated humanoid robot tasks."}),"\n",(0,s.jsx)(n.p,{children:"In the next module, we'll explore Vision-Language-Action (VLA) systems, which represent the cutting edge of AI-driven humanoid robotics, integrating perception, language understanding, and action execution in unified frameworks."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>o});var s=r(6540);const a={},i=s.createContext(a);function t(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);